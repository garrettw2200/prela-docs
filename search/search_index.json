{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Prela","text":"<p>Observability for AI Agents</p> <p>Prela is a production-ready Python SDK for tracing, monitoring, and evaluating autonomous AI agents. Get complete visibility into what your LLM-powered applications are doing with zero-code auto-instrumentation.</p> <p>Production Validated</p> <p>\u2705 21/21 core features validated with real API calls \u2705 4/4 performance criteria met (SDK overhead &lt;5%, CLI &lt;1s response) \u2705 1,068 passing tests with 100% coverage on new code \u2705 Ready for PyPI publication</p>"},{"location":"#why-prela","title":"Why Prela?","text":"<p>Building AI agents is hard. Understanding what they're doing is even harder. Prela solves this by providing:</p> <ul> <li>\ud83d\udd0d Automatic Tracing: Zero-code instrumentation for 10 frameworks (OpenAI, Anthropic, LangChain, LlamaIndex, CrewAI, AutoGen, LangGraph, Swarm, n8n)</li> <li>\ud83e\udd16 Multi-Agent Support: First-class support for multi-agent orchestration, conversations, and handoffs</li> <li>\ud83d\udcca Complete Visibility: See every LLM call, tool invocation, agent decision, and state change</li> <li>\ud83e\uddea Built-in Testing: Comprehensive evaluation framework with 17+ assertion types including multi-agent and workflow patterns</li> <li>\ud83d\udd04 Deterministic Replay: Re-execute traces with different models, compare outputs, test tool execution</li> <li>\ud83d\ude80 Production Ready: 1,068 tests, 100% coverage on new code, type-safe, minimal dependencies</li> <li>\u26a1 Minimal Overhead: ~0.5-2ms per span, smart sampling, efficient serialization</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<p>Install Prela:</p> <pre><code>pip install prela\n</code></pre> <p>Add one line to your code:</p> <pre><code>import prela\nfrom anthropic import Anthropic\n\n# Initialize Prela - auto-instruments all LLM SDKs\nprela.init(service_name=\"my-agent\")\n\n# Use your LLM SDK normally - tracing happens automatically!\nclient = Anthropic()\nresponse = client.messages.create(\n    model=\"claude-sonnet-4-20250514\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n\n# Spans are automatically created, traced, and exported \u2728\n</code></pre> <p>That's it! All your LLM calls are now traced with:</p> <ul> <li>Request/response attributes (model, tokens, latency)</li> <li>Tool calls and function invocations</li> <li>Error capture and stack traces</li> <li>Parent-child span relationships</li> </ul>"},{"location":"#architecture-overview","title":"Architecture Overview","text":"<pre><code>graph TB\n    subgraph \"Your Application\"\n        A[Your Code] --&gt;|calls| B[OpenAI SDK]\n        A --&gt;|calls| C[Anthropic SDK]\n        A --&gt;|calls| D[LangChain]\n    end\n\n    subgraph \"Prela SDK\"\n        B --&gt;|auto-instrumented| E[Tracer]\n        C --&gt;|auto-instrumented| E\n        D --&gt;|auto-instrumented| E\n        E --&gt;|creates| F[Spans]\n        F --&gt;|exports to| G[Exporter]\n    end\n\n    subgraph \"Outputs\"\n        G --&gt;|writes| H[Console]\n        G --&gt;|writes| I[Files]\n        G --&gt;|sends| J[OTLP Backend]\n    end\n\n    style E fill:#4F46E5\n    style F fill:#6366F1\n    style G fill:#818CF8</code></pre>"},{"location":"#key-features","title":"Key Features","text":""},{"location":"#auto-instrumentation","title":"\ud83c\udfaf Auto-Instrumentation","text":"<p>Works out of the box with 10 popular frameworks:</p> OpenAIAnthropicLangChainCrewAIAutoGenLangGraphn8n <pre><code>import prela\nimport openai\n\nprela.init()\n\n# Automatically traced\nresponse = openai.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello\"}]\n)\n</code></pre> <pre><code>import prela\nfrom anthropic import Anthropic\n\nprela.init()\n\n# Automatically traced\nclient = Anthropic()\nresponse = client.messages.create(\n    model=\"claude-sonnet-4-20250514\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello\"}]\n)\n</code></pre> <pre><code>import prela\nfrom langchain.agents import initialize_agent\n\nprela.init()\n\n# Automatically traced\nagent = initialize_agent(tools, llm, agent=\"zero-shot-react-description\")\nresult = agent.run(\"What is the weather?\")\n</code></pre> <pre><code>import prela\nfrom crewai import Agent, Task, Crew\n\nprela.init()\n\n# Automatically traced - task delegation, agent collaboration\ncrew = Crew(agents=[researcher, writer], tasks=[research_task, write_task])\nresult = crew.kickoff()\n</code></pre> <pre><code>import prela\nfrom autogen import ConversableAgent\n\nprela.init()\n\n# Automatically traced - conversations, message flow\nuser_proxy.initiate_chat(assistant, message=\"Hello!\")\n</code></pre> <pre><code>import prela\nfrom langgraph.graph import StateGraph\n\nprela.init()\n\n# Automatically traced - state changes, node execution\ngraph = StateGraph(state_schema={...})\ncompiled = graph.compile()\nresult = compiled.invoke(state)\n</code></pre> <pre><code>import prela\n\n# Start webhook receiver for n8n workflows (multi-tenant support)\nprela.init(n8n_webhook_port=8787)\n\n# Configure n8n to send webhooks to http://localhost:8787\n# - All workflow executions, AI nodes, and tool calls automatically traced\n# - Multi-tenant: Monitor multiple n8n instances with project isolation\n# - Real-time dashboard with WebSocket updates (&lt;1s latency)\n# - Webhook URL routing: ?project=prod-n8n or header X-Prela-Project\n</code></pre>"},{"location":"#complete-span-data","title":"\ud83d\udcc8 Complete Span Data","text":"<p>Every span captures:</p> <ul> <li>Timing: Start time, end time, duration</li> <li>Attributes: Model, temperature, max tokens, token usage</li> <li>Events: Request sent, response received, tool calls</li> <li>Errors: Exception type, message, stack trace</li> <li>Context: Trace ID, span ID, parent span ID</li> </ul>"},{"location":"#multi-agent-observability","title":"\ud83e\udd16 Multi-Agent Observability","text":"<p>First-class support for multi-agent systems:</p> <p>CrewAI - Task-based orchestration: - Crew executions with agent collaboration - Task delegation tracking (assigner \u2192 assignee) - Tool usage per agent - Sequential vs hierarchical process modes</p> <p>AutoGen - Conversational agents: - Multi-turn conversation tracking - Message flow (speaker \u2192 recipient) - Function calling with agent context - Turn-by-turn conversation history</p> <p>LangGraph - Stateful workflows: - Graph execution with node tracing - State change detection (what each node modified) - Streaming support with step counting - Support for prebuilt agents (create_react_agent)</p> <p>Swarm - Agent handoffs: - Handoff tracking (initial \u2192 final agent) - Context variable flow (privacy-safe keys) - Deterministic agent IDs - Execution isolation for concurrent runs</p> <p>See Multi-Agent Integrations for detailed documentation.</p>"},{"location":"#evaluation-framework","title":"\ud83e\uddea Evaluation Framework","text":"<p>Test your agents systematically:</p> <pre><code>from prela.evals import EvalSuite, EvalCase, EvalInput, EvalExpected\nfrom prela.evals import EvalRunner\nfrom prela.evals.reporters import ConsoleReporter\n\n# Define test cases\nsuite = EvalSuite(\n    name=\"Customer Support Bot\",\n    cases=[\n        EvalCase(\n            id=\"test_greeting\",\n            name=\"Responds to greeting\",\n            input=EvalInput(query=\"Hello!\"),\n            expected=EvalExpected(contains=[\"Hi\", \"Hello\"]),\n        ),\n    ]\n)\n\n# Run evaluation\nrunner = EvalRunner(suite, agent_function=my_agent)\nresult = runner.run()\n\n# Report results\nConsoleReporter(verbose=True).report(result)\n</code></pre>"},{"location":"#deterministic-replay","title":"\ud83d\udd04 Deterministic Replay","text":"<p>Re-execute captured traces with modifications for testing and experimentation:</p> <pre><code>from prela.replay import ReplayEngine, TraceLoader\n\n# Load captured trace\ntrace = TraceLoader.from_file(\"trace.json\")\n\n# Replay with different model\nengine = ReplayEngine(trace)\nresult = engine.replay_with_modifications(\n    model=\"gpt-4o\",           # Try different model\n    temperature=0.7,          # Adjust parameters\n    enable_tools=True,        # Re-execute tool calls\n    enable_retrieval=True     # Re-execute vector searches\n)\n\n# Compare original vs replayed execution\ncomparison = result.compare_with_original()\nprint(f\"Output similarity: {comparison.semantic_similarity:.2%}\")\nprint(f\"Token difference: {comparison.token_difference}\")\n</code></pre> <p>Replay capabilities: - Switch between models (OpenAI \u2194 Anthropic) - Adjust temperature, max tokens, and other parameters - Re-execute tool calls with allowlist/blocklist controls - Re-execute retrieval operations with vector DB support - Automatic retry logic with exponential backoff - Semantic similarity comparison with fallback methods - Side-by-side diff visualization</p> <p>See Replay Concepts and Replay Examples for detailed documentation.</p>"},{"location":"#multiple-export-formats","title":"\ud83d\udcca Multiple Export Formats","text":"<p>Choose your output:</p> <ul> <li>Console: Pretty-printed JSON for development</li> <li>File: JSONL format for production (with rotation)</li> <li>OTLP: OpenTelemetry protocol for cloud backends</li> </ul>"},{"location":"#what-gets-traced","title":"What Gets Traced?","text":"<pre><code>sequenceDiagram\n    participant App as Your App\n    participant Prela as Prela SDK\n    participant LLM as LLM API\n    participant Export as Exporter\n\n    App-&gt;&gt;Prela: prela.init()\n    Note over Prela: Auto-instruments SDKs\n\n    App-&gt;&gt;LLM: client.messages.create(...)\n    Note over Prela: Intercepts call\n    Prela-&gt;&gt;Prela: Create span\n    Prela-&gt;&gt;Prela: Record request attributes\n    Prela-&gt;&gt;LLM: Forward request\n    LLM-&gt;&gt;Prela: Return response\n    Prela-&gt;&gt;Prela: Record response attributes\n    Prela-&gt;&gt;Prela: Calculate duration\n    Prela-&gt;&gt;Export: Export span\n    Prela-&gt;&gt;App: Return response</code></pre> <p>Captured automatically:</p> <ul> <li>Model name and provider</li> <li>Input/output token counts</li> <li>Request parameters (temperature, max_tokens, etc.)</li> <li>Response metadata (finish_reason, stop_reason)</li> <li>Tool/function calls with arguments</li> <li>Streaming metrics (time-to-first-token)</li> <li>Errors and exceptions</li> </ul>"},{"location":"#use-cases","title":"Use Cases","text":""},{"location":"#debugging","title":"\ud83d\udc1b Debugging","text":"<p>Find out why your agent is misbehaving:</p> <pre><code># List recent traces\nprela list\n\n# Show detailed trace\nprela show trace_abc123\n\n# Search for errors\nprela search --status error\n</code></pre>"},{"location":"#monitoring","title":"\ud83d\udcca Monitoring","text":"<p>Track performance in production:</p> <pre><code>prela.init(\n    service_name=\"production-agent\",\n    exporter=\"file\",\n    directory=\"./traces\",\n    sample_rate=0.1  # Sample 10% of traces\n)\n</code></pre>"},{"location":"#testing","title":"\u2705 Testing","text":"<p>Validate agent behavior:</p> <pre><code># eval_suite.yaml\nname: Agent Test Suite\ncases:\n  - id: test_1\n    name: Booking flow\n    input:\n      query: \"Book a flight to NYC\"\n    expected:\n      contains:\n        - \"flight\"\n        - \"New York\"\n</code></pre> <pre><code>prela eval run eval_suite.yaml --reporter junit --output results.xml\n</code></pre>"},{"location":"#analysis","title":"\ud83d\udd2c Analysis","text":"<p>Export traces for analysis:</p> <pre><code>from prela.evals.reporters import JSONReporter\n\n# Export to JSON for analysis\nJSONReporter(\"analysis/results.json\").report(eval_result)\n</code></pre>"},{"location":"#documentation","title":"Documentation","text":""},{"location":"#getting-started","title":"Getting Started","text":"<ul> <li>Installation - Install Prela</li> <li>Quick Start - First trace in 5 minutes</li> <li>Configuration - Configuration options</li> </ul>"},{"location":"#concepts","title":"Concepts","text":"<ul> <li>Tracing - Distributed tracing fundamentals</li> <li>Spans - Understanding spans</li> <li>Context Propagation - Thread and async safety</li> <li>Sampling - Control trace volume</li> <li>Exporters - Where traces go</li> </ul>"},{"location":"#integrations","title":"Integrations","text":"<p>LLM Providers: - OpenAI - GPT-3.5, GPT-4, embeddings - Anthropic - Claude models</p> <p>Agent Frameworks: - LangChain - Chains and agents - LlamaIndex - Query engines</p> <p>Multi-Agent Frameworks: - CrewAI - Task-based orchestration - AutoGen - Conversational agents - LangGraph - Stateful workflows - Swarm - Agent handoffs</p> <p>Workflow Automation: - n8n - Webhook-based workflow tracing - n8n Code Nodes - Python code instrumentation</p>"},{"location":"#evaluation","title":"Evaluation","text":"<ul> <li>Overview - Testing framework</li> <li>Writing Tests - Create test cases</li> <li>Assertions - 10 assertion types for agent outputs</li> <li>Multi-Agent Assertions - 7 assertion types for multi-agent systems</li> <li>N8N Workflow Evaluation - Test n8n workflows systematically</li> <li>Running Evaluations - Execution modes</li> <li>CI Integration - GitHub Actions, Jenkins</li> </ul>"},{"location":"#replay","title":"Replay","text":"<ul> <li>Replay Concepts - Deterministic replay fundamentals</li> <li>Replay Advanced - Tool/retrieval re-execution</li> <li>Replay Examples - Basic replay scenarios</li> <li>Replay Multi-Agent - Multi-agent replay patterns</li> <li>Replay with Tools - Tool re-execution examples</li> </ul>"},{"location":"#community-support","title":"Community &amp; Support","text":"<ul> <li>GitHub: github.com/garrettw2200/prela-sdk</li> <li>PyPI: pypi.org/project/prela</li> <li>Issues: Report bugs or request features</li> <li>Discussions: Ask questions and share ideas</li> <li>Discord: Join our community</li> <li>License: Apache 2.0 (Open Source)</li> </ul>"},{"location":"#whats-next","title":"What's Next?","text":"<ul> <li> <p> Install Prela</p> <p>Get started with Prela in minutes</p> <p> Installation guide</p> </li> <li> <p> Quick Start</p> <p>Your first trace in 5 minutes</p> <p> Quick start</p> </li> <li> <p> Learn Concepts</p> <p>Understand how Prela works</p> <p> Concepts</p> </li> <li> <p> API Reference</p> <p>Detailed API documentation</p> <p> API docs</p> </li> </ul>"},{"location":"changelog/","title":"Changelog","text":"<p>All notable changes to Prela will be documented in this file.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"changelog/#unreleased","title":"Unreleased","text":""},{"location":"changelog/#changed","title":"Changed","text":"<ul> <li>Repository Split: SDK moved to separate repository at github.com/garrettw2200/prela-sdk</li> <li>PyPI Publication: SDK now available on PyPI at pypi.org/project/prela</li> <li>Installation: Primary installation method is now <code>pip install prela</code></li> </ul>"},{"location":"changelog/#added","title":"Added","text":"<ul> <li>Production Validation (Phase 6):</li> <li>21/21 core features validated with real API calls</li> <li>6 production test scenarios with complete validation evidence</li> <li>Performance validation: SDK overhead &lt;5%, CLI response &lt;1s</li> <li>Documentation validation: All scenarios documented with expected outputs</li> <li>Test scenarios copied to examples directory with comprehensive README</li> <li>Comprehensive documentation site</li> <li>Production deployment examples</li> <li>Multi-environment configuration guides</li> </ul>"},{"location":"changelog/#020-2025-01-26","title":"0.2.0 - 2025-01-26","text":""},{"location":"changelog/#added_1","title":"Added","text":"<ul> <li>Evaluation Framework: Complete testing framework for AI agents</li> <li>EvalCase and EvalSuite for test definition</li> <li>10 assertion types (structural, tool, semantic)</li> <li>EvalRunner with sequential and parallel execution</li> <li>Three reporters: Console, JSON, JUnit</li> <li>YAML/JSON test suite support</li> <li>CI/CD integration guides</li> <li>CLI Tool: Command-line interface for trace management</li> <li><code>prela init</code> - Initialize new projects</li> <li><code>prela list</code> - List available traces</li> <li><code>prela show</code> - Display specific traces</li> <li><code>prela search</code> - Search traces by attributes</li> <li><code>prela eval run</code> - Run evaluation suites</li> <li><code>prela export</code> - Export traces to different formats</li> <li>Enhanced FileExporter:</li> <li>Tree-based directory structure (by service/date)</li> <li>Trace search and filtering</li> <li>File rotation by size</li> <li>Improved organization</li> <li>Enhanced ConsoleExporter:</li> <li>Three verbosity levels (minimal, normal, verbose)</li> <li>Colored output with rich library support</li> <li>Tree visualization for nested spans</li> <li>Configurable formatting</li> </ul>"},{"location":"changelog/#changed_1","title":"Changed","text":"<ul> <li>ConsoleExporter API: Changed <code>quiet</code> parameter to <code>verbosity</code></li> <li>FileExporter API: Changed from single file to directory-based organization</li> </ul>"},{"location":"changelog/#fixed","title":"Fixed","text":"<ul> <li>Context propagation in thread pools</li> <li>Timing precision in latency assertions</li> </ul>"},{"location":"changelog/#010-2025-01-20","title":"0.1.0 - 2025-01-20","text":""},{"location":"changelog/#added_2","title":"Added","text":"<ul> <li>Core Tracing: Complete span and context system</li> <li>Span class with immutability after end()</li> <li>SpanType enum (AGENT, LLM, TOOL, RETRIEVAL, EMBEDDING, CUSTOM)</li> <li>SpanStatus enum (PENDING, SUCCESS, ERROR)</li> <li>SpanEvent for timestamped occurrences</li> <li>High-resolution clock utilities</li> <li>Thread-safe and async-safe context propagation</li> <li>Tracer: Main orchestration class</li> <li>Context manager interface for spans</li> <li>Automatic parent-child linking</li> <li>Global tracer management</li> <li>Service name injection</li> <li>Sampling: Four sampling strategies</li> <li>AlwaysOnSampler (development)</li> <li>AlwaysOffSampler (disable tracing)</li> <li>ProbabilitySampler (probabilistic sampling)</li> <li>RateLimitingSampler (token bucket rate limiting)</li> <li>Exporters: Base export system</li> <li>BaseExporter abstract class</li> <li>BatchExporter with retry logic</li> <li>ConsoleExporter for development</li> <li>FileExporter for production (JSONL format)</li> <li>Exponential backoff retry</li> <li>Auto-Instrumentation: Automatic SDK tracing</li> <li>OpenAI SDK support (chat, completions, embeddings)</li> <li>Anthropic SDK support (messages, streaming, tools)</li> <li>LangChain integration (chains, agents, tools)</li> <li>Auto-discovery and registration</li> <li>Public API: Simple initialization</li> <li><code>prela.init()</code> - One-line setup</li> <li><code>prela.get_tracer()</code> - Access global tracer</li> <li><code>prela.auto_instrument()</code> - Manual instrumentation</li> <li>Environment variable support</li> </ul>"},{"location":"changelog/#features","title":"Features","text":"<ul> <li>OpenAI Instrumentation:</li> <li>Sync and async chat completions</li> <li>Streaming responses</li> <li>Function/tool calling</li> <li>Embeddings API</li> <li>Legacy completions</li> <li>Token usage tracking</li> <li>Error capturing</li> <li>Anthropic Instrumentation:</li> <li>Sync and async messages</li> <li>Streaming responses</li> <li>Tool use detection</li> <li>Extended thinking capture</li> <li>Token usage tracking</li> <li>Error capturing</li> <li>LangChain Instrumentation:</li> <li>Chain executions (LLMChain, SequentialChain)</li> <li>Agent workflows</li> <li>Tool invocations</li> <li>Retriever queries</li> <li>Callback-based integration</li> </ul>"},{"location":"changelog/#performance","title":"Performance","text":"<ul> <li><code>__slots__</code> for memory efficiency</li> <li>Lazy serialization</li> <li>Minimal overhead (&lt;100\u03bcs per span)</li> <li>Thread-safe by design</li> <li>Async-compatible</li> </ul>"},{"location":"changelog/#testing","title":"Testing","text":"<ul> <li>573 comprehensive tests</li> <li>95%+ code coverage</li> <li>Unit, integration, and edge case tests</li> <li>Thread safety validation</li> <li>Async support validation</li> </ul>"},{"location":"changelog/#001-2025-01-15","title":"0.0.1 - 2025-01-15","text":""},{"location":"changelog/#added_3","title":"Added","text":"<ul> <li>Initial project structure</li> <li>Basic span implementation (prototype)</li> <li>Proof of concept</li> </ul>"},{"location":"changelog/#version-history","title":"Version History","text":"<ul> <li>0.2.0: Evaluation framework, CLI tool, enhanced exporters</li> <li>0.1.0: Core tracing, auto-instrumentation, public API</li> <li>0.0.1: Initial prototype</li> </ul>"},{"location":"changelog/#migration-guides","title":"Migration Guides","text":""},{"location":"changelog/#migrating-from-010-to-020","title":"Migrating from 0.1.0 to 0.2.0","text":""},{"location":"changelog/#consoleexporter-changes","title":"ConsoleExporter Changes","text":"<pre><code># Old (0.1.0)\nConsoleExporter(quiet=False)\n\n# New (0.2.0)\nConsoleExporter(verbosity=\"normal\")  # or \"minimal\", \"verbose\"\n</code></pre>"},{"location":"changelog/#fileexporter-changes","title":"FileExporter Changes","text":"<pre><code># Old (0.1.0)\nFileExporter(file_path=\"traces.jsonl\")\n\n# New (0.2.0)\nFileExporter(directory=\"./traces\")  # Organized by service/date\n</code></pre>"},{"location":"changelog/#deprecation-notices","title":"Deprecation Notices","text":"<p>None currently.</p>"},{"location":"changelog/#security","title":"Security","text":"<p>For security vulnerabilities, please email security@prela.dev instead of using the issue tracker.</p>"},{"location":"changelog/#links","title":"Links","text":"<ul> <li>Documentation</li> <li>GitHub Repository</li> <li>PyPI Package</li> <li>GitHub Discussions</li> <li>Discord Community</li> </ul>"},{"location":"contributing/","title":"Contributing to Prela","text":"<p>Thank you for your interest in contributing to Prela! This guide will help you get started.</p>"},{"location":"contributing/#development-setup","title":"Development Setup","text":""},{"location":"contributing/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.9+</li> <li>Git</li> <li>pip and virtualenv</li> </ul>"},{"location":"contributing/#clone-the-repository","title":"Clone the Repository","text":"<pre><code>git clone https://github.com/garrettw2200/prela-sdk.git\ncd prela-sdk\n</code></pre>"},{"location":"contributing/#setup-development-environment","title":"Setup Development Environment","text":"<pre><code># Create virtual environment\npython -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n\n# Install dependencies\npip install -e \".[dev,all]\"\n</code></pre>"},{"location":"contributing/#run-tests","title":"Run Tests","text":"<pre><code># Run all tests\npytest\n\n# Run with coverage\npytest --cov=prela --cov-report=html\n\n# Run specific test file\npytest tests/test_span.py\n\n# Run specific test\npytest tests/test_span.py::test_span_creation\n</code></pre>"},{"location":"contributing/#code-quality","title":"Code Quality","text":""},{"location":"contributing/#formatting","title":"Formatting","text":"<p>We use <code>black</code> for code formatting:</p> <pre><code># Format all files\nblack prela tests\n\n# Check formatting\nblack --check prela tests\n</code></pre>"},{"location":"contributing/#linting","title":"Linting","text":"<p>We use <code>ruff</code> for linting:</p> <pre><code># Lint code\nruff check prela tests\n\n# Auto-fix issues\nruff check --fix prela tests\n</code></pre>"},{"location":"contributing/#type-checking","title":"Type Checking","text":"<p>We use <code>mypy</code> for type checking:</p> <pre><code># Type check\nmypy prela\n</code></pre>"},{"location":"contributing/#making-changes","title":"Making Changes","text":""},{"location":"contributing/#create-a-branch","title":"Create a Branch","text":"<pre><code>git checkout -b feature/your-feature-name\n# or\ngit checkout -b fix/issue-description\n</code></pre>"},{"location":"contributing/#commit-guidelines","title":"Commit Guidelines","text":"<p>Use conventional commit messages:</p> <pre><code>feat: add support for custom exporters\nfix: resolve context propagation in thread pools\ndocs: update installation guide\ntest: add tests for span serialization\nrefactor: simplify sampler interface\n</code></pre>"},{"location":"contributing/#write-tests","title":"Write Tests","text":"<p>All new features and bug fixes should include tests:</p> <pre><code># tests/test_your_feature.py\nimport pytest\nfrom prela import Span, SpanType\n\ndef test_your_feature():\n    \"\"\"Test description.\"\"\"\n    span = Span(name=\"test\", span_type=SpanType.CUSTOM)\n    # Test assertions\n    assert span.name == \"test\"\n</code></pre>"},{"location":"contributing/#update-documentation","title":"Update Documentation","text":"<p>Update relevant documentation:</p> <ul> <li>API docstrings</li> <li>Usage examples</li> <li>README updates</li> <li>Migration guides (if breaking changes)</li> </ul>"},{"location":"contributing/#pull-request-process","title":"Pull Request Process","text":""},{"location":"contributing/#before-submitting","title":"Before Submitting","text":"<ol> <li>Run all tests: <code>pytest</code></li> <li>Check coverage: <code>pytest --cov=prela</code></li> <li>Format code: <code>black prela tests</code></li> <li>Lint code: <code>ruff check prela tests</code></li> <li>Type check: <code>mypy prela</code></li> <li>Update CHANGELOG.md</li> </ol>"},{"location":"contributing/#submitting-pr","title":"Submitting PR","text":"<ol> <li>Push your branch to GitHub</li> <li>Create a pull request</li> <li>Fill out the PR template</li> <li>Link related issues</li> <li>Request review</li> </ol>"},{"location":"contributing/#pr-checklist","title":"PR Checklist","text":"<ul> <li> Tests pass</li> <li> Code coverage maintained/improved</li> <li> Documentation updated</li> <li> CHANGELOG.md updated</li> <li> Commit messages follow conventions</li> <li> Code formatted with black</li> <li> No linting errors</li> <li> Type hints added</li> </ul>"},{"location":"contributing/#development-guidelines","title":"Development Guidelines","text":""},{"location":"contributing/#code-style","title":"Code Style","text":"<ul> <li>Use type hints for all functions</li> <li>Write docstrings in Google style</li> <li>Keep functions focused and small</li> <li>Prefer composition over inheritance</li> </ul>"},{"location":"contributing/#testing","title":"Testing","text":"<ul> <li>Aim for 100% test coverage</li> <li>Test edge cases</li> <li>Test error handling</li> <li>Use descriptive test names</li> </ul>"},{"location":"contributing/#documentation","title":"Documentation","text":"<ul> <li>Document all public APIs</li> <li>Include usage examples</li> <li>Keep docs up to date</li> <li>Use clear, concise language</li> </ul>"},{"location":"contributing/#project-structure","title":"Project Structure","text":"<pre><code>prela-sdk/\n\u251c\u2500\u2500 prela/              # Source code\n\u2502   \u251c\u2500\u2500 core/           # Core tracing\n\u2502   \u251c\u2500\u2500 exporters/      # Export backends\n\u2502   \u251c\u2500\u2500 instrumentation/  # Auto-instrumentation\n\u2502   \u251c\u2500\u2500 evals/          # Evaluation framework\n\u2502   \u2514\u2500\u2500 contrib/        # CLI and extras\n\u251c\u2500\u2500 tests/              # Test suite\n\u251c\u2500\u2500 examples/           # Example scripts\n\u251c\u2500\u2500 docs/               # Documentation\n\u2514\u2500\u2500 .github/            # GitHub workflows\n</code></pre>"},{"location":"contributing/#release-process","title":"Release Process","text":"<p>Maintainers will handle releases. Process:</p> <ol> <li>Update version in <code>_version.py</code></li> <li>Update CHANGELOG.md</li> <li>Create git tag</li> <li>Build package: <code>python -m build</code></li> <li>Publish to PyPI: <code>twine upload dist/*</code></li> </ol>"},{"location":"contributing/#getting-help","title":"Getting Help","text":"<ul> <li>\ud83d\udcac Discord Community</li> <li>\ud83d\udc1b GitHub Issues</li> <li>\ud83d\udca1 GitHub Discussions</li> </ul>"},{"location":"contributing/#code-of-conduct","title":"Code of Conduct","text":""},{"location":"contributing/#our-standards","title":"Our Standards","text":"<ul> <li>Be respectful and inclusive</li> <li>Welcome newcomers</li> <li>Accept constructive criticism</li> <li>Focus on what's best for the community</li> </ul>"},{"location":"contributing/#unacceptable-behavior","title":"Unacceptable Behavior","text":"<ul> <li>Harassment or discrimination</li> <li>Trolling or insulting comments</li> <li>Personal or political attacks</li> <li>Publishing others' private information</li> </ul>"},{"location":"contributing/#enforcement","title":"Enforcement","text":"<p>Violations may result in temporary or permanent ban from project participation.</p>"},{"location":"contributing/#license","title":"License","text":"<p>By contributing, you agree that your contributions will be licensed under the MIT License.</p>"},{"location":"contributing/#recognition","title":"Recognition","text":"<p>Contributors will be recognized in:</p> <ul> <li>CONTRIBUTORS.md</li> <li>Release notes</li> <li>Documentation credits</li> </ul>"},{"location":"contributing/#thank-you","title":"Thank You!","text":"<p>Your contributions make Prela better for everyone. Thank you for being part of our community!</p>"},{"location":"api/core/","title":"Core API","text":"<p>The core API provides the fundamental building blocks for tracing AI agent applications.</p>"},{"location":"api/core/#prelainit","title":"prela.init()","text":""},{"location":"api/core/#prela.init","title":"<code>prela.init(service_name=None, exporter=None, auto_instrument=True, sample_rate=None, capture_for_replay=False, project_id=None, n8n_webhook_port=None, n8n_webhook_host='0.0.0.0', **kwargs)</code>","text":"<p>Initialize Prela tracing with one line of code.</p> <p>This is the primary entry point for the Prela SDK. It: 1. Creates a tracer with the specified configuration 2. Sets it as the global tracer 3. Auto-instruments detected LLM SDKs (Anthropic, OpenAI, etc.) 4. Optionally starts n8n webhook receiver for zero-code workflow tracing 5. Returns the tracer for manual span creation</p> <p>After calling init(), all LLM SDK calls are automatically traced!</p> <p>Parameters:</p> Name Type Description Default <code>service_name</code> <code>str | None</code> <p>Name of your service (default: $PRELA_SERVICE_NAME or \"default\")</p> <code>None</code> <code>exporter</code> <code>str | BaseExporter | None</code> <p>Where to send traces: - \"console\": Pretty-print to console (default) - \"file\": Write to JSONL file - \"http\": Send to HTTP endpoint (Railway, cloud backend) - BaseExporter instance: Custom exporter - Default: $PRELA_EXPORTER or \"console\"</p> <code>None</code> <code>auto_instrument</code> <code>bool</code> <p>Whether to auto-instrument detected libraries (default: True, disable with $PRELA_AUTO_INSTRUMENT=false)</p> <code>True</code> <code>sample_rate</code> <code>float | None</code> <p>Sampling rate 0.0-1.0 (default: $PRELA_SAMPLE_RATE or 1.0)</p> <code>None</code> <code>capture_for_replay</code> <code>bool</code> <p>Enable full replay data capture (default: False) When enabled, captures complete request/response data including: - LLM: Full prompts, responses, streaming chunks, model info - Tools: Input args, output, side effects flag - Retrieval: Queries, documents, scores, metadata - Agents: System prompts, available tools, memory, config Use for debugging, testing, and auditing. Increases storage costs.</p> <code>False</code> <code>project_id</code> <code>str | None</code> <p>Project ID for multi-tenant deployments (default: $PRELA_PROJECT_ID or None) Used for: - Organizing traces in multi-project deployments - Filtering dashboards by project - n8n webhook routing (?project={project_id})</p> <code>None</code> <code>n8n_webhook_port</code> <code>int | None</code> <p>Port for n8n webhook receiver (optional, default: None) Set to enable n8n webhook-based tracing (e.g., 8787) Example: n8n_webhook_port=8787</p> <code>None</code> <code>n8n_webhook_host</code> <code>str</code> <p>Host for n8n webhook receiver (default: \"0.0.0.0\") Usually \"0.0.0.0\" for accepting external connections</p> <code>'0.0.0.0'</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments passed to exporter For ConsoleExporter: verbosity, color, show_timestamps For FileExporter: directory, format, max_file_size_mb, rotate For HTTPExporter: endpoint, api_key, bearer_token, compress, headers</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tracer</code> <p>Configured Tracer instance (also set as global tracer)</p> Environment Variables <p>PRELA_SERVICE_NAME: Default service name PRELA_PROJECT_ID: Default project ID for multi-tenant setups PRELA_EXPORTER: Default exporter (\"console\", \"file\", or \"http\") PRELA_SAMPLE_RATE: Default sampling rate (0.0-1.0) PRELA_CAPTURE_REPLAY: Enable replay capture (\"true\", \"1\", or \"yes\") PRELA_AUTO_INSTRUMENT: Enable auto-instrumentation (\"true\" or \"false\") PRELA_DEBUG: Enable debug logging (\"true\" or \"false\") PRELA_TRACE_DIR: Directory for file exporter (default: ./traces) PRELA_HTTP_ENDPOINT: HTTP endpoint for http exporter PRELA_API_KEY: API key for http exporter PRELA_N8N_WEBHOOK_PORT: Port for n8n webhook receiver (optional)</p> Example <pre><code>import prela\n\n# Simple initialization\nprela.init(service_name=\"my-agent\")\n\n# All Anthropic/OpenAI calls now auto-traced!\nfrom anthropic import Anthropic\nclient = Anthropic()\nresponse = client.messages.create(\n    model=\"claude-sonnet-4-20250514\",\n    max_tokens=1024,\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n# Trace is automatically captured and exported\n\n# Manual span creation\nwith prela.get_tracer().span(\"custom_operation\") as span:\n    span.set_attribute(\"key\", \"value\")\n    # Do work...\n</code></pre> <p>Example with console exporter (verbose mode):     <pre><code>import prela\n\nprela.init(\n    service_name=\"my-agent\",\n    exporter=\"console\",\n    verbosity=\"verbose\",  # \"minimal\", \"normal\", or \"verbose\"\n    color=True,\n    show_timestamps=True\n)\n</code></pre></p> Example with file exporter <pre><code>import prela\n\nprela.init(\n    service_name=\"my-agent\",\n    exporter=\"file\",\n    directory=\"./traces\",\n    max_file_size_mb=100,  # 100 MB per file\n    rotate=True\n)\n</code></pre> <p>Example with HTTP exporter (Railway deployment):     <pre><code>import prela\n\nprela.init(\n    service_name=\"my-agent\",\n    exporter=\"http\",\n    endpoint=\"https://prela-ingest-gateway-xxx.railway.app/v1/traces\",\n    api_key=\"your-api-key\",  # Optional\n    compress=True  # Enable gzip compression\n)\n</code></pre></p> Example with n8n webhook receiver <pre><code>import prela\n\n# Start webhook receiver on port 8787\nprela.init(\n    service_name=\"n8n-workflows\",\n    exporter=\"http\",\n    endpoint=\"https://prela-ingest-gateway-xxx.railway.app/v1/traces\",\n    n8n_webhook_port=8787\n)\n\n# Now configure n8n HTTP Request node to POST to:\n# http://your-server:8787/webhook\n# Body: {\"workflow\": \"{{ $workflow }}\", \"execution\": \"{{ $execution }}\", ...}\n</code></pre> Example with custom exporter <pre><code>from prela import init, BaseExporter, ExportResult\n\nclass MyExporter(BaseExporter):\n    def export(self, spans):\n        # Send to your backend\n        return ExportResult.SUCCESS\n\ninit(service_name=\"my-agent\", exporter=MyExporter())\n</code></pre>"},{"location":"api/core/#tracer","title":"Tracer","text":""},{"location":"api/core/#prela.core.tracer.Tracer","title":"<code>prela.core.tracer.Tracer</code>","text":"<p>Main tracer for creating and managing spans.</p> <p>The Tracer is responsible for: - Creating spans with proper trace/span IDs - Managing trace context and span hierarchies - Applying sampling decisions - Exporting completed spans</p> Example <pre><code>from prela.core.tracer import Tracer\nfrom prela.exporters.console import ConsoleExporter\n\ntracer = Tracer(\n    service_name=\"my-agent\",\n    exporter=ConsoleExporter()\n)\n\n# Create spans using context manager\nwith tracer.span(\"operation\") as span:\n    span.set_attribute(\"key\", \"value\")\n    # Nested spans inherit trace context\n    with tracer.span(\"sub-operation\") as child:\n        child.set_attribute(\"nested\", True)\n</code></pre>"},{"location":"api/core/#prela.core.tracer.Tracer-functions","title":"Functions","text":""},{"location":"api/core/#prela.core.tracer.Tracer.__init__","title":"<code>__init__(service_name='default', exporter=None, sampler=None, capture_for_replay=False)</code>","text":"<p>Initialize a tracer.</p> <p>Parameters:</p> Name Type Description Default <code>service_name</code> <code>str</code> <p>Name of the service (added to all spans as service.name)</p> <code>'default'</code> <code>exporter</code> <code>BaseExporter | None</code> <p>Exporter for sending spans to backend (None = no export)</p> <code>None</code> <code>sampler</code> <code>BaseSampler | None</code> <p>Sampler for controlling trace volume (default: AlwaysOnSampler)</p> <code>None</code> <code>capture_for_replay</code> <code>bool</code> <p>If True, capture full replay data (default: False)</p> <code>False</code>"},{"location":"api/core/#prela.core.tracer.Tracer.span","title":"<code>span(name, span_type=SpanType.CUSTOM, attributes=None)</code>","text":"<p>Create a new span as a context manager.</p> <p>The span is automatically: - Started when entering the context - Ended when exiting the context - Exported if it's a root span and sampling decision is True - Linked to parent span if one exists in current context</p> <p>Exceptions are automatically captured and recorded on the span.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the span (e.g., \"process_request\", \"llm_call\")</p> required <code>span_type</code> <code>SpanType</code> <p>Type of operation (LLM, TOOL, AGENT, etc.)</p> <code>CUSTOM</code> <code>attributes</code> <code>dict[str, Any] | None</code> <p>Initial attributes to set on the span</p> <code>None</code> <p>Yields:</p> Name Type Description <code>Span</code> <code>Span</code> <p>The created span (can be used to add attributes/events)</p> Example <pre><code>with tracer.span(\"database_query\", SpanType.CUSTOM) as span:\n    span.set_attribute(\"query\", \"SELECT * FROM users\")\n    result = execute_query()\n    span.set_attribute(\"row_count\", len(result))\n</code></pre>"},{"location":"api/core/#prela.core.tracer.Tracer.set_global","title":"<code>set_global()</code>","text":"<p>Set this tracer as the global default.</p> <p>After calling this, get_tracer() will return this tracer instance. This is useful for auto-instrumentation where instrumentors need access to a tracer without explicit passing.</p> Example <pre><code>tracer = Tracer(service_name=\"my-app\")\ntracer.set_global()\n\n# Later, from anywhere in the code\nfrom prela.core.tracer import get_tracer\ntracer = get_tracer()\n</code></pre>"},{"location":"api/core/#span","title":"Span","text":""},{"location":"api/core/#prela.core.span.Span","title":"<code>prela.core.span.Span</code>","text":"<p>A span represents a unit of work in a distributed trace.</p> <p>Spans are immutable after being ended. Any attempt to modify an ended span will raise a RuntimeError.</p>"},{"location":"api/core/#prela.core.span.Span-functions","title":"Functions","text":""},{"location":"api/core/#prela.core.span.Span.__init__","title":"<code>__init__(span_id=None, trace_id=None, parent_span_id=None, name='', span_type=SpanType.CUSTOM, started_at=None, ended_at=None, status=SpanStatus.PENDING, status_message=None, attributes=None, events=None, _ended=False, replay_snapshot=None)</code>","text":"<p>Initialize a new span.</p> <p>Parameters:</p> Name Type Description Default <code>span_id</code> <code>str | None</code> <p>Unique identifier for this span (generates UUID if not provided)</p> <code>None</code> <code>trace_id</code> <code>str | None</code> <p>Trace ID this span belongs to (generates UUID if not provided)</p> <code>None</code> <code>parent_span_id</code> <code>str | None</code> <p>Parent span ID if this is a child span</p> <code>None</code> <code>name</code> <code>str</code> <p>Human-readable name for this span</p> <code>''</code> <code>span_type</code> <code>SpanType</code> <p>Type of operation this span represents</p> <code>CUSTOM</code> <code>started_at</code> <code>datetime | None</code> <p>When the span started (uses current time if not provided)</p> <code>None</code> <code>ended_at</code> <code>datetime | None</code> <p>When the span ended (None if still running)</p> <code>None</code> <code>status</code> <code>SpanStatus</code> <p>Current status of the span</p> <code>PENDING</code> <code>status_message</code> <code>str | None</code> <p>Optional message describing the status</p> <code>None</code> <code>attributes</code> <code>dict[str, Any] | None</code> <p>Key-value pairs of metadata</p> <code>None</code> <code>events</code> <code>list[SpanEvent] | None</code> <p>List of events that occurred during span execution</p> <code>None</code> <code>_ended</code> <code>bool</code> <p>Internal flag for immutability (do not set manually)</p> <code>False</code> <code>replay_snapshot</code> <code>Any</code> <p>Optional replay data for deterministic re-execution</p> <code>None</code>"},{"location":"api/core/#prela.core.span.Span.end","title":"<code>end(end_time=None)</code>","text":"<p>End the span.</p> <p>Parameters:</p> Name Type Description Default <code>end_time</code> <code>datetime | None</code> <p>When the span ended (uses current time if not provided)</p> <code>None</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the span has already ended</p>"},{"location":"api/core/#prela.core.span.Span.set_attribute","title":"<code>set_attribute(key, value)</code>","text":"<p>Set an attribute on the span.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Attribute key</p> required <code>value</code> <code>Any</code> <p>Attribute value</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the span has already ended</p>"},{"location":"api/core/#prela.core.span.Span.add_event","title":"<code>add_event(name, attributes=None)</code>","text":"<p>Add an event to the span.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Event name</p> required <code>attributes</code> <code>dict[str, Any] | None</code> <p>Optional event attributes</p> <code>None</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the span has already ended</p>"},{"location":"api/core/#prela.core.span.Span.to_dict","title":"<code>to_dict()</code>","text":"<p>Convert span to dictionary representation.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary containing all span data</p>"},{"location":"api/core/#prela.core.span.Span.from_dict","title":"<code>from_dict(data)</code>  <code>classmethod</code>","text":"<p>Create span from dictionary representation.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict[str, Any]</code> <p>Dictionary containing span data</p> required <p>Returns:</p> Type Description <code>Span</code> <p>Reconstructed Span instance</p>"},{"location":"api/core/#spanevent","title":"SpanEvent","text":""},{"location":"api/core/#prela.core.span.SpanEvent","title":"<code>prela.core.span.SpanEvent</code>  <code>dataclass</code>","text":"<p>An event that occurred during a span's execution.</p>"},{"location":"api/core/#prela.core.span.SpanEvent-functions","title":"Functions","text":""},{"location":"api/core/#prela.core.span.SpanEvent.to_dict","title":"<code>to_dict()</code>","text":"<p>Convert event to dictionary representation.</p>"},{"location":"api/core/#prela.core.span.SpanEvent.from_dict","title":"<code>from_dict(data)</code>  <code>classmethod</code>","text":"<p>Create event from dictionary representation.</p>"},{"location":"api/core/#spantype","title":"SpanType","text":""},{"location":"api/core/#prela.core.span.SpanType","title":"<code>prela.core.span.SpanType</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Type of span in the trace.</p>"},{"location":"api/core/#spanstatus","title":"SpanStatus","text":""},{"location":"api/core/#prela.core.span.SpanStatus","title":"<code>prela.core.span.SpanStatus</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Status of a span.</p>"},{"location":"api/core/#context-management","title":"Context Management","text":""},{"location":"api/core/#prela.core.context.get_current_context","title":"<code>prela.core.context.get_current_context()</code>","text":"<p>Get the current trace context.</p> <p>Returns:</p> Type Description <code>TraceContext | None</code> <p>The active trace context, or None if no context is active</p>"},{"location":"api/core/#prela.core.context.get_current_span","title":"<code>prela.core.context.get_current_span()</code>","text":"<p>Get the currently active span.</p> <p>Returns:</p> Type Description <code>Span | None</code> <p>The active span, or None if no context or no active span</p>"},{"location":"api/core/#prela.core.context.new_trace_context","title":"<code>prela.core.context.new_trace_context(trace_id=None, sampled=True, baggage=None)</code>","text":"<p>Create a new trace context for the duration of the context manager.</p> <p>This context manager creates a new trace context, sets it as active, yields it for use, and automatically resets it on exit.</p> <p>Parameters:</p> Name Type Description Default <code>trace_id</code> <code>str | None</code> <p>Unique identifier for this trace (generates UUID if not provided)</p> <code>None</code> <code>sampled</code> <code>bool</code> <p>Whether this trace should be sampled/recorded</p> <code>True</code> <code>baggage</code> <code>dict[str, str] | None</code> <p>Initial baggage metadata to propagate</p> <code>None</code> <p>Yields:</p> Type Description <code>TraceContext</code> <p>The newly created trace context</p> Example <p>with new_trace_context() as ctx: ...     span = Span(name=\"operation\", trace_id=ctx.trace_id) ...     ctx.push_span(span) ...     # Do work ...     ctx.pop_span()</p>"},{"location":"api/core/#prela.core.context.copy_context_to_thread","title":"<code>prela.core.context.copy_context_to_thread(func)</code>","text":"<p>Create a wrapper that copies the current context to a new thread.</p> <p>This function captures the current contextvars context at call time and creates a wrapper that will run the function in that context. This is essential for maintaining trace continuity when using thread pools.</p> <p>IMPORTANT: Call this function INSIDE the context you want to propagate, BEFORE submitting to the thread pool.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable[..., Any]</code> <p>The function to wrap</p> required <p>Returns:</p> Type Description <code>Callable[..., Any]</code> <p>Wrapped function that will run in the captured context</p> Example <p>def background_task(): ...     span = get_current_span() ...     print(f\"Span: {span}\")</p> <p>with new_trace_context() as ctx: ...     # Capture context NOW, before submitting to pool ...     wrapped = copy_context_to_thread(background_task) ...     with ThreadPoolExecutor() as executor: ...         future = executor.submit(wrapped) ...         future.result()</p>"},{"location":"api/core/#prela.core.context.get_current_trace_id","title":"<code>prela.core.context.get_current_trace_id()</code>","text":"<p>Get the current trace ID.</p> <p>Returns:</p> Type Description <code>str | None</code> <p>The active trace ID, or None if no context is active</p>"},{"location":"api/core/#prela.core.context.set_context","title":"<code>prela.core.context.set_context(ctx)</code>","text":"<p>Set the current trace context.</p> <p>Parameters:</p> Name Type Description Default <code>ctx</code> <code>TraceContext</code> <p>The trace context to set as active</p> required <p>Returns:</p> Type Description <code>Token[TraceContext | None]</code> <p>A token that can be used to reset the context</p>"},{"location":"api/core/#prela.core.context.reset_context","title":"<code>prela.core.context.reset_context(token)</code>","text":"<p>Reset the context to its previous value.</p> <p>Parameters:</p> Name Type Description Default <code>token</code> <code>Token[TraceContext | None]</code> <p>The token returned by set_context</p> required"},{"location":"api/core/#tracecontext","title":"TraceContext","text":""},{"location":"api/core/#prela.core.context.TraceContext","title":"<code>prela.core.context.TraceContext</code>","text":"<p>A trace context manages the current trace and span stack.</p> <p>This class maintains the active trace ID, a stack of active spans, and baggage (inherited metadata) that propagates through the trace.</p>"},{"location":"api/core/#prela.core.context.TraceContext-functions","title":"Functions","text":""},{"location":"api/core/#prela.core.context.TraceContext.__init__","title":"<code>__init__(trace_id=None, sampled=True, baggage=None)</code>","text":"<p>Initialize a new trace context.</p> <p>Parameters:</p> Name Type Description Default <code>trace_id</code> <code>str | None</code> <p>Unique identifier for this trace (generates UUID if not provided)</p> <code>None</code> <code>sampled</code> <code>bool</code> <p>Whether this trace should be sampled/recorded</p> <code>True</code> <code>baggage</code> <code>dict[str, str] | None</code> <p>Initial baggage metadata to propagate</p> <code>None</code>"},{"location":"api/core/#prela.core.context.TraceContext.push_span","title":"<code>push_span(span)</code>","text":"<p>Push a span onto the stack.</p> <p>Parameters:</p> Name Type Description Default <code>span</code> <code>Span</code> <p>The span to make active</p> required"},{"location":"api/core/#prela.core.context.TraceContext.pop_span","title":"<code>pop_span()</code>","text":"<p>Pop the current span from the stack.</p> <p>Returns:</p> Type Description <code>Span | None</code> <p>The popped span, or None if stack was empty</p>"},{"location":"api/core/#prela.core.context.TraceContext.current_span","title":"<code>current_span()</code>","text":"<p>Get the currently active span.</p> <p>Returns:</p> Type Description <code>Span | None</code> <p>The span at the top of the stack, or None if stack is empty</p>"},{"location":"api/core/#sampling","title":"Sampling","text":""},{"location":"api/core/#prela.core.sampler.BaseSampler","title":"<code>prela.core.sampler.BaseSampler</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for trace samplers.</p> <p>Samplers determine whether a trace should be collected based on the trace ID and potentially other factors.</p>"},{"location":"api/core/#prela.core.sampler.BaseSampler-functions","title":"Functions","text":""},{"location":"api/core/#prela.core.sampler.BaseSampler.should_sample","title":"<code>should_sample(trace_id)</code>  <code>abstractmethod</code>","text":"<p>Determine if a trace should be sampled.</p> <p>Parameters:</p> Name Type Description Default <code>trace_id</code> <code>str</code> <p>The trace ID to make a sampling decision for</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the trace should be sampled, False otherwise</p>"},{"location":"api/core/#prela.core.sampler.AlwaysOnSampler","title":"<code>prela.core.sampler.AlwaysOnSampler</code>","text":"<p>               Bases: <code>BaseSampler</code></p> <p>Sampler that always samples every trace.</p> <p>Use this in development or when you need complete trace coverage. Be aware this may generate high data volumes in production.</p>"},{"location":"api/core/#prela.core.sampler.AlwaysOnSampler-functions","title":"Functions","text":""},{"location":"api/core/#prela.core.sampler.AlwaysOnSampler.should_sample","title":"<code>should_sample(trace_id)</code>","text":"<p>Always return True.</p> <p>Parameters:</p> Name Type Description Default <code>trace_id</code> <code>str</code> <p>The trace ID (unused)</p> required <p>Returns:</p> Type Description <code>bool</code> <p>Always True</p>"},{"location":"api/core/#prela.core.sampler.AlwaysOffSampler","title":"<code>prela.core.sampler.AlwaysOffSampler</code>","text":"<p>               Bases: <code>BaseSampler</code></p> <p>Sampler that never samples any traces.</p> <p>Use this to completely disable tracing, for example during maintenance windows or in testing environments.</p>"},{"location":"api/core/#prela.core.sampler.AlwaysOffSampler-functions","title":"Functions","text":""},{"location":"api/core/#prela.core.sampler.AlwaysOffSampler.should_sample","title":"<code>should_sample(trace_id)</code>","text":"<p>Always return False.</p> <p>Parameters:</p> Name Type Description Default <code>trace_id</code> <code>str</code> <p>The trace ID (unused)</p> required <p>Returns:</p> Type Description <code>bool</code> <p>Always False</p>"},{"location":"api/core/#prela.core.sampler.ProbabilitySampler","title":"<code>prela.core.sampler.ProbabilitySampler</code>","text":"<p>               Bases: <code>BaseSampler</code></p> <p>Sampler that samples traces with a fixed probability.</p> <p>This sampler uses a deterministic hash-based approach to ensure consistent sampling decisions for the same trace ID across different services and processes.</p>"},{"location":"api/core/#prela.core.sampler.ProbabilitySampler-functions","title":"Functions","text":""},{"location":"api/core/#prela.core.sampler.ProbabilitySampler.__init__","title":"<code>__init__(rate)</code>","text":"<p>Initialize the probability sampler.</p> <p>Parameters:</p> Name Type Description Default <code>rate</code> <code>float</code> <p>Sampling rate between 0.0 and 1.0 (inclusive)</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If rate is not between 0.0 and 1.0</p>"},{"location":"api/core/#prela.core.sampler.ProbabilitySampler.should_sample","title":"<code>should_sample(trace_id)</code>","text":"<p>Sample based on trace ID hash.</p> <p>Uses MD5 hash of trace_id to make a deterministic sampling decision. This ensures the same trace_id always gets the same sampling decision across different processes and services.</p> <p>Parameters:</p> Name Type Description Default <code>trace_id</code> <code>str</code> <p>The trace ID to make a sampling decision for</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the trace should be sampled, False otherwise</p>"},{"location":"api/core/#prela.core.sampler.RateLimitingSampler","title":"<code>prela.core.sampler.RateLimitingSampler</code>","text":"<p>               Bases: <code>BaseSampler</code></p> <p>Sampler that limits the number of traces sampled per second.</p> <p>This sampler uses a token bucket algorithm to enforce a maximum rate of sampled traces per second. Useful for controlling costs and backend load.</p>"},{"location":"api/core/#prela.core.sampler.RateLimitingSampler-functions","title":"Functions","text":""},{"location":"api/core/#prela.core.sampler.RateLimitingSampler.__init__","title":"<code>__init__(traces_per_second)</code>","text":"<p>Initialize the rate limiting sampler.</p> <p>Parameters:</p> Name Type Description Default <code>traces_per_second</code> <code>float</code> <p>Maximum number of traces to sample per second</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If traces_per_second is negative</p>"},{"location":"api/core/#prela.core.sampler.RateLimitingSampler.should_sample","title":"<code>should_sample(trace_id)</code>","text":"<p>Sample if tokens are available.</p> <p>Uses a token bucket algorithm: tokens regenerate at the configured rate, and each sampling decision consumes one token.</p> <p>Parameters:</p> Name Type Description Default <code>trace_id</code> <code>str</code> <p>The trace ID (unused)</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if a token is available, False otherwise</p>"},{"location":"api/core/#clock-utilities","title":"Clock Utilities","text":""},{"location":"api/core/#prela.core.clock.now","title":"<code>prela.core.clock.now()</code>","text":"<p>Get the current UTC time with microsecond precision.</p> <p>Returns:</p> Type Description <code>datetime</code> <p>Current datetime in UTC with microsecond precision</p> Example <p>timestamp = now() timestamp.tzinfo == timezone.utc True</p>"},{"location":"api/core/#prela.core.clock.monotonic_ns","title":"<code>prela.core.clock.monotonic_ns()</code>","text":"<p>Get monotonic time in nanoseconds.</p> <p>This is useful for measuring durations as it's not affected by system clock adjustments.</p> <p>Returns:</p> Type Description <code>int</code> <p>Monotonic time in nanoseconds</p> Example <p>start = monotonic_ns()</p>"},{"location":"api/core/#prela.core.clock.monotonic_ns--do-work","title":"... do work ...","text":"<p>end = monotonic_ns() elapsed_ms = duration_ms(start, end)</p>"},{"location":"api/core/#prela.core.clock.duration_ms","title":"<code>prela.core.clock.duration_ms(start_ns, end_ns)</code>","text":"<p>Calculate duration in milliseconds from nanosecond timestamps.</p> <p>Parameters:</p> Name Type Description Default <code>start_ns</code> <code>int</code> <p>Start time in nanoseconds (from monotonic_ns)</p> required <code>end_ns</code> <code>int</code> <p>End time in nanoseconds (from monotonic_ns)</p> required <p>Returns:</p> Type Description <code>float</code> <p>Duration in milliseconds</p> Example <p>start = monotonic_ns() end = start + 1_500_000  # 1.5ms later duration_ms(start, end) 1.5</p>"},{"location":"api/core/#prela.core.clock.format_timestamp","title":"<code>prela.core.clock.format_timestamp(dt)</code>","text":"<p>Format a datetime as ISO 8601 string.</p> <p>Parameters:</p> Name Type Description Default <code>dt</code> <code>datetime</code> <p>Datetime to format</p> required <p>Returns:</p> Type Description <code>str</code> <p>ISO 8601 formatted string</p> Example <p>dt = datetime(2024, 1, 15, 12, 30, 45, 123456, tzinfo=timezone.utc) format_timestamp(dt) '2024-01-15T12:30:45.123456+00:00'</p>"},{"location":"api/core/#prela.core.clock.parse_timestamp","title":"<code>prela.core.clock.parse_timestamp(s)</code>","text":"<p>Parse an ISO 8601 timestamp string.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>str</code> <p>ISO 8601 formatted timestamp string</p> required <p>Returns:</p> Type Description <code>datetime</code> <p>Parsed datetime object</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the string is not a valid ISO 8601 timestamp</p> Example <p>dt = parse_timestamp('2024-01-15T12:30:45.123456+00:00') dt.year 2024</p>"},{"location":"api/evaluations/","title":"Evaluations API","text":"<p>Framework for testing and evaluating AI agent behavior.</p>"},{"location":"api/evaluations/#test-case-definition","title":"Test Case Definition","text":""},{"location":"api/evaluations/#evalinput","title":"EvalInput","text":""},{"location":"api/evaluations/#prela.evals.case.EvalInput","title":"<code>prela.evals.case.EvalInput</code>  <code>dataclass</code>","text":"<p>Input data for an eval case.</p> <p>Represents what goes into the agent being tested. Can be a simple query, a list of messages, or custom context data.</p> <p>Attributes:</p> Name Type Description <code>query</code> <code>str | None</code> <p>Simple string query/prompt (for basic use cases)</p> <code>messages</code> <code>list[dict] | None</code> <p>List of message dicts (for chat-based agents)</p> <code>context</code> <code>dict[str, Any] | None</code> <p>Additional context data (e.g., retrieved documents, metadata)</p> Example"},{"location":"api/evaluations/#prela.evals.case.EvalInput--simple-query","title":"Simple query","text":"<p>input1 = EvalInput(query=\"What is the capital of France?\")</p>"},{"location":"api/evaluations/#prela.evals.case.EvalInput--chat-messages","title":"Chat messages","text":"<p>input2 = EvalInput(messages=[ ...     {\"role\": \"system\", \"content\": \"You are a helpful assistant\"}, ...     {\"role\": \"user\", \"content\": \"Hello!\"} ... ])</p>"},{"location":"api/evaluations/#prela.evals.case.EvalInput--query-with-context","title":"Query with context","text":"<p>input3 = EvalInput( ...     query=\"Summarize the document\", ...     context={\"document\": \"Long text here...\"} ... )</p>"},{"location":"api/evaluations/#prela.evals.case.EvalInput-functions","title":"Functions","text":""},{"location":"api/evaluations/#prela.evals.case.EvalInput.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate that at least one input type is provided.</p>"},{"location":"api/evaluations/#prela.evals.case.EvalInput.to_agent_input","title":"<code>to_agent_input()</code>","text":"<p>Convert to format that agent expects.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary with all non-None input fields.</p> Example <p>input = EvalInput(query=\"Hello\", context={\"user_id\": \"123\"}) input.to_agent_input() {'query': 'Hello', 'context': {'user_id': '123'}}</p>"},{"location":"api/evaluations/#prela.evals.case.EvalInput.from_dict","title":"<code>from_dict(data)</code>  <code>classmethod</code>","text":"<p>Create EvalInput from dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict[str, Any]</code> <p>Dictionary with 'query', 'messages', and/or 'context' keys</p> required <p>Returns:</p> Type Description <code>EvalInput</code> <p>EvalInput instance</p> Example <p>data = {\"query\": \"Hello\", \"context\": {\"key\": \"value\"}} input = EvalInput.from_dict(data)</p>"},{"location":"api/evaluations/#prela.evals.case.EvalInput.to_dict","title":"<code>to_dict()</code>","text":"<p>Convert to dictionary for serialization.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary representation of the input.</p>"},{"location":"api/evaluations/#evalexpected","title":"EvalExpected","text":""},{"location":"api/evaluations/#prela.evals.case.EvalExpected","title":"<code>prela.evals.case.EvalExpected</code>  <code>dataclass</code>","text":"<p>Expected output for an eval case.</p> <p>Defines what the agent's output should look like. Supports multiple validation strategies: - Exact output match - Contains/not_contains substring checks - Tool call validation - Custom metadata checks</p> <p>Attributes:</p> Name Type Description <code>output</code> <code>str | None</code> <p>Exact expected output string</p> <code>contains</code> <code>list[str] | None</code> <p>List of substrings that must appear in output</p> <code>not_contains</code> <code>list[str] | None</code> <p>List of substrings that must NOT appear in output</p> <code>tool_calls</code> <code>list[dict[str, Any]] | None</code> <p>Expected tool calls (list of dicts with 'name', 'args', etc.)</p> <code>metadata</code> <code>dict[str, Any] | None</code> <p>Expected metadata fields (e.g., final_answer, confidence)</p> Example"},{"location":"api/evaluations/#prela.evals.case.EvalExpected--exact-match","title":"Exact match","text":"<p>expected1 = EvalExpected(output=\"The answer is 42\")</p>"},{"location":"api/evaluations/#prela.evals.case.EvalExpected--substring-checks","title":"Substring checks","text":"<p>expected2 = EvalExpected( ...     contains=[\"Paris\", \"capital\"], ...     not_contains=[\"London\", \"Berlin\"] ... )</p>"},{"location":"api/evaluations/#prela.evals.case.EvalExpected--tool-call-validation","title":"Tool call validation","text":"<p>expected3 = EvalExpected(tool_calls=[ ...     {\"name\": \"search\", \"args\": {\"query\": \"weather\"}} ... ])</p>"},{"location":"api/evaluations/#prela.evals.case.EvalExpected-functions","title":"Functions","text":""},{"location":"api/evaluations/#prela.evals.case.EvalExpected.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate that at least one expectation is provided.</p>"},{"location":"api/evaluations/#prela.evals.case.EvalExpected.from_dict","title":"<code>from_dict(data)</code>  <code>classmethod</code>","text":"<p>Create EvalExpected from dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict[str, Any]</code> <p>Dictionary with expected output specifications</p> required <p>Returns:</p> Type Description <code>EvalExpected</code> <p>EvalExpected instance</p> Example <p>data = {\"contains\": [\"Paris\"], \"not_contains\": [\"London\"]} expected = EvalExpected.from_dict(data)</p>"},{"location":"api/evaluations/#prela.evals.case.EvalExpected.to_dict","title":"<code>to_dict()</code>","text":"<p>Convert to dictionary for serialization.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary representation of the expected output.</p>"},{"location":"api/evaluations/#evalcase","title":"EvalCase","text":""},{"location":"api/evaluations/#prela.evals.case.EvalCase","title":"<code>prela.evals.case.EvalCase</code>  <code>dataclass</code>","text":"<p>Complete evaluation test case.</p> <p>Represents a single test case with input, expected output, and assertions. Eval cases are the building blocks of eval suites.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>str</code> <p>Unique identifier for this test case</p> <code>name</code> <code>str</code> <p>Human-readable test case name</p> <code>input</code> <code>EvalInput</code> <p>Input data for the agent</p> <code>expected</code> <code>EvalExpected | None</code> <p>Expected output (optional, can use assertions instead)</p> <code>assertions</code> <code>list[dict[str, Any]] | None</code> <p>List of assertion configurations (dicts with 'type', 'value', etc.)</p> <code>tags</code> <code>list[str]</code> <p>Tags for filtering/grouping test cases</p> <code>timeout_seconds</code> <code>float</code> <p>Maximum execution time for this test case</p> <code>metadata</code> <code>dict[str, Any]</code> <p>Additional metadata for this test case</p> Example <p>case = EvalCase( ...     id=\"test_basic_qa\", ...     name=\"Basic factual question\", ...     input=EvalInput(query=\"What is the capital of France?\"), ...     expected=EvalExpected(contains=[\"Paris\"]), ...     assertions=[ ...         {\"type\": \"contains\", \"value\": \"Paris\"}, ...         {\"type\": \"semantic_similarity\", \"threshold\": 0.8} ...     ], ...     tags=[\"qa\", \"geography\"], ...     timeout_seconds=10.0 ... )</p>"},{"location":"api/evaluations/#prela.evals.case.EvalCase-functions","title":"Functions","text":""},{"location":"api/evaluations/#prela.evals.case.EvalCase.__init__","title":"<code>__init__(id, name, input, expected=None, assertions=None, tags=list(), timeout_seconds=30.0, metadata=dict())</code>","text":""},{"location":"api/evaluations/#prela.evals.case.EvalCase.to_dict","title":"<code>to_dict()</code>","text":"<p>Convert to dictionary for serialization.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary representation of the test case.</p> Example <p>case = EvalCase( ...     id=\"test_1\", ...     name=\"Test\", ...     input=EvalInput(query=\"Hello\"), ...     expected=EvalExpected(contains=[\"Hi\"]) ... ) data = case.to_dict() data[\"id\"] 'test_1'</p>"},{"location":"api/evaluations/#prela.evals.case.EvalCase.from_dict","title":"<code>from_dict(data)</code>  <code>classmethod</code>","text":"<p>Create EvalCase from dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict[str, Any]</code> <p>Dictionary with test case specification</p> required <p>Returns:</p> Type Description <code>EvalCase</code> <p>EvalCase instance</p> Example <p>data = { ...     \"id\": \"test_1\", ...     \"name\": \"Test case 1\", ...     \"input\": {\"query\": \"Hello\"}, ...     \"expected\": {\"contains\": [\"Hi\"]}, ...     \"tags\": [\"greeting\"] ... } case = EvalCase.from_dict(data)</p>"},{"location":"api/evaluations/#test-suite","title":"Test Suite","text":""},{"location":"api/evaluations/#evalsuite","title":"EvalSuite","text":""},{"location":"api/evaluations/#prela.evals.suite.EvalSuite","title":"<code>prela.evals.suite.EvalSuite</code>  <code>dataclass</code>","text":"<p>Collection of eval cases with shared configuration.</p> <p>An eval suite organizes multiple test cases with: - Shared setup/teardown hooks - Default assertions applied to all cases - YAML serialization for easy configuration - Tagging and filtering capabilities</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Suite name (e.g., \"RAG Quality Suite\")</p> <code>description</code> <code>str</code> <p>Human-readable description of what this suite tests</p> <code>cases</code> <code>list[EvalCase]</code> <p>List of eval cases in this suite</p> <code>default_assertions</code> <code>list[dict[str, Any]] | None</code> <p>Assertions applied to all cases (unless overridden)</p> <code>setup</code> <code>Callable[[], None] | None</code> <p>Callable run before executing the suite (e.g., start services)</p> <code>teardown</code> <code>Callable[[], None] | None</code> <p>Callable run after executing the suite (e.g., cleanup)</p> <code>metadata</code> <code>dict[str, Any]</code> <p>Additional metadata for the suite</p> Example <p>suite = EvalSuite( ...     name=\"RAG Quality Suite\", ...     description=\"Tests for RAG pipeline quality\", ...     cases=[ ...         EvalCase( ...             id=\"test_basic_qa\", ...             name=\"Basic factual question\", ...             input=EvalInput(query=\"What is the capital of France?\"), ...             expected=EvalExpected(contains=[\"Paris\"]) ...         ) ...     ], ...     default_assertions=[ ...         {\"type\": \"latency\", \"max_ms\": 5000}, ...         {\"type\": \"no_errors\"} ...     ] ... )</p>"},{"location":"api/evaluations/#prela.evals.suite.EvalSuite-functions","title":"Functions","text":""},{"location":"api/evaluations/#prela.evals.suite.EvalSuite.__init__","title":"<code>__init__(name, description='', cases=list(), default_assertions=None, setup=None, teardown=None, metadata=dict())</code>","text":""},{"location":"api/evaluations/#prela.evals.suite.EvalSuite.add_case","title":"<code>add_case(case)</code>","text":"<p>Add a test case to the suite.</p> <p>Parameters:</p> Name Type Description Default <code>case</code> <code>EvalCase</code> <p>Eval case to add</p> required Example <p>suite = EvalSuite(name=\"My Suite\") case = EvalCase( ...     id=\"test_1\", ...     name=\"Test\", ...     input=EvalInput(query=\"Hello\"), ...     expected=EvalExpected(contains=[\"Hi\"]) ... ) suite.add_case(case)</p>"},{"location":"api/evaluations/#prela.evals.suite.EvalSuite.filter_by_tags","title":"<code>filter_by_tags(tags)</code>","text":"<p>Filter test cases by tags.</p> <p>Returns cases that have ALL specified tags.</p> <p>Parameters:</p> Name Type Description Default <code>tags</code> <code>list[str]</code> <p>List of tags to filter by</p> required <p>Returns:</p> Type Description <code>list[EvalCase]</code> <p>List of matching test cases</p> Example <p>suite = EvalSuite(name=\"My Suite\", cases=[...]) qa_cases = suite.filter_by_tags([\"qa\"]) geography_qa = suite.filter_by_tags([\"qa\", \"geography\"])</p>"},{"location":"api/evaluations/#prela.evals.suite.EvalSuite.to_yaml","title":"<code>to_yaml(path)</code>","text":"<p>Save eval suite to YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>Path to save YAML file</p> required <p>Raises:</p> Type Description <code>ImportError</code> <p>If PyYAML is not installed</p> Example <p>suite = EvalSuite(name=\"My Suite\", cases=[...]) suite.to_yaml(\"suite.yaml\")</p>"},{"location":"api/evaluations/#prela.evals.suite.EvalSuite.from_yaml","title":"<code>from_yaml(path)</code>  <code>classmethod</code>","text":"<p>Load eval suite from YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>Path to YAML file</p> required <p>Returns:</p> Type Description <code>EvalSuite</code> <p>EvalSuite instance</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If PyYAML is not installed</p> <code>FileNotFoundError</code> <p>If file doesn't exist</p> <code>YAMLError</code> <p>If YAML parsing fails</p> Example <p>suite = EvalSuite.from_yaml(\"tests/suite.yaml\")</p>"},{"location":"api/evaluations/#test-execution","title":"Test Execution","text":""},{"location":"api/evaluations/#evalrunner","title":"EvalRunner","text":""},{"location":"api/evaluations/#prela.evals.runner.EvalRunner","title":"<code>prela.evals.runner.EvalRunner</code>","text":"<p>Runner for executing evaluation suites against AI agents.</p> <p>The runner executes test cases, runs assertions, captures traces, and aggregates results. Supports parallel execution with thread pools.</p> Example <p>from prela.evals import EvalSuite, EvalRunner from prela import get_tracer</p> <p>suite = EvalSuite.from_yaml(\"tests.yaml\") tracer = get_tracer()</p> <p>def my_agent(input_data): ...     # Your agent logic here ...     return \"agent output\"</p> <p>runner = EvalRunner(suite, my_agent, tracer=tracer) result = runner.run() print(result.summary())</p>"},{"location":"api/evaluations/#prela.evals.runner.EvalRunner-functions","title":"Functions","text":""},{"location":"api/evaluations/#prela.evals.runner.EvalRunner.__init__","title":"<code>__init__(suite, agent, tracer=None, parallel=False, max_workers=4, on_case_complete=None)</code>","text":"<p>Initialize the evaluation runner.</p> <p>Parameters:</p> Name Type Description Default <code>suite</code> <code>EvalSuite</code> <p>The evaluation suite to run.</p> required <code>agent</code> <code>Callable[[EvalInput], Any]</code> <p>Callable that takes an EvalInput and returns agent output.</p> required <code>tracer</code> <code>Tracer | None</code> <p>Optional tracer for capturing execution traces.</p> <code>None</code> <code>parallel</code> <code>bool</code> <p>Whether to run cases in parallel using a thread pool.</p> <code>False</code> <code>max_workers</code> <code>int</code> <p>Maximum number of worker threads if parallel=True.</p> <code>4</code> <code>on_case_complete</code> <code>Callable[[CaseResult], None] | None</code> <p>Optional callback invoked after each case completes.</p> <code>None</code>"},{"location":"api/evaluations/#prela.evals.runner.EvalRunner.run","title":"<code>run()</code>","text":"<p>Run all test cases in the evaluation suite.</p> <p>Executes setup/teardown hooks, runs all cases (sequentially or in parallel), executes assertions, and aggregates results.</p> <p>Returns:</p> Type Description <code>EvalRunResult</code> <p>EvalRunResult with aggregated statistics and individual case results.</p>"},{"location":"api/evaluations/#prela.evals.runner.EvalRunner.run_case","title":"<code>run_case(case)</code>","text":"<p>Run a single test case.</p> <p>Executes the agent with the case input, runs all assertions, captures the trace ID if a tracer is configured, and returns aggregated results.</p> <p>Parameters:</p> Name Type Description Default <code>case</code> <code>EvalCase</code> <p>The test case to run.</p> required <p>Returns:</p> Type Description <code>CaseResult</code> <p>CaseResult with pass/fail status and assertion results.</p>"},{"location":"api/evaluations/#caseresult","title":"CaseResult","text":""},{"location":"api/evaluations/#prela.evals.runner.CaseResult","title":"<code>prela.evals.runner.CaseResult</code>  <code>dataclass</code>","text":"<p>Result of running a single eval case.</p>"},{"location":"api/evaluations/#prela.evals.runner.CaseResult-functions","title":"Functions","text":""},{"location":"api/evaluations/#prela.evals.runner.CaseResult.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate fields.</p>"},{"location":"api/evaluations/#evalrunresult","title":"EvalRunResult","text":""},{"location":"api/evaluations/#prela.evals.runner.EvalRunResult","title":"<code>prela.evals.runner.EvalRunResult</code>  <code>dataclass</code>","text":"<p>Result of running an evaluation suite.</p>"},{"location":"api/evaluations/#prela.evals.runner.EvalRunResult-functions","title":"Functions","text":""},{"location":"api/evaluations/#prela.evals.runner.EvalRunResult.summary","title":"<code>summary()</code>","text":"<p>Return human-readable summary of the evaluation run.</p> <p>Returns:</p> Type Description <code>str</code> <p>Multi-line string with summary statistics and case results.</p>"},{"location":"api/evaluations/#create_assertion","title":"create_assertion","text":""},{"location":"api/evaluations/#prela.evals.runner.create_assertion","title":"<code>prela.evals.runner.create_assertion(config)</code>","text":"<p>Factory function to create assertion instances from configuration.</p> <p>This maps assertion type strings to concrete assertion classes and instantiates them with the provided configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>Dictionary with \"type\" key and type-specific parameters.</p> required <p>Returns:</p> Type Description <code>BaseAssertion</code> <p>Instantiated assertion object.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If assertion type is unknown or configuration is invalid.</p> Example <p>assertion = create_assertion({ ...     \"type\": \"contains\", ...     \"text\": \"hello\", ...     \"case_sensitive\": False ... }) result = assertion.evaluate(\"Hello world\", None, None) assert result.passed</p>"},{"location":"api/evaluations/#assertions","title":"Assertions","text":""},{"location":"api/evaluations/#base-assertion","title":"Base Assertion","text":""},{"location":"api/evaluations/#prela.evals.assertions.base.BaseAssertion","title":"<code>prela.evals.assertions.base.BaseAssertion</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for all assertions.</p> <p>Assertions evaluate agent outputs and traces to determine if they meet expected criteria. Subclasses should implement the evaluate() method to perform the actual check.</p>"},{"location":"api/evaluations/#prela.evals.assertions.base.BaseAssertion-functions","title":"Functions","text":""},{"location":"api/evaluations/#prela.evals.assertions.base.BaseAssertion.evaluate","title":"<code>evaluate(output, expected, trace)</code>  <code>abstractmethod</code>","text":"<p>Evaluate the assertion against the output and trace.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>Any</code> <p>The actual output from the agent/function under test</p> required <code>expected</code> <code>Any | None</code> <p>The expected output (format depends on assertion type)</p> required <code>trace</code> <code>list[Span] | None</code> <p>Optional list of spans from the traced execution</p> required <p>Returns:</p> Type Description <code>AssertionResult</code> <p>AssertionResult with pass/fail status and details</p>"},{"location":"api/evaluations/#prela.evals.assertions.base.BaseAssertion.from_config","title":"<code>from_config(config)</code>  <code>abstractmethod</code> <code>classmethod</code>","text":"<p>Create an assertion instance from configuration dict.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict[str, Any]</code> <p>Configuration dictionary with assertion-specific parameters</p> required <p>Returns:</p> Type Description <code>BaseAssertion</code> <p>Configured assertion instance</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If configuration is invalid</p>"},{"location":"api/evaluations/#prela.evals.assertions.base.AssertionResult","title":"<code>prela.evals.assertions.base.AssertionResult</code>  <code>dataclass</code>","text":"<p>Result of an assertion evaluation.</p> <p>Attributes:</p> Name Type Description <code>passed</code> <code>bool</code> <p>Whether the assertion passed</p> <code>assertion_type</code> <code>str</code> <p>Type of assertion (e.g., \"contains\", \"semantic_similarity\")</p> <code>message</code> <code>str</code> <p>Human-readable message describing the result</p> <code>score</code> <code>float | None</code> <p>Optional score between 0-1 for partial credit assertions</p> <code>expected</code> <code>Any</code> <p>Expected value (if applicable)</p> <code>actual</code> <code>Any</code> <p>Actual value that was evaluated</p> <code>details</code> <code>dict[str, Any]</code> <p>Additional details about the evaluation</p>"},{"location":"api/evaluations/#prela.evals.assertions.base.AssertionResult-functions","title":"Functions","text":""},{"location":"api/evaluations/#prela.evals.assertions.base.AssertionResult.__str__","title":"<code>__str__()</code>","text":"<p>Human-readable string representation.</p>"},{"location":"api/evaluations/#structural-assertions","title":"Structural Assertions","text":""},{"location":"api/evaluations/#prela.evals.assertions.structural.ContainsAssertion","title":"<code>prela.evals.assertions.structural.ContainsAssertion</code>","text":"<p>               Bases: <code>BaseAssertion</code></p> <p>Assert that output contains specified text.</p> Example <p>assertion = ContainsAssertion(text=\"error\", case_sensitive=False) result = assertion.evaluate(output=\"Error occurred\", expected=None, trace=None) assert result.passed</p>"},{"location":"api/evaluations/#prela.evals.assertions.structural.ContainsAssertion-functions","title":"Functions","text":""},{"location":"api/evaluations/#prela.evals.assertions.structural.ContainsAssertion.__init__","title":"<code>__init__(text, case_sensitive=True)</code>","text":"<p>Initialize contains assertion.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text that must be present in output</p> required <code>case_sensitive</code> <code>bool</code> <p>Whether to perform case-sensitive matching</p> <code>True</code>"},{"location":"api/evaluations/#prela.evals.assertions.structural.ContainsAssertion.evaluate","title":"<code>evaluate(output, expected, trace)</code>","text":"<p>Check if output contains the specified text.</p>"},{"location":"api/evaluations/#prela.evals.assertions.structural.ContainsAssertion.from_config","title":"<code>from_config(config)</code>  <code>classmethod</code>","text":"<p>Create from configuration.</p> Config format <p>{     \"text\": \"required text\",     \"case_sensitive\": true  # optional, default: true }</p>"},{"location":"api/evaluations/#prela.evals.assertions.structural.NotContainsAssertion","title":"<code>prela.evals.assertions.structural.NotContainsAssertion</code>","text":"<p>               Bases: <code>BaseAssertion</code></p> <p>Assert that output does NOT contain specified text.</p> Example <p>assertion = NotContainsAssertion(text=\"error\") result = assertion.evaluate(output=\"Success!\", expected=None, trace=None) assert result.passed</p>"},{"location":"api/evaluations/#prela.evals.assertions.structural.NotContainsAssertion-functions","title":"Functions","text":""},{"location":"api/evaluations/#prela.evals.assertions.structural.NotContainsAssertion.__init__","title":"<code>__init__(text, case_sensitive=True)</code>","text":"<p>Initialize not-contains assertion.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Text that must NOT be present in output</p> required <code>case_sensitive</code> <code>bool</code> <p>Whether to perform case-sensitive matching</p> <code>True</code>"},{"location":"api/evaluations/#prela.evals.assertions.structural.NotContainsAssertion.evaluate","title":"<code>evaluate(output, expected, trace)</code>","text":"<p>Check if output does not contain the specified text.</p>"},{"location":"api/evaluations/#prela.evals.assertions.structural.NotContainsAssertion.from_config","title":"<code>from_config(config)</code>  <code>classmethod</code>","text":"<p>Create from configuration.</p> Config format <p>{     \"text\": \"forbidden text\",     \"case_sensitive\": true  # optional, default: true }</p>"},{"location":"api/evaluations/#prela.evals.assertions.structural.RegexAssertion","title":"<code>prela.evals.assertions.structural.RegexAssertion</code>","text":"<p>               Bases: <code>BaseAssertion</code></p> <p>Assert that output matches a regular expression pattern.</p> Example <p>assertion = RegexAssertion(pattern=r\"\\d{3}-\\d{4}\") result = assertion.evaluate(output=\"Call 555-1234\", expected=None, trace=None) assert result.passed</p>"},{"location":"api/evaluations/#prela.evals.assertions.structural.RegexAssertion-functions","title":"Functions","text":""},{"location":"api/evaluations/#prela.evals.assertions.structural.RegexAssertion.__init__","title":"<code>__init__(pattern, flags=0)</code>","text":"<p>Initialize regex assertion.</p> <p>Parameters:</p> Name Type Description Default <code>pattern</code> <code>str</code> <p>Regular expression pattern to match</p> required <code>flags</code> <code>int</code> <p>Optional regex flags (e.g., re.IGNORECASE)</p> <code>0</code>"},{"location":"api/evaluations/#prela.evals.assertions.structural.RegexAssertion.evaluate","title":"<code>evaluate(output, expected, trace)</code>","text":"<p>Check if output matches the regex pattern.</p>"},{"location":"api/evaluations/#prela.evals.assertions.structural.RegexAssertion.from_config","title":"<code>from_config(config)</code>  <code>classmethod</code>","text":"<p>Create from configuration.</p> Config format <p>{     \"pattern\": \"\\d{3}-\\d{4}\",     \"flags\": 2  # optional, e.g., re.IGNORECASE }</p>"},{"location":"api/evaluations/#prela.evals.assertions.structural.LengthAssertion","title":"<code>prela.evals.assertions.structural.LengthAssertion</code>","text":"<p>               Bases: <code>BaseAssertion</code></p> <p>Assert that output length is within specified bounds.</p> Example <p>assertion = LengthAssertion(min_length=10, max_length=100) result = assertion.evaluate(output=\"Hello, world!\", expected=None, trace=None) assert result.passed</p>"},{"location":"api/evaluations/#prela.evals.assertions.structural.LengthAssertion-functions","title":"Functions","text":""},{"location":"api/evaluations/#prela.evals.assertions.structural.LengthAssertion.__init__","title":"<code>__init__(min_length=None, max_length=None)</code>","text":"<p>Initialize length assertion.</p> <p>Parameters:</p> Name Type Description Default <code>min_length</code> <code>int | None</code> <p>Minimum acceptable length (inclusive)</p> <code>None</code> <code>max_length</code> <code>int | None</code> <p>Maximum acceptable length (inclusive)</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If both min_length and max_length are None</p>"},{"location":"api/evaluations/#prela.evals.assertions.structural.LengthAssertion.evaluate","title":"<code>evaluate(output, expected, trace)</code>","text":"<p>Check if output length is within bounds.</p>"},{"location":"api/evaluations/#prela.evals.assertions.structural.LengthAssertion.from_config","title":"<code>from_config(config)</code>  <code>classmethod</code>","text":"<p>Create from configuration.</p> Config format <p>{     \"min_length\": 10,  # optional     \"max_length\": 100  # optional }</p>"},{"location":"api/evaluations/#prela.evals.assertions.structural.JSONValidAssertion","title":"<code>prela.evals.assertions.structural.JSONValidAssertion</code>","text":"<p>               Bases: <code>BaseAssertion</code></p> <p>Assert that output is valid JSON, optionally matching a schema.</p> Example <p>assertion = JSONValidAssertion() result = assertion.evaluate(output='{\"key\": \"value\"}', expected=None, trace=None) assert result.passed</p>"},{"location":"api/evaluations/#prela.evals.assertions.structural.JSONValidAssertion-functions","title":"Functions","text":""},{"location":"api/evaluations/#prela.evals.assertions.structural.JSONValidAssertion.__init__","title":"<code>__init__(schema=None)</code>","text":"<p>Initialize JSON validation assertion.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>dict[str, Any] | None</code> <p>Optional JSON schema to validate against (using jsonschema library)</p> <code>None</code>"},{"location":"api/evaluations/#prela.evals.assertions.structural.JSONValidAssertion.evaluate","title":"<code>evaluate(output, expected, trace)</code>","text":"<p>Check if output is valid JSON and optionally matches schema.</p>"},{"location":"api/evaluations/#prela.evals.assertions.structural.JSONValidAssertion.from_config","title":"<code>from_config(config)</code>  <code>classmethod</code>","text":"<p>Create from configuration.</p> Config format <p>{     \"schema\": {  # optional         \"type\": \"object\",         \"properties\": {             \"name\": {\"type\": \"string\"}         }     } }</p>"},{"location":"api/evaluations/#tool-assertions","title":"Tool Assertions","text":""},{"location":"api/evaluations/#prela.evals.assertions.tool.ToolCalledAssertion","title":"<code>prela.evals.assertions.tool.ToolCalledAssertion</code>","text":"<p>               Bases: <code>BaseAssertion</code></p> <p>Assert that a specific tool was called during execution.</p> <p>This assertion examines the trace to verify that a tool span with the specified name exists.</p> Example <p>assertion = ToolCalledAssertion(tool_name=\"web_search\") result = assertion.evaluate(output=None, expected=None, trace=spans) assert result.passed</p>"},{"location":"api/evaluations/#prela.evals.assertions.tool.ToolCalledAssertion-functions","title":"Functions","text":""},{"location":"api/evaluations/#prela.evals.assertions.tool.ToolCalledAssertion.__init__","title":"<code>__init__(tool_name)</code>","text":"<p>Initialize tool called assertion.</p> <p>Parameters:</p> Name Type Description Default <code>tool_name</code> <code>str</code> <p>Name of the tool that should have been called</p> required"},{"location":"api/evaluations/#prela.evals.assertions.tool.ToolCalledAssertion.evaluate","title":"<code>evaluate(output, expected, trace)</code>","text":"<p>Check if the specified tool was called in the trace.</p>"},{"location":"api/evaluations/#prela.evals.assertions.tool.ToolCalledAssertion.from_config","title":"<code>from_config(config)</code>  <code>classmethod</code>","text":"<p>Create from configuration.</p> Config format <p>{     \"tool_name\": \"web_search\" }</p>"},{"location":"api/evaluations/#prela.evals.assertions.tool.ToolArgsAssertion","title":"<code>prela.evals.assertions.tool.ToolArgsAssertion</code>","text":"<p>               Bases: <code>BaseAssertion</code></p> <p>Assert that a tool was called with expected arguments.</p> <p>This assertion verifies both that the tool was called and that it was called with specific argument values.</p> Example <p>assertion = ToolArgsAssertion( ...     tool_name=\"web_search\", ...     expected_args={\"query\": \"Python tutorial\"} ... ) result = assertion.evaluate(output=None, expected=None, trace=spans) assert result.passed</p>"},{"location":"api/evaluations/#prela.evals.assertions.tool.ToolArgsAssertion-functions","title":"Functions","text":""},{"location":"api/evaluations/#prela.evals.assertions.tool.ToolArgsAssertion.__init__","title":"<code>__init__(tool_name, expected_args, partial_match=True)</code>","text":"<p>Initialize tool args assertion.</p> <p>Parameters:</p> Name Type Description Default <code>tool_name</code> <code>str</code> <p>Name of the tool to check</p> required <code>expected_args</code> <code>dict[str, Any]</code> <p>Expected argument key-value pairs</p> required <code>partial_match</code> <code>bool</code> <p>If True, only check that expected_args are present           (allow additional args). If False, require exact match.</p> <code>True</code>"},{"location":"api/evaluations/#prela.evals.assertions.tool.ToolArgsAssertion.evaluate","title":"<code>evaluate(output, expected, trace)</code>","text":"<p>Check if tool was called with expected arguments.</p>"},{"location":"api/evaluations/#prela.evals.assertions.tool.ToolArgsAssertion.from_config","title":"<code>from_config(config)</code>  <code>classmethod</code>","text":"<p>Create from configuration.</p> Config format <p>{     \"tool_name\": \"web_search\",     \"expected_args\": {\"query\": \"Python\"},     \"partial_match\": true  # optional, default: true }</p>"},{"location":"api/evaluations/#prela.evals.assertions.tool.ToolSequenceAssertion","title":"<code>prela.evals.assertions.tool.ToolSequenceAssertion</code>","text":"<p>               Bases: <code>BaseAssertion</code></p> <p>Assert that tools were called in a specific order.</p> <p>This assertion verifies that tools appear in the trace in the expected sequence, though other tools may appear between them.</p> Example <p>assertion = ToolSequenceAssertion( ...     sequence=[\"web_search\", \"calculator\", \"summarize\"] ... ) result = assertion.evaluate(output=None, expected=None, trace=spans) assert result.passed</p>"},{"location":"api/evaluations/#prela.evals.assertions.tool.ToolSequenceAssertion-functions","title":"Functions","text":""},{"location":"api/evaluations/#prela.evals.assertions.tool.ToolSequenceAssertion.__init__","title":"<code>__init__(sequence, strict=False)</code>","text":"<p>Initialize tool sequence assertion.</p> <p>Parameters:</p> Name Type Description Default <code>sequence</code> <code>list[str]</code> <p>Expected sequence of tool names</p> required <code>strict</code> <code>bool</code> <p>If True, no other tools can appear between expected ones.    If False, other tools are allowed between expected sequence.</p> <code>False</code>"},{"location":"api/evaluations/#prela.evals.assertions.tool.ToolSequenceAssertion.evaluate","title":"<code>evaluate(output, expected, trace)</code>","text":"<p>Check if tools were called in the expected sequence.</p>"},{"location":"api/evaluations/#prela.evals.assertions.tool.ToolSequenceAssertion.from_config","title":"<code>from_config(config)</code>  <code>classmethod</code>","text":"<p>Create from configuration.</p> Config format <p>{     \"sequence\": [\"tool1\", \"tool2\", \"tool3\"],     \"strict\": false  # optional, default: false }</p>"},{"location":"api/evaluations/#semantic-assertions","title":"Semantic Assertions","text":""},{"location":"api/evaluations/#prela.evals.assertions.semantic.SemanticSimilarityAssertion","title":"<code>prela.evals.assertions.semantic.SemanticSimilarityAssertion</code>","text":"<p>               Bases: <code>BaseAssertion</code></p> <p>Assert that output is semantically similar to expected text.</p> <p>Uses sentence embeddings to compare semantic meaning rather than exact text matching. Useful for evaluating LLM outputs where phrasing varies but meaning should be consistent.</p> Example <p>assertion = SemanticSimilarityAssertion( ...     expected_text=\"The weather is nice today\", ...     threshold=0.8 ... ) result = assertion.evaluate( ...     output=\"Today has beautiful weather\", ...     expected=None, ...     trace=None ... ) assert result.passed  # High similarity despite different wording</p> Requires <p>pip install sentence-transformers</p>"},{"location":"api/evaluations/#prela.evals.assertions.semantic.SemanticSimilarityAssertion-functions","title":"Functions","text":""},{"location":"api/evaluations/#prela.evals.assertions.semantic.SemanticSimilarityAssertion.__init__","title":"<code>__init__(expected_text, threshold=0.8, model_name='all-MiniLM-L6-v2')</code>","text":"<p>Initialize semantic similarity assertion.</p> <p>Parameters:</p> Name Type Description Default <code>expected_text</code> <code>str</code> <p>Text to compare against</p> required <code>threshold</code> <code>float</code> <p>Minimum cosine similarity score (0-1) to pass</p> <code>0.8</code> <code>model_name</code> <code>str</code> <p>Sentence transformer model to use        (default: all-MiniLM-L6-v2, fast and accurate)</p> <code>'all-MiniLM-L6-v2'</code> <p>Raises:</p> Type Description <code>ImportError</code> <p>If sentence-transformers is not installed</p> <code>ValueError</code> <p>If threshold is not between 0 and 1</p>"},{"location":"api/evaluations/#prela.evals.assertions.semantic.SemanticSimilarityAssertion.evaluate","title":"<code>evaluate(output, expected, trace)</code>","text":"<p>Check if output is semantically similar to expected text.</p>"},{"location":"api/evaluations/#prela.evals.assertions.semantic.SemanticSimilarityAssertion.from_config","title":"<code>from_config(config)</code>  <code>classmethod</code>","text":"<p>Create from configuration.</p> Config format <p>{     \"expected_text\": \"The expected output\",     \"threshold\": 0.8,  # optional, default: 0.8     \"model_name\": \"all-MiniLM-L6-v2\"  # optional }</p>"},{"location":"api/evaluations/#prela.evals.assertions.semantic.SemanticSimilarityAssertion.clear_cache","title":"<code>clear_cache()</code>  <code>classmethod</code>","text":"<p>Clear the embedding cache. Useful for testing or memory management.</p>"},{"location":"api/evaluations/#prela.evals.assertions.semantic.SemanticSimilarityAssertion.get_cache_size","title":"<code>get_cache_size()</code>  <code>classmethod</code>","text":"<p>Get the number of cached embeddings.</p>"},{"location":"api/evaluations/#reporters","title":"Reporters","text":""},{"location":"api/evaluations/#consolereporter","title":"ConsoleReporter","text":""},{"location":"api/evaluations/#prela.evals.reporters.console.ConsoleReporter","title":"<code>prela.evals.reporters.console.ConsoleReporter</code>","text":"<p>Reporter that pretty-prints evaluation results to the console.</p> <p>Uses rich library for colored output if available, falls back to plain text formatting otherwise. Provides: - Summary statistics (pass rate, duration) - List of all test cases with pass/fail status - Detailed failure information for failed cases - Color coding (green=pass, red=fail, yellow=warning)</p> Example <p>from prela.evals import EvalRunner from prela.evals.reporters import ConsoleReporter</p> <p>runner = EvalRunner(suite, agent) result = runner.run()</p> <p>reporter = ConsoleReporter(verbose=True, use_colors=True) reporter.report(result) \u2713 Geography QA Suite \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 Total: 10 | Passed: 9 (90.0%) | Failed: 1 Duration: 2.5s ...</p>"},{"location":"api/evaluations/#prela.evals.reporters.console.ConsoleReporter-functions","title":"Functions","text":""},{"location":"api/evaluations/#prela.evals.reporters.console.ConsoleReporter.__init__","title":"<code>__init__(verbose=True, use_colors=True)</code>","text":"<p>Initialize the console reporter.</p> <p>Parameters:</p> Name Type Description Default <code>verbose</code> <code>bool</code> <p>If True, show detailed failure information. If False,      only show summary statistics and failed case names.</p> <code>True</code> <code>use_colors</code> <code>bool</code> <p>If True and rich is available, use colored output.         If False or rich unavailable, use plain text.</p> <code>True</code>"},{"location":"api/evaluations/#prela.evals.reporters.console.ConsoleReporter.report","title":"<code>report(result)</code>","text":"<p>Print the evaluation results to the console.</p> <p>Parameters:</p> Name Type Description Default <code>result</code> <code>EvalRunResult</code> <p>The evaluation run result to report.</p> required"},{"location":"api/evaluations/#jsonreporter","title":"JSONReporter","text":""},{"location":"api/evaluations/#prela.evals.reporters.json.JSONReporter","title":"<code>prela.evals.reporters.json.JSONReporter</code>","text":"<p>Reporter that writes evaluation results to a JSON file.</p> <p>Outputs a structured JSON file containing all evaluation data: - Suite metadata (name, timestamps, duration) - Summary statistics (total, passed, failed, pass rate) - Individual case results with assertion details - Full error messages and stack traces</p> <p>The JSON format is designed for: - Programmatic analysis of test results - Integration with data processing pipelines - Historical comparison of evaluation runs - CI/CD artifact storage</p> Example <p>from prela.evals import EvalRunner from prela.evals.reporters import JSONReporter</p> <p>runner = EvalRunner(suite, agent) result = runner.run()</p> <p>reporter = JSONReporter(\"results/eval_run_123.json\") reporter.report(result)</p>"},{"location":"api/evaluations/#prela.evals.reporters.json.JSONReporter--creates-resultseval_run_123json-with-full-results","title":"Creates results/eval_run_123.json with full results","text":""},{"location":"api/evaluations/#prela.evals.reporters.json.JSONReporter-functions","title":"Functions","text":""},{"location":"api/evaluations/#prela.evals.reporters.json.JSONReporter.__init__","title":"<code>__init__(output_path, indent=2)</code>","text":"<p>Initialize the JSON reporter.</p> <p>Parameters:</p> Name Type Description Default <code>output_path</code> <code>str | Path</code> <p>Path where the JSON file will be written.          Parent directories will be created if they don't exist.</p> required <code>indent</code> <code>int</code> <p>Number of spaces for JSON indentation (default: 2).     Set to None for compact output.</p> <code>2</code>"},{"location":"api/evaluations/#prela.evals.reporters.json.JSONReporter.report","title":"<code>report(result)</code>","text":"<p>Write the evaluation results to a JSON file.</p> <p>Creates parent directories if they don't exist. Overwrites any existing file at the output path.</p> <p>Parameters:</p> Name Type Description Default <code>result</code> <code>EvalRunResult</code> <p>The evaluation run result to write.</p> required <p>Raises:</p> Type Description <code>OSError</code> <p>If unable to write to the output path.</p>"},{"location":"api/evaluations/#junitreporter","title":"JUnitReporter","text":""},{"location":"api/evaluations/#prela.evals.reporters.junit.JUnitReporter","title":"<code>prela.evals.reporters.junit.JUnitReporter</code>","text":"<p>Reporter that generates JUnit XML format for CI/CD integration.</p> <p>Creates a JUnit XML file that can be consumed by continuous integration systems for test result visualization, trend analysis, and failure reporting.</p> <p>The XML format follows the JUnit schema with: -  root element with summary statistics -  elements for each test case -  elements for failed assertions -  elements for execution errors -  for additional output/trace information <p>Supported CI/CD platforms: - Jenkins (JUnit plugin) - GitLab CI/CD (junit report artifacts) - GitHub Actions (test reporters) - Azure DevOps (publish test results) - CircleCI (store_test_results)</p> Example <p>from prela.evals import EvalRunner from prela.evals.reporters import JUnitReporter</p> <p>runner = EvalRunner(suite, agent) result = runner.run()</p> <p>reporter = JUnitReporter(\"test-results/junit.xml\") reporter.report(result)</p>"},{"location":"api/evaluations/#prela.evals.reporters.junit.JUnitReporter--creates-junit-xml-at-test-resultsjunitxml","title":"Creates JUnit XML at test-results/junit.xml","text":""},{"location":"api/evaluations/#prela.evals.reporters.junit.JUnitReporter-functions","title":"Functions","text":""},{"location":"api/evaluations/#prela.evals.reporters.junit.JUnitReporter.__init__","title":"<code>__init__(output_path)</code>","text":"<p>Initialize the JUnit XML reporter.</p> <p>Parameters:</p> Name Type Description Default <code>output_path</code> <code>str | Path</code> <p>Path where the JUnit XML file will be written.          Parent directories will be created if they don't exist.</p> required"},{"location":"api/evaluations/#prela.evals.reporters.junit.JUnitReporter.report","title":"<code>report(result)</code>","text":"<p>Generate and write JUnit XML for the evaluation results.</p> <p>Creates parent directories if they don't exist. Overwrites any existing file at the output path.</p> <p>Parameters:</p> Name Type Description Default <code>result</code> <code>EvalRunResult</code> <p>The evaluation run result to convert to JUnit XML.</p> required <p>Raises:</p> Type Description <code>OSError</code> <p>If unable to write to the output path.</p>"},{"location":"api/exporters/","title":"Exporters API","text":"<p>Exporters send trace data to various backends for storage and analysis.</p>"},{"location":"api/exporters/#base-exporter","title":"Base Exporter","text":""},{"location":"api/exporters/#prela.exporters.base.BaseExporter","title":"<code>prela.exporters.base.BaseExporter</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for span exporters.</p> <p>Exporters are responsible for sending spans to external systems. Implementations must handle serialization, network requests, and error handling.</p>"},{"location":"api/exporters/#prela.exporters.base.BaseExporter-functions","title":"Functions","text":""},{"location":"api/exporters/#prela.exporters.base.BaseExporter.export","title":"<code>export(spans)</code>  <code>abstractmethod</code>","text":"<p>Export a batch of spans.</p> <p>Parameters:</p> Name Type Description Default <code>spans</code> <code>list[Span]</code> <p>List of spans to export</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>If export fails and should not be retried</p>"},{"location":"api/exporters/#prela.exporters.base.BaseExporter.shutdown","title":"<code>shutdown()</code>  <code>abstractmethod</code>","text":"<p>Shutdown the exporter and flush any pending data.</p> <p>This method should be called before the application exits to ensure all spans are properly exported.</p>"},{"location":"api/exporters/#batchexporter","title":"BatchExporter","text":""},{"location":"api/exporters/#prela.exporters.base.BatchExporter","title":"<code>prela.exporters.base.BatchExporter</code>","text":"<p>               Bases: <code>BaseExporter</code></p> <p>Base class for exporters that batch spans with retry logic.</p> <p>This class handles common batching concerns: - Retry with exponential backoff - Timeout handling - Error logging</p> <p>Subclasses only need to implement _do_export() to define how spans are actually sent to the backend.</p>"},{"location":"api/exporters/#prela.exporters.base.BatchExporter-functions","title":"Functions","text":""},{"location":"api/exporters/#prela.exporters.base.BatchExporter.__init__","title":"<code>__init__(max_retries=3, initial_backoff_ms=100.0, max_backoff_ms=10000.0, timeout_ms=30000.0)</code>","text":"<p>Initialize the batch exporter.</p> <p>Parameters:</p> Name Type Description Default <code>max_retries</code> <code>int</code> <p>Maximum number of retry attempts</p> <code>3</code> <code>initial_backoff_ms</code> <code>float</code> <p>Initial backoff delay in milliseconds</p> <code>100.0</code> <code>max_backoff_ms</code> <code>float</code> <p>Maximum backoff delay in milliseconds</p> <code>10000.0</code> <code>timeout_ms</code> <code>float</code> <p>Timeout for export operation in milliseconds</p> <code>30000.0</code>"},{"location":"api/exporters/#prela.exporters.base.BatchExporter.export","title":"<code>export(spans)</code>","text":"<p>Export spans with retry logic.</p> <p>Parameters:</p> Name Type Description Default <code>spans</code> <code>list[Span]</code> <p>List of spans to export</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If exporter is shutdown</p> <code>Exception</code> <p>If export fails after all retries</p>"},{"location":"api/exporters/#prela.exporters.base.BatchExporter.shutdown","title":"<code>shutdown()</code>","text":"<p>Shutdown the exporter.</p> <p>Subclasses can override this to implement custom shutdown logic like flushing buffers or closing connections.</p>"},{"location":"api/exporters/#prela.exporters.base.BatchExporter._do_export","title":"<code>_do_export(spans)</code>  <code>abstractmethod</code>","text":"<p>Perform the actual export operation.</p> <p>This method should be implemented by subclasses to define how spans are sent to the backend system.</p> <p>Parameters:</p> Name Type Description Default <code>spans</code> <code>list[Span]</code> <p>List of spans to export</p> required <p>Returns:</p> Type Description <code>ExportResult</code> <p>ExportResult indicating success, failure, or retry needed</p>"},{"location":"api/exporters/#exportresult","title":"ExportResult","text":""},{"location":"api/exporters/#prela.exporters.base.ExportResult","title":"<code>prela.exporters.base.ExportResult</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Result of an export operation.</p>"},{"location":"api/exporters/#console-exporter","title":"Console Exporter","text":""},{"location":"api/exporters/#prela.exporters.console.ConsoleExporter","title":"<code>prela.exporters.console.ConsoleExporter</code>","text":"<p>               Bases: <code>BaseExporter</code></p> <p>Export spans to console with pretty-printed tree visualization.</p> <p>Features: - Tree structure showing parent-child relationships - Color-coded output (when rich library is available) - Multiple verbosity levels (minimal, normal, verbose) - Duration and status indicators - Key attribute display</p> Example <pre><code>from prela.core.tracer import Tracer\nfrom prela.exporters.console import ConsoleExporter\n\ntracer = Tracer(\n    service_name=\"my-agent\",\n    exporter=ConsoleExporter(\n        verbosity=\"normal\",\n        color=True,\n        show_timestamps=True\n    )\n)\n\nwith tracer.span(\"research_agent\", span_type=\"agent\") as span:\n    with tracer.span(\"gpt-4\", span_type=\"llm\") as llm_span:\n        llm_span.set_attribute(\"llm.model\", \"gpt-4\")\n        llm_span.set_attribute(\"llm.input_tokens\", 150)\n        llm_span.set_attribute(\"llm.output_tokens\", 89)\n# Output:\n# \u2500 agent: research_agent (1.523s) \u2713\n#   \u2514\u2500 llm: gpt-4 (823ms) \u2713\n#      model: gpt-4 | tokens: 150 \u2192 89\n</code></pre>"},{"location":"api/exporters/#prela.exporters.console.ConsoleExporter-functions","title":"Functions","text":""},{"location":"api/exporters/#prela.exporters.console.ConsoleExporter.__init__","title":"<code>__init__(verbosity='normal', color=True, show_timestamps=True)</code>","text":"<p>Initialize console exporter.</p> <p>Parameters:</p> Name Type Description Default <code>verbosity</code> <code>str</code> <p>Output verbosity level: - \"minimal\": name + duration + status only - \"normal\": + key attributes (model, tokens, query) - \"verbose\": + all attributes + events</p> <code>'normal'</code> <code>color</code> <code>bool</code> <p>Enable colored output (requires rich library)</p> <code>True</code> <code>show_timestamps</code> <code>bool</code> <p>Show timestamps in output</p> <code>True</code>"},{"location":"api/exporters/#prela.exporters.console.ConsoleExporter.export","title":"<code>export(spans)</code>","text":"<p>Export spans to console with tree visualization.</p> <p>Parameters:</p> Name Type Description Default <code>spans</code> <code>list[Span]</code> <p>List of spans to export</p> required <p>Returns:</p> Type Description <code>ExportResult</code> <p>ExportResult.SUCCESS (console export never fails)</p>"},{"location":"api/exporters/#file-exporter","title":"File Exporter","text":""},{"location":"api/exporters/#prela.exporters.file.FileExporter","title":"<code>prela.exporters.file.FileExporter</code>","text":"<p>               Bases: <code>BaseExporter</code></p> <p>Export spans to JSONL files with rotation and trace management.</p> <p>Features: - Thread-safe writes using a lock - Automatic directory creation - Date-based file naming with sequence numbers - Optional file rotation based on size - Trace retrieval by trace_id - Trace listing by date range - Old trace cleanup</p> <p>File naming: traces-{date}-{sequence}.jsonl Example: traces-2025-01-26-001.jsonl</p> <p>The JSONL format writes one JSON object per line, making it easy to stream and process large trace files.</p> Example <pre><code>from prela.core.tracer import Tracer\nfrom prela.exporters.file import FileExporter\n\ntracer = Tracer(\n    service_name=\"my-app\",\n    exporter=FileExporter(\n        directory=\"./traces\",\n        max_file_size_mb=100,\n        rotate=True\n    )\n)\n\nwith tracer.span(\"operation\") as span:\n    span.set_attribute(\"key\", \"value\")\n# Span is automatically written to ./traces/traces-2025-01-26-001.jsonl\n</code></pre>"},{"location":"api/exporters/#prela.exporters.file.FileExporter-functions","title":"Functions","text":""},{"location":"api/exporters/#prela.exporters.file.FileExporter.__init__","title":"<code>__init__(directory='./traces', format='jsonl', max_file_size_mb=100, rotate=True)</code>","text":"<p>Initialize file exporter.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>str | Path</code> <p>Directory to store trace files (e.g., \"./traces\")</p> <code>'./traces'</code> <code>format</code> <code>str</code> <p>File format - \"jsonl\" or \"ndjson\" (both are equivalent)</p> <code>'jsonl'</code> <code>max_file_size_mb</code> <code>int</code> <p>Maximum file size in MB before rotation</p> <code>100</code> <code>rotate</code> <code>bool</code> <p>Whether to rotate files when size exceeded</p> <code>True</code>"},{"location":"api/exporters/#prela.exporters.file.FileExporter.export","title":"<code>export(spans)</code>","text":"<p>Export spans to file.</p> <p>Parameters:</p> Name Type Description Default <code>spans</code> <code>list[Span]</code> <p>List of spans to export</p> required <p>Returns:</p> Type Description <code>ExportResult</code> <p>ExportResult.SUCCESS if successful, ExportResult.FAILURE otherwise</p>"},{"location":"api/exporters/#prela.exporters.file.FileExporter.shutdown","title":"<code>shutdown()</code>","text":"<p>Shutdown the exporter.</p> <p>No cleanup needed for file exporter - file handle is closed after each write.</p>"},{"location":"api/exporters/#prela.exporters.file.FileExporter.list_traces","title":"<code>list_traces(start, end)</code>","text":"<p>List trace IDs within a date range.</p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>datetime</code> <p>Start datetime (inclusive)</p> required <code>end</code> <code>datetime</code> <p>End datetime (inclusive)</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List of unique trace IDs found in the date range</p>"},{"location":"api/exporters/#prela.exporters.file.FileExporter.cleanup_old_traces","title":"<code>cleanup_old_traces(days)</code>","text":"<p>Delete trace files older than specified days.</p> <p>Parameters:</p> Name Type Description Default <code>days</code> <code>int</code> <p>Delete files older than this many days (0 means keep today and delete all older)</p> required <p>Returns:</p> Type Description <code>int</code> <p>Number of files deleted</p>"},{"location":"api/instrumentation/","title":"Instrumentation API","text":"<p>Auto-instrumentation for popular LLM SDKs and frameworks.</p>"},{"location":"api/instrumentation/#base-instrumentor","title":"Base Instrumentor","text":""},{"location":"api/instrumentation/#prela.instrumentation.base.Instrumentor","title":"<code>prela.instrumentation.base.Instrumentor</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for library instrumentors.</p> <p>Instrumentors provide automatic tracing for external libraries by monkey-patching their functions to create spans around operations.</p> Example <pre><code>class OpenAIInstrumentor(Instrumentor):\n    def instrument(self, tracer: Tracer) -&gt; None:\n        # Wrap OpenAI API calls\n        wrap_function(openai, \"create\", wrapper)\n\n    def uninstrument(self) -&gt; None:\n        # Restore original functions\n        unwrap_function(openai, \"create\")\n\n    @property\n    def is_instrumented(self) -&gt; bool:\n        return hasattr(openai, _ORIGINALS_ATTR)\n</code></pre>"},{"location":"api/instrumentation/#prela.instrumentation.base.Instrumentor-attributes","title":"Attributes","text":""},{"location":"api/instrumentation/#prela.instrumentation.base.Instrumentor.is_instrumented","title":"<code>is_instrumented</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Check if this library is currently instrumented.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if instrumentation is active, False otherwise</p>"},{"location":"api/instrumentation/#prela.instrumentation.base.Instrumentor-functions","title":"Functions","text":""},{"location":"api/instrumentation/#prela.instrumentation.base.Instrumentor.instrument","title":"<code>instrument(tracer)</code>  <code>abstractmethod</code>","text":"<p>Enable instrumentation for this library.</p> <p>This method should wrap the library's functions to create spans automatically. It should be idempotent - calling it multiple times should not create multiple layers of wrapping.</p> <p>Parameters:</p> Name Type Description Default <code>tracer</code> <code>Tracer</code> <p>The tracer to use for creating spans</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If instrumentation fails</p>"},{"location":"api/instrumentation/#prela.instrumentation.base.Instrumentor.uninstrument","title":"<code>uninstrument()</code>  <code>abstractmethod</code>","text":"<p>Disable instrumentation and restore original functions.</p> <p>This method should unwrap all previously wrapped functions and restore the library to its original state. It should be idempotent - calling it when not instrumented should be a no-op.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If uninstrumentation fails</p>"},{"location":"api/instrumentation/#auto-instrumentation","title":"Auto-Instrumentation","text":""},{"location":"api/instrumentation/#prela.instrumentation.auto.auto_instrument","title":"<code>prela.instrumentation.auto.auto_instrument(tracer)</code>","text":"<p>Automatically instrument all detected libraries.</p> <p>This function: 1. Checks which supported LLM SDKs are installed 2. Imports and initializes their instrumentors 3. Calls instrument(tracer) on each 4. Returns list of successfully instrumented libraries</p> <p>The function is designed to be safe: - Missing libraries are skipped (not an error) - Instrumentation failures are logged but don't crash - Returns empty list if nothing was instrumented</p> <p>Parameters:</p> Name Type Description Default <code>tracer</code> <code>Tracer</code> <p>The tracer instance to use for instrumentation</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List of library names that were successfully instrumented</p> <code>list[str]</code> <p>(e.g., [\"anthropic\", \"openai\"])</p> Example <pre><code>from prela.core.tracer import Tracer\nfrom prela.instrumentation.auto import auto_instrument\n\ntracer = Tracer(service_name=\"my-app\")\ninstrumented = auto_instrument(tracer)\nprint(f\"Auto-instrumented: {instrumented}\")\n# Output: Auto-instrumented: ['anthropic', 'openai']\n\n# Now all calls to these SDKs are automatically traced\nfrom anthropic import Anthropic\nclient = Anthropic()\nresponse = client.messages.create(...)  # Automatically traced!\n</code></pre>"},{"location":"api/instrumentation/#openai-instrumentor","title":"OpenAI Instrumentor","text":""},{"location":"api/instrumentation/#prela.instrumentation.openai.OpenAIInstrumentor","title":"<code>prela.instrumentation.openai.OpenAIInstrumentor</code>","text":"<p>               Bases: <code>Instrumentor</code></p> <p>Instrumentor for OpenAI SDK.</p> <p>Patches the following methods: - openai.OpenAI.chat.completions.create (sync) - openai.AsyncOpenAI.chat.completions.create (async) - openai.OpenAI.completions.create (sync, legacy) - openai.OpenAI.embeddings.create (sync)</p> <p>Captures detailed information about requests, responses, tool usage, and streaming events.</p>"},{"location":"api/instrumentation/#prela.instrumentation.openai.OpenAIInstrumentor-attributes","title":"Attributes","text":""},{"location":"api/instrumentation/#prela.instrumentation.openai.OpenAIInstrumentor.is_instrumented","title":"<code>is_instrumented</code>  <code>property</code>","text":"<p>Check if OpenAI SDK is currently instrumented.</p>"},{"location":"api/instrumentation/#prela.instrumentation.openai.OpenAIInstrumentor-functions","title":"Functions","text":""},{"location":"api/instrumentation/#prela.instrumentation.openai.OpenAIInstrumentor.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the OpenAI instrumentor.</p>"},{"location":"api/instrumentation/#prela.instrumentation.openai.OpenAIInstrumentor.instrument","title":"<code>instrument(tracer)</code>","text":"<p>Enable instrumentation for OpenAI SDK.</p> <p>Parameters:</p> Name Type Description Default <code>tracer</code> <code>Tracer</code> <p>The tracer to use for creating spans</p> required <p>Raises:</p> Type Description <code>ImportError</code> <p>If openai package is not installed</p> <code>RuntimeError</code> <p>If instrumentation fails</p>"},{"location":"api/instrumentation/#prela.instrumentation.openai.OpenAIInstrumentor.uninstrument","title":"<code>uninstrument()</code>","text":"<p>Disable instrumentation and restore original functions.</p>"},{"location":"api/instrumentation/#anthropic-instrumentor","title":"Anthropic Instrumentor","text":""},{"location":"api/instrumentation/#prela.instrumentation.anthropic.AnthropicInstrumentor","title":"<code>prela.instrumentation.anthropic.AnthropicInstrumentor</code>","text":"<p>               Bases: <code>Instrumentor</code></p> <p>Instrumentor for Anthropic SDK.</p> <p>Patches the following methods: - anthropic.Anthropic.messages.create (sync) - anthropic.AsyncAnthropic.messages.create (async) - anthropic.Anthropic.messages.stream (sync) - anthropic.AsyncAnthropic.messages.stream (async)</p> <p>Captures detailed information about requests, responses, tool usage, and streaming events.</p>"},{"location":"api/instrumentation/#prela.instrumentation.anthropic.AnthropicInstrumentor-attributes","title":"Attributes","text":""},{"location":"api/instrumentation/#prela.instrumentation.anthropic.AnthropicInstrumentor.is_instrumented","title":"<code>is_instrumented</code>  <code>property</code>","text":"<p>Check if Anthropic SDK is currently instrumented.</p>"},{"location":"api/instrumentation/#prela.instrumentation.anthropic.AnthropicInstrumentor-functions","title":"Functions","text":""},{"location":"api/instrumentation/#prela.instrumentation.anthropic.AnthropicInstrumentor.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the Anthropic instrumentor.</p>"},{"location":"api/instrumentation/#prela.instrumentation.anthropic.AnthropicInstrumentor.instrument","title":"<code>instrument(tracer)</code>","text":"<p>Enable instrumentation for Anthropic SDK.</p> <p>Parameters:</p> Name Type Description Default <code>tracer</code> <code>Tracer</code> <p>The tracer to use for creating spans</p> required <p>Raises:</p> Type Description <code>ImportError</code> <p>If anthropic package is not installed</p> <code>RuntimeError</code> <p>If instrumentation fails</p>"},{"location":"api/instrumentation/#prela.instrumentation.anthropic.AnthropicInstrumentor.uninstrument","title":"<code>uninstrument()</code>","text":"<p>Disable instrumentation and restore original functions.</p>"},{"location":"api/instrumentation/#langchain-instrumentor","title":"LangChain Instrumentor","text":""},{"location":"api/instrumentation/#prela.instrumentation.langchain.LangChainInstrumentor","title":"<code>prela.instrumentation.langchain.LangChainInstrumentor</code>","text":"<p>               Bases: <code>Instrumentor</code></p> <p>Instrumentor for LangChain framework.</p> <p>This instrumentor injects a PrelaCallbackHandler into LangChain's global callback system, which automatically traces all LangChain operations including LLM calls, chains, tools, retrievers, and agent actions.</p> <p>Unlike other instrumentors that use function wrapping, this uses LangChain's built-in callback mechanism for more robust and comprehensive tracing.</p>"},{"location":"api/instrumentation/#prela.instrumentation.langchain.LangChainInstrumentor-attributes","title":"Attributes","text":""},{"location":"api/instrumentation/#prela.instrumentation.langchain.LangChainInstrumentor.is_instrumented","title":"<code>is_instrumented</code>  <code>property</code>","text":"<p>Check if LangChain is currently instrumented.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if instrumentation is active, False otherwise</p>"},{"location":"api/instrumentation/#prela.instrumentation.langchain.LangChainInstrumentor-functions","title":"Functions","text":""},{"location":"api/instrumentation/#prela.instrumentation.langchain.LangChainInstrumentor.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the LangChain instrumentor.</p>"},{"location":"api/instrumentation/#prela.instrumentation.langchain.LangChainInstrumentor.instrument","title":"<code>instrument(tracer)</code>","text":"<p>Enable instrumentation for LangChain.</p> <p>This adds a PrelaCallbackHandler to LangChain's global callback manager, which will receive events for all LangChain operations.</p> <p>Parameters:</p> Name Type Description Default <code>tracer</code> <code>Tracer</code> <p>The tracer to use for creating spans</p> required <p>Raises:</p> Type Description <code>ImportError</code> <p>If langchain-core package is not installed</p> <code>RuntimeError</code> <p>If instrumentation fails</p>"},{"location":"api/instrumentation/#prela.instrumentation.langchain.LangChainInstrumentor.uninstrument","title":"<code>uninstrument()</code>","text":"<p>Disable instrumentation and remove callback handler.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If uninstrumentation fails</p>"},{"location":"api/instrumentation/#prelacallbackhandler","title":"PrelaCallbackHandler","text":""},{"location":"api/instrumentation/#prela.instrumentation.langchain.PrelaCallbackHandler","title":"<code>prela.instrumentation.langchain.PrelaCallbackHandler</code>","text":"<p>LangChain callback handler that creates Prela spans.</p> <p>This handler implements LangChain's BaseCallbackHandler interface and creates spans for all major LangChain operations. It maintains a mapping from run_id to span to properly handle concurrent executions and nested operations.</p> <p>The handler tracks: - LLM calls: Model invocations with prompts and responses - Chains: Sequential operations and workflows - Tools: External tool invocations - Retrievers: Document retrieval operations - Agents: Agent reasoning and actions</p>"},{"location":"api/instrumentation/#prela.instrumentation.langchain.PrelaCallbackHandler-functions","title":"Functions","text":""},{"location":"api/instrumentation/#prela.instrumentation.langchain.PrelaCallbackHandler.__init__","title":"<code>__init__(tracer)</code>","text":"<p>Initialize the callback handler.</p> <p>Parameters:</p> Name Type Description Default <code>tracer</code> <code>Tracer</code> <p>The tracer to use for creating spans</p> required"},{"location":"api/instrumentation/#prela.instrumentation.langchain.PrelaCallbackHandler.on_llm_start","title":"<code>on_llm_start(serialized, prompts, *, run_id, parent_run_id=None, tags=None, metadata=None, **kwargs)</code>","text":"<p>Called when an LLM starts running.</p> <p>Parameters:</p> Name Type Description Default <code>serialized</code> <code>dict[str, Any]</code> <p>Serialized LLM configuration</p> required <code>prompts</code> <code>list[str]</code> <p>Input prompts to the LLM</p> required <code>run_id</code> <code>UUID</code> <p>Unique identifier for this LLM run</p> required <code>parent_run_id</code> <code>UUID | None</code> <p>ID of parent operation (if nested)</p> <code>None</code> <code>tags</code> <code>list[str] | None</code> <p>Optional tags for categorization</p> <code>None</code> <code>metadata</code> <code>dict[str, Any] | None</code> <p>Optional metadata</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional LLM parameters</p> <code>{}</code>"},{"location":"api/instrumentation/#prela.instrumentation.langchain.PrelaCallbackHandler.on_llm_end","title":"<code>on_llm_end(response, *, run_id, parent_run_id=None, **kwargs)</code>","text":"<p>Called when an LLM finishes running.</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>Any</code> <p>LLM response object</p> required <code>run_id</code> <code>UUID</code> <p>Unique identifier for this LLM run</p> required <code>parent_run_id</code> <code>UUID | None</code> <p>ID of parent operation</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional parameters</p> <code>{}</code>"},{"location":"api/instrumentation/#prela.instrumentation.langchain.PrelaCallbackHandler.on_llm_error","title":"<code>on_llm_error(error, *, run_id, parent_run_id=None, **kwargs)</code>","text":"<p>Called when an LLM errors.</p> <p>Parameters:</p> Name Type Description Default <code>error</code> <code>BaseException</code> <p>The error that occurred</p> required <code>run_id</code> <code>UUID</code> <p>Unique identifier for this LLM run</p> required <code>parent_run_id</code> <code>UUID | None</code> <p>ID of parent operation</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional parameters</p> <code>{}</code>"},{"location":"api/instrumentation/#prela.instrumentation.langchain.PrelaCallbackHandler.on_chain_start","title":"<code>on_chain_start(serialized, inputs, *, run_id, parent_run_id=None, tags=None, metadata=None, **kwargs)</code>","text":"<p>Called when a chain starts running.</p> <p>Parameters:</p> Name Type Description Default <code>serialized</code> <code>dict[str, Any]</code> <p>Serialized chain configuration</p> required <code>inputs</code> <code>dict[str, Any]</code> <p>Input values to the chain</p> required <code>run_id</code> <code>UUID</code> <p>Unique identifier for this chain run</p> required <code>parent_run_id</code> <code>UUID | None</code> <p>ID of parent operation</p> <code>None</code> <code>tags</code> <code>list[str] | None</code> <p>Optional tags</p> <code>None</code> <code>metadata</code> <code>dict[str, Any] | None</code> <p>Optional metadata</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional parameters</p> <code>{}</code>"},{"location":"api/instrumentation/#prela.instrumentation.langchain.PrelaCallbackHandler.on_chain_end","title":"<code>on_chain_end(outputs, *, run_id, parent_run_id=None, **kwargs)</code>","text":"<p>Called when a chain finishes running.</p> <p>Parameters:</p> Name Type Description Default <code>outputs</code> <code>dict[str, Any]</code> <p>Output values from the chain</p> required <code>run_id</code> <code>UUID</code> <p>Unique identifier for this chain run</p> required <code>parent_run_id</code> <code>UUID | None</code> <p>ID of parent operation</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional parameters</p> <code>{}</code>"},{"location":"api/instrumentation/#prela.instrumentation.langchain.PrelaCallbackHandler.on_chain_error","title":"<code>on_chain_error(error, *, run_id, parent_run_id=None, **kwargs)</code>","text":"<p>Called when a chain errors.</p> <p>Parameters:</p> Name Type Description Default <code>error</code> <code>BaseException</code> <p>The error that occurred</p> required <code>run_id</code> <code>UUID</code> <p>Unique identifier for this chain run</p> required <code>parent_run_id</code> <code>UUID | None</code> <p>ID of parent operation</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional parameters</p> <code>{}</code>"},{"location":"api/instrumentation/#prela.instrumentation.langchain.PrelaCallbackHandler.on_tool_start","title":"<code>on_tool_start(serialized, input_str, *, run_id, parent_run_id=None, tags=None, metadata=None, **kwargs)</code>","text":"<p>Called when a tool starts running.</p> <p>Parameters:</p> Name Type Description Default <code>serialized</code> <code>dict[str, Any]</code> <p>Serialized tool configuration</p> required <code>input_str</code> <code>str</code> <p>Input string to the tool</p> required <code>run_id</code> <code>UUID</code> <p>Unique identifier for this tool run</p> required <code>parent_run_id</code> <code>UUID | None</code> <p>ID of parent operation</p> <code>None</code> <code>tags</code> <code>list[str] | None</code> <p>Optional tags</p> <code>None</code> <code>metadata</code> <code>dict[str, Any] | None</code> <p>Optional metadata</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional parameters</p> <code>{}</code>"},{"location":"api/instrumentation/#prela.instrumentation.langchain.PrelaCallbackHandler.on_tool_end","title":"<code>on_tool_end(output, *, run_id, parent_run_id=None, **kwargs)</code>","text":"<p>Called when a tool finishes running.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>str</code> <p>Output from the tool</p> required <code>run_id</code> <code>UUID</code> <p>Unique identifier for this tool run</p> required <code>parent_run_id</code> <code>UUID | None</code> <p>ID of parent operation</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional parameters</p> <code>{}</code>"},{"location":"api/instrumentation/#prela.instrumentation.langchain.PrelaCallbackHandler.on_tool_error","title":"<code>on_tool_error(error, *, run_id, parent_run_id=None, **kwargs)</code>","text":"<p>Called when a tool errors.</p> <p>Parameters:</p> Name Type Description Default <code>error</code> <code>BaseException</code> <p>The error that occurred</p> required <code>run_id</code> <code>UUID</code> <p>Unique identifier for this tool run</p> required <code>parent_run_id</code> <code>UUID | None</code> <p>ID of parent operation</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional parameters</p> <code>{}</code>"},{"location":"api/instrumentation/#prela.instrumentation.langchain.PrelaCallbackHandler.on_retriever_start","title":"<code>on_retriever_start(serialized, query, *, run_id, parent_run_id=None, tags=None, metadata=None, **kwargs)</code>","text":"<p>Called when a retriever starts.</p> <p>Parameters:</p> Name Type Description Default <code>serialized</code> <code>dict[str, Any]</code> <p>Serialized retriever configuration</p> required <code>query</code> <code>str</code> <p>Query string for retrieval</p> required <code>run_id</code> <code>UUID</code> <p>Unique identifier for this retriever run</p> required <code>parent_run_id</code> <code>UUID | None</code> <p>ID of parent operation</p> <code>None</code> <code>tags</code> <code>list[str] | None</code> <p>Optional tags</p> <code>None</code> <code>metadata</code> <code>dict[str, Any] | None</code> <p>Optional metadata</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional parameters</p> <code>{}</code>"},{"location":"api/instrumentation/#prela.instrumentation.langchain.PrelaCallbackHandler.on_retriever_end","title":"<code>on_retriever_end(documents, *, run_id, parent_run_id=None, **kwargs)</code>","text":"<p>Called when a retriever finishes.</p> <p>Parameters:</p> Name Type Description Default <code>documents</code> <code>list[Any]</code> <p>Retrieved documents</p> required <code>run_id</code> <code>UUID</code> <p>Unique identifier for this retriever run</p> required <code>parent_run_id</code> <code>UUID | None</code> <p>ID of parent operation</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional parameters</p> <code>{}</code>"},{"location":"api/instrumentation/#llamaindex-instrumentor","title":"LlamaIndex Instrumentor","text":""},{"location":"api/instrumentation/#prela.instrumentation.llamaindex.LlamaIndexInstrumentor","title":"<code>prela.instrumentation.llamaindex.LlamaIndexInstrumentor</code>","text":"<p>               Bases: <code>Instrumentor</code></p> <p>Instrumentor for LlamaIndex framework.</p> <p>This instrumentor adds automatic tracing to LlamaIndex operations by injecting a callback handler into the global callback manager.</p> Example <pre><code>from prela.instrumentation.llamaindex import LlamaIndexInstrumentor\nfrom prela.core.tracer import Tracer\n\ntracer = Tracer()\ninstrumentor = LlamaIndexInstrumentor()\ninstrumentor.instrument(tracer)\n\n# All LlamaIndex operations now traced\n</code></pre>"},{"location":"api/instrumentation/#prela.instrumentation.llamaindex.LlamaIndexInstrumentor-attributes","title":"Attributes","text":""},{"location":"api/instrumentation/#prela.instrumentation.llamaindex.LlamaIndexInstrumentor.is_instrumented","title":"<code>is_instrumented</code>  <code>property</code>","text":"<p>Check if LlamaIndex is currently instrumented.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if instrumented, False otherwise</p>"},{"location":"api/instrumentation/#prela.instrumentation.llamaindex.LlamaIndexInstrumentor-functions","title":"Functions","text":""},{"location":"api/instrumentation/#prela.instrumentation.llamaindex.LlamaIndexInstrumentor.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the instrumentor.</p>"},{"location":"api/instrumentation/#prela.instrumentation.llamaindex.LlamaIndexInstrumentor.instrument","title":"<code>instrument(tracer)</code>","text":"<p>Enable instrumentation for LlamaIndex.</p> <p>Parameters:</p> Name Type Description Default <code>tracer</code> <code>Tracer</code> <p>The tracer to use for creating spans</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If llama-index-core is not installed</p>"},{"location":"api/instrumentation/#prela.instrumentation.llamaindex.LlamaIndexInstrumentor.uninstrument","title":"<code>uninstrument()</code>","text":"<p>Disable instrumentation and remove callback handler.</p>"},{"location":"api/instrumentation/#prelahandler","title":"PrelaHandler","text":""},{"location":"api/instrumentation/#prela.instrumentation.llamaindex.PrelaHandler","title":"<code>prela.instrumentation.llamaindex.PrelaHandler</code>","text":"<p>LlamaIndex callback handler that creates Prela spans.</p> <p>This handler implements LlamaIndex's BaseCallbackHandler interface and creates spans for all major LlamaIndex operations. It maintains a mapping from event_id to span to properly handle concurrent executions and nested operations.</p> <p>The handler tracks: - LLM calls: Model invocations with prompts and responses - Embeddings: Vector generation operations - Retrieval: Document retrieval with similarity scores - Query: Query engine operations - Synthesis: Response synthesis from retrieved documents</p>"},{"location":"api/instrumentation/#prela.instrumentation.llamaindex.PrelaHandler-functions","title":"Functions","text":""},{"location":"api/instrumentation/#prela.instrumentation.llamaindex.PrelaHandler.__init__","title":"<code>__init__(tracer)</code>","text":"<p>Initialize the callback handler.</p> <p>Parameters:</p> Name Type Description Default <code>tracer</code> <code>Tracer</code> <p>The tracer to use for creating spans</p> required"},{"location":"api/instrumentation/#prela.instrumentation.llamaindex.PrelaHandler.on_event_start","title":"<code>on_event_start(event_type, payload=None, event_id='', parent_id='', **kwargs)</code>","text":"<p>Called when an event starts.</p> <p>Parameters:</p> Name Type Description Default <code>event_type</code> <code>str</code> <p>Type of event (LLM, EMBEDDING, RETRIEVE, etc.)</p> required <code>payload</code> <code>Optional[dict[str, Any]]</code> <p>Event-specific data</p> <code>None</code> <code>event_id</code> <code>str</code> <p>Unique identifier for this event</p> <code>''</code> <code>parent_id</code> <code>str</code> <p>ID of parent event (if nested)</p> <code>''</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The event_id for tracking</p>"},{"location":"api/instrumentation/#prela.instrumentation.llamaindex.PrelaHandler.on_event_end","title":"<code>on_event_end(event_type, payload=None, event_id='', **kwargs)</code>","text":"<p>Called when an event ends.</p> <p>Parameters:</p> Name Type Description Default <code>event_type</code> <code>str</code> <p>Type of event (LLM, EMBEDDING, RETRIEVE, etc.)</p> required <code>payload</code> <code>Optional[dict[str, Any]]</code> <p>Event-specific response data</p> <code>None</code> <code>event_id</code> <code>str</code> <p>Unique identifier for this event</p> <code>''</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments</p> <code>{}</code>"},{"location":"api/instrumentation/#n8n-webhook-handler","title":"N8N Webhook Handler","text":""},{"location":"api/instrumentation/#prela.instrumentation.n8n.webhook.N8nWebhookHandler","title":"<code>prela.instrumentation.n8n.webhook.N8nWebhookHandler</code>","text":"<p>HTTP server for receiving n8n webhook traces locally.</p> <p>This handler runs a lightweight HTTP server that receives webhook POST requests from n8n workflows and automatically converts them into Prela spans.</p> Example <pre><code>from prela import init\nfrom prela.instrumentation.n8n.webhook import N8nWebhookHandler\n\ntracer = init(service_name=\"n8n-workflows\")\nhandler = N8nWebhookHandler(tracer, port=8787)\nhandler.start()\n\n# Configure n8n webhook node to POST to http://localhost:8787/webhook\n# Handler will automatically trace all workflow executions\n</code></pre>"},{"location":"api/instrumentation/#prela.instrumentation.n8n.webhook.N8nWebhookHandler-functions","title":"Functions","text":""},{"location":"api/instrumentation/#prela.instrumentation.n8n.webhook.N8nWebhookHandler.__init__","title":"<code>__init__(tracer, port=8787, host='0.0.0.0')</code>","text":"<p>Initialize the webhook handler.</p> <p>Parameters:</p> Name Type Description Default <code>tracer</code> <code>Tracer</code> <p>Prela tracer instance for creating spans</p> required <code>port</code> <code>int</code> <p>Port to listen on (default: 8787)</p> <code>8787</code> <code>host</code> <code>str</code> <p>Host to bind to (default: 0.0.0.0)</p> <code>'0.0.0.0'</code>"},{"location":"api/instrumentation/#prela.instrumentation.n8n.webhook.N8nWebhookHandler.start","title":"<code>start()</code>","text":"<p>Start the HTTP server.</p> <p>This method starts an aiohttp server on the configured host and port. It runs in the current event loop, so it should be called from an async context or run in a separate thread.</p>"},{"location":"api/instrumentation/#prela.instrumentation.n8n.webhook.N8nWebhookHandler.start_background","title":"<code>start_background()</code>","text":"<p>Start the HTTP server in a background thread.</p> <p>This method creates a new event loop and runs the server in it. Designed to be called from a background thread via threading.Thread.</p>"},{"location":"api/instrumentation/#prela.instrumentation.n8n.webhook.N8nWebhookHandler.stop","title":"<code>stop()</code>","text":"<p>Stop the HTTP server.</p>"},{"location":"api/instrumentation/#n8n-code-node-context","title":"N8N Code Node Context","text":""},{"location":"api/instrumentation/#prela.instrumentation.n8n.code_node.PrelaN8nContext","title":"<code>prela.instrumentation.n8n.code_node.PrelaN8nContext</code>","text":"<p>Context manager for tracing custom Python code within n8n Code nodes.</p> <p>This class creates a workflow-level span and node-level span that properly integrate with Prela's tracing infrastructure. It provides helper methods for logging LLM calls, tool calls, and retrieval operations.</p> Example <pre><code># Inside n8n Code node\nfrom prela.instrumentation.n8n import PrelaN8nContext\n\nctx = PrelaN8nContext(\n    workflow_id=$workflow.id,\n    workflow_name=$workflow.name,\n    execution_id=$execution.id,\n    node_name=$node.name\n)\n\nwith ctx:\n    # Your custom code\n    response = call_my_llm(prompt)\n    ctx.log_llm_call(\n        model=\"gpt-4\",\n        prompt=prompt,\n        response=response,\n        tokens={\"prompt\": 100, \"completion\": 50}\n    )\n\n    return [{\"json\": {\"result\": response}}]\n</code></pre>"},{"location":"api/instrumentation/#prela.instrumentation.n8n.code_node.PrelaN8nContext-functions","title":"Functions","text":""},{"location":"api/instrumentation/#prela.instrumentation.n8n.code_node.PrelaN8nContext.__init__","title":"<code>__init__(workflow_id, workflow_name, execution_id, node_name, node_type='n8n-nodes-base.code', tracer=None, api_key=None, endpoint=None)</code>","text":"<p>Initialize n8n Code node tracing context.</p> <p>Parameters:</p> Name Type Description Default <code>workflow_id</code> <code>str</code> <p>Unique workflow identifier</p> required <code>workflow_name</code> <code>str</code> <p>Human-readable workflow name</p> required <code>execution_id</code> <code>str</code> <p>Unique execution identifier</p> required <code>node_name</code> <code>str</code> <p>Name of the Code node</p> required <code>node_type</code> <code>str</code> <p>Node type identifier (default: n8n-nodes-base.code)</p> <code>'n8n-nodes-base.code'</code> <code>tracer</code> <code>Optional[Tracer]</code> <p>Prela tracer instance (defaults to global tracer)</p> <code>None</code> <code>api_key</code> <code>Optional[str]</code> <p>Optional API key for remote export</p> <code>None</code> <code>endpoint</code> <code>Optional[str]</code> <p>Optional endpoint URL for remote export</p> <code>None</code>"},{"location":"api/instrumentation/#prela.instrumentation.n8n.code_node.PrelaN8nContext.__enter__","title":"<code>__enter__()</code>","text":"<p>Start tracing context.</p>"},{"location":"api/instrumentation/#prela.instrumentation.n8n.code_node.PrelaN8nContext.__exit__","title":"<code>__exit__(exc_type, exc_val, exc_tb)</code>","text":"<p>End tracing context and export spans.</p>"},{"location":"api/instrumentation/#prela.instrumentation.n8n.code_node.PrelaN8nContext.log_llm_call","title":"<code>log_llm_call(model, prompt, response, tokens=None, provider=None, temperature=None, **kwargs)</code>","text":"<p>Log an LLM call within the Code node.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Model identifier (e.g., \"gpt-4\", \"claude-3-opus\")</p> required <code>prompt</code> <code>str</code> <p>Input prompt text</p> required <code>response</code> <code>str</code> <p>Model response text</p> required <code>tokens</code> <code>Optional[dict]</code> <p>Token usage dict with \"prompt\", \"completion\", \"total\" keys</p> <code>None</code> <code>provider</code> <code>Optional[str]</code> <p>AI provider (openai, anthropic, etc.)</p> <code>None</code> <code>temperature</code> <code>Optional[float]</code> <p>Temperature parameter used</p> <code>None</code> <code>**kwargs</code> <p>Additional attributes to attach to the span</p> <code>{}</code>"},{"location":"api/instrumentation/#prela.instrumentation.n8n.code_node.PrelaN8nContext.log_tool_call","title":"<code>log_tool_call(tool_name, input, output, error=None, **kwargs)</code>","text":"<p>Log a tool call within the Code node.</p> <p>Parameters:</p> Name Type Description Default <code>tool_name</code> <code>str</code> <p>Name of the tool/function called</p> required <code>input</code> <code>Any</code> <p>Input parameters to the tool</p> required <code>output</code> <code>Any</code> <p>Output result from the tool</p> required <code>error</code> <code>Optional[str]</code> <p>Optional error message if tool failed</p> <code>None</code> <code>**kwargs</code> <p>Additional attributes to attach to the span</p> <code>{}</code>"},{"location":"api/instrumentation/#prela.instrumentation.n8n.code_node.PrelaN8nContext.log_retrieval","title":"<code>log_retrieval(query, documents, retriever_type=None, similarity_top_k=None, **kwargs)</code>","text":"<p>Log a retrieval/search operation within the Code node.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>Search query text</p> required <code>documents</code> <code>list[dict]</code> <p>Retrieved documents (list of dicts with text/score/metadata)</p> required <code>retriever_type</code> <code>Optional[str]</code> <p>Type of retriever used (vector, keyword, hybrid)</p> <code>None</code> <code>similarity_top_k</code> <code>Optional[int]</code> <p>Number of documents requested</p> <code>None</code> <code>**kwargs</code> <p>Additional attributes to attach to the span</p> <code>{}</code>"},{"location":"api/instrumentation/#n8n-models","title":"N8N Models","text":""},{"location":"api/instrumentation/#n8nworkflowexecution","title":"N8nWorkflowExecution","text":""},{"location":"api/instrumentation/#prela.instrumentation.n8n.models.N8nWorkflowExecution","title":"<code>prela.instrumentation.n8n.models.N8nWorkflowExecution</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents a complete n8n workflow execution.</p> <p>Captures high-level metadata about a workflow run, including timing, status, node counts, and aggregate token/cost metrics.</p>"},{"location":"api/instrumentation/#prela.instrumentation.n8n.models.N8nWorkflowExecution-functions","title":"Functions","text":""},{"location":"api/instrumentation/#prela.instrumentation.n8n.models.N8nWorkflowExecution.validate_non_negative","title":"<code>validate_non_negative(v)</code>  <code>classmethod</code>","text":"<p>Ensure counts are non-negative.</p>"},{"location":"api/instrumentation/#prela.instrumentation.n8n.models.N8nWorkflowExecution.validate_cost","title":"<code>validate_cost(v)</code>  <code>classmethod</code>","text":"<p>Ensure cost is non-negative.</p>"},{"location":"api/instrumentation/#prela.instrumentation.n8n.models.N8nWorkflowExecution.validate_completed_at","title":"<code>validate_completed_at(v, info)</code>  <code>classmethod</code>","text":"<p>Ensure completed_at is after started_at if both exist.</p>"},{"location":"api/instrumentation/#prela.instrumentation.n8n.models.N8nWorkflowExecution.duration_ms","title":"<code>duration_ms()</code>","text":"<p>Calculate execution duration in milliseconds.</p>"},{"location":"api/instrumentation/#prela.instrumentation.n8n.models.N8nWorkflowExecution.to_span_attributes","title":"<code>to_span_attributes()</code>","text":"<p>Convert to Prela span attributes.</p>"},{"location":"api/instrumentation/#n8nnodeexecution","title":"N8nNodeExecution","text":""},{"location":"api/instrumentation/#prela.instrumentation.n8n.models.N8nNodeExecution","title":"<code>prela.instrumentation.n8n.models.N8nNodeExecution</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents execution of a single node within a workflow.</p> <p>Captures node-level telemetry including input/output data, timing, status, and error information.</p>"},{"location":"api/instrumentation/#prela.instrumentation.n8n.models.N8nNodeExecution-functions","title":"Functions","text":""},{"location":"api/instrumentation/#prela.instrumentation.n8n.models.N8nNodeExecution.validate_non_negative","title":"<code>validate_non_negative(v)</code>  <code>classmethod</code>","text":"<p>Ensure counts are non-negative.</p>"},{"location":"api/instrumentation/#prela.instrumentation.n8n.models.N8nNodeExecution.validate_completed_at","title":"<code>validate_completed_at(v, info)</code>  <code>classmethod</code>","text":"<p>Ensure completed_at is after started_at if both exist.</p>"},{"location":"api/instrumentation/#prela.instrumentation.n8n.models.N8nNodeExecution.duration_ms","title":"<code>duration_ms()</code>","text":"<p>Calculate node execution duration in milliseconds.</p>"},{"location":"api/instrumentation/#prela.instrumentation.n8n.models.N8nNodeExecution.to_span_attributes","title":"<code>to_span_attributes()</code>","text":"<p>Convert to Prela span attributes.</p>"},{"location":"api/instrumentation/#n8nainodeexecution","title":"N8nAINodeExecution","text":""},{"location":"api/instrumentation/#prela.instrumentation.n8n.models.N8nAINodeExecution","title":"<code>prela.instrumentation.n8n.models.N8nAINodeExecution</code>","text":"<p>               Bases: <code>N8nNodeExecution</code></p> <p>Represents execution of an AI-specific node (LLM, agent, vector store, etc.).</p> <p>Extends N8nNodeExecution with AI-specific telemetry including model info, token usage, costs, prompts, and retrieval data.</p>"},{"location":"api/instrumentation/#prela.instrumentation.n8n.models.N8nAINodeExecution-functions","title":"Functions","text":""},{"location":"api/instrumentation/#prela.instrumentation.n8n.models.N8nAINodeExecution.validate_non_negative_tokens","title":"<code>validate_non_negative_tokens(v)</code>  <code>classmethod</code>","text":"<p>Ensure token counts are non-negative.</p>"},{"location":"api/instrumentation/#prela.instrumentation.n8n.models.N8nAINodeExecution.validate_cost","title":"<code>validate_cost(v)</code>  <code>classmethod</code>","text":"<p>Ensure cost is non-negative.</p>"},{"location":"api/instrumentation/#prela.instrumentation.n8n.models.N8nAINodeExecution.validate_temperature","title":"<code>validate_temperature(v)</code>  <code>classmethod</code>","text":"<p>Ensure temperature is in valid range.</p>"},{"location":"api/instrumentation/#prela.instrumentation.n8n.models.N8nAINodeExecution.to_span_attributes","title":"<code>to_span_attributes()</code>","text":"<p>Convert to Prela span attributes with AI-specific fields.</p>"},{"location":"api/instrumentation/#prela.instrumentation.n8n.models.N8nAINodeExecution.infer_span_type","title":"<code>infer_span_type()</code>","text":"<p>Infer the appropriate span type based on node characteristics.</p> <p>Returns:</p> Type Description <code>N8nSpanType</code> <p>N8nSpanType appropriate for this node's function</p>"},{"location":"api/replay/","title":"Replay API Reference","text":"<p>Complete API documentation for the Prela replay module.</p>"},{"location":"api/replay/#module-prelareplay","title":"Module: <code>prela.replay</code>","text":"<p>Main replay functionality for deterministic re-execution of traces.</p>"},{"location":"api/replay/#replayengine","title":"ReplayEngine","text":"<p>Main engine for replaying captured traces.</p> <pre><code>from prela.replay import ReplayEngine\n</code></pre>"},{"location":"api/replay/#constructor","title":"Constructor","text":"<pre><code>ReplayEngine(\n    trace: Trace,\n    max_retries: int = 3,\n    retry_initial_delay: float = 1.0,\n    retry_max_delay: float = 60.0,\n    retry_exponential_base: float = 2.0,\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>trace</code> (Trace): Loaded trace object from TraceLoader</li> <li><code>max_retries</code> (int, optional): Maximum retry attempts for API calls (default: 3)</li> <li><code>retry_initial_delay</code> (float, optional): Initial delay before first retry in seconds (default: 1.0)</li> <li><code>retry_max_delay</code> (float, optional): Maximum delay between retries in seconds (default: 60.0)</li> <li><code>retry_exponential_base</code> (float, optional): Base for exponential backoff (default: 2.0)</li> </ul> <p>Example:</p> <pre><code>from prela.replay.loader import TraceLoader\n\ntrace = TraceLoader.from_file(\"trace.jsonl\")\n\n# Default retry configuration\nengine = ReplayEngine(trace)\n\n# Custom retry configuration (more aggressive for flaky networks)\nengine = ReplayEngine(\n    trace,\n    max_retries=5,\n    retry_initial_delay=2.0,\n    retry_max_delay=120.0,\n)\n</code></pre>"},{"location":"api/replay/#methods","title":"Methods","text":""},{"location":"api/replay/#replay_exact","title":"<code>replay_exact()</code>","text":"<p>Execute deterministic replay using cached data (no API calls).</p> <pre><code>def replay_exact() -&gt; ReplayResult\n</code></pre> <p>Returns: <code>ReplayResult</code> with replayed spans and metrics</p> <p>Example:</p> <pre><code>result = engine.replay_exact()\nprint(f\"Duration: {result.total_duration_ms}ms\")\nprint(f\"Cost: ${result.total_cost_usd:.4f}\")\n</code></pre>"},{"location":"api/replay/#replay_with_modifications","title":"<code>replay_with_modifications()</code>","text":"<p>Execute replay with parameter modifications (makes real API calls for modified spans).</p> <pre><code>def replay_with_modifications(\n    model: str | None = None,\n    temperature: float | None = None,\n    system_prompt: str | None = None,\n    max_tokens: int | None = None,\n    mock_tool_responses: dict[str, Any] | None = None,\n    mock_retrieval_results: list[dict[str, Any]] | None = None,\n    enable_tool_execution: bool = False,\n    tool_execution_allowlist: list[str] | None = None,\n    tool_execution_blocklist: list[str] | None = None,\n    tool_registry: dict[str, Any] | None = None,\n    enable_retrieval_execution: bool = False,\n    retrieval_client: Any | None = None,\n    retrieval_query_override: str | None = None,\n    stream: bool = False,\n    stream_callback: Callable[[str], None] | None = None,\n) -&gt; ReplayResult\n</code></pre> <p>LLM Parameters:</p> <ul> <li><code>model</code> (str, optional): Override LLM model name</li> <li>Examples: <code>\"gpt-4o\"</code>, <code>\"claude-sonnet-4-20250514\"</code></li> <li><code>temperature</code> (float, optional): Override temperature (0.0-1.0)</li> <li><code>system_prompt</code> (str, optional): Override system instructions</li> <li><code>max_tokens</code> (int, optional): Override max output tokens</li> <li><code>stream</code> (bool, optional): Enable streaming responses (default: False)</li> <li><code>stream_callback</code> (callable, optional): Callback for streaming chunks: <code>(chunk: str) -&gt; None</code></li> </ul> <p>Tool Parameters:</p> <ul> <li><code>mock_tool_responses</code> (dict, optional): Mock tool outputs (highest priority)</li> <li>Format: <code>{tool_name: {output_data}}</code></li> <li><code>enable_tool_execution</code> (bool, optional): Re-execute tools instead of using cached data (default: False)</li> <li><code>tool_execution_allowlist</code> (list[str], optional): Only execute tools in this list</li> <li>If provided, tools not in list will fail with error</li> <li><code>tool_execution_blocklist</code> (list[str], optional): Never execute tools in this list</li> <li>Blocklist takes precedence over allowlist</li> <li><code>tool_registry</code> (dict, optional): Map of tool names to callable functions</li> <li>Required when <code>enable_tool_execution=True</code></li> <li>Format: <code>{tool_name: callable}</code></li> </ul> <p>Retrieval Parameters:</p> <ul> <li><code>mock_retrieval_results</code> (list[dict], optional): Mock retrieval documents (highest priority)</li> <li>Format: <code>[{\"text\": \"...\", \"score\": 0.9}, ...]</code></li> <li><code>enable_retrieval_execution</code> (bool, optional): Re-query vector database (default: False)</li> <li><code>retrieval_client</code> (Any, optional): Vector database client (ChromaDB, Pinecone, Qdrant, Weaviate)</li> <li>Required when <code>enable_retrieval_execution=True</code></li> <li><code>retrieval_query_override</code> (str, optional): Override query for all retrieval spans</li> </ul> <p>Returns: <code>ReplayResult</code> with modified execution</p> <p>Priority System:</p> <p>For tool and retrieval spans, Prela uses a 3-tier priority system:</p> <ol> <li>Mock responses (highest priority) - Always used if provided</li> <li>Real execution - Used if enabled and no mocks provided</li> <li>Cached data (default) - Original captured data</li> </ol> <p>This ensures predictable behavior and prevents accidental tool execution.</p> <p>Basic Example:</p> <pre><code># LLM parameter modification\nresult = engine.replay_with_modifications(\n    model=\"gpt-4o\",\n    temperature=0.7,\n    system_prompt=\"Be concise and direct\"\n)\n</code></pre> <p>Tool Re-execution Example:</p> <pre><code># Define tool registry\ndef my_calculator(input_data):\n    return {\"result\": input_data[\"a\"] + input_data[\"b\"]}\n\ntool_registry = {\"calculator\": my_calculator}\n\n# Re-execute tools with allowlist\nresult = engine.replay_with_modifications(\n    enable_tool_execution=True,\n    tool_execution_allowlist=[\"calculator\"],  # Only allow calculator\n    tool_registry=tool_registry,\n)\n</code></pre> <p>Retrieval Re-execution Example:</p> <pre><code>import chromadb\n\n# Setup ChromaDB client\nclient = chromadb.Client()\n\n# Re-query vector database\nresult = engine.replay_with_modifications(\n    enable_retrieval_execution=True,\n    retrieval_client=client,\n    retrieval_query_override=\"Updated search query\",  # Optional\n)\n</code></pre>"},{"location":"api/replay/#compare_replays","title":"compare_replays()","text":"<p>Compare two replay results and generate difference report.</p> <pre><code>from prela.replay import compare_replays\n</code></pre> <pre><code>def compare_replays(\n    original: ReplayResult,\n    modified: ReplayResult\n) -&gt; ReplayComparison\n</code></pre> <p>Parameters:</p> <ul> <li><code>original</code> (ReplayResult): Baseline replay result</li> <li><code>modified</code> (ReplayResult): Modified replay result</li> </ul> <p>Returns: <code>ReplayComparison</code> with differences and summary</p> <p>Example:</p> <pre><code>original = engine.replay_exact()\nmodified = engine.replay_with_modifications(model=\"gpt-4o\")\n\ncomparison = compare_replays(original, modified)\nprint(comparison.generate_summary())\n</code></pre>"},{"location":"api/replay/#module-prelareplayloader","title":"Module: <code>prela.replay.loader</code>","text":"<p>Load traces from various formats.</p>"},{"location":"api/replay/#traceloader","title":"TraceLoader","text":"<p>Utility for loading traces from files or data structures.</p> <pre><code>from prela.replay.loader import TraceLoader\n</code></pre>"},{"location":"api/replay/#class-methods","title":"Class Methods","text":""},{"location":"api/replay/#from_file","title":"<code>from_file()</code>","text":"<p>Load trace from JSON or JSONL file.</p> <pre><code>@classmethod\ndef from_file(cls, file_path: str) -&gt; Trace\n</code></pre> <p>Parameters:</p> <ul> <li><code>file_path</code> (str): Path to trace file (.json or .jsonl)</li> </ul> <p>Returns: <code>Trace</code> object</p> <p>Example:</p> <pre><code>trace = TraceLoader.from_file(\"traces.jsonl\")\n</code></pre>"},{"location":"api/replay/#from_dict","title":"<code>from_dict()</code>","text":"<p>Load trace from dictionary.</p> <pre><code>@classmethod\ndef from_dict(cls, data: dict) -&gt; Trace\n</code></pre> <p>Parameters:</p> <ul> <li><code>data</code> (dict): Trace dictionary with <code>trace_id</code> and <code>spans</code> keys</li> </ul> <p>Returns: <code>Trace</code> object</p> <p>Example:</p> <pre><code>trace_dict = {\n    \"trace_id\": \"abc-123\",\n    \"spans\": [...]\n}\ntrace = TraceLoader.from_dict(trace_dict)\n</code></pre>"},{"location":"api/replay/#from_spans","title":"<code>from_spans()</code>","text":"<p>Load trace from list of Span objects.</p> <pre><code>@classmethod\ndef from_spans(cls, spans: list[Span]) -&gt; Trace\n</code></pre> <p>Parameters:</p> <ul> <li><code>spans</code> (list[Span]): List of Span objects</li> </ul> <p>Returns: <code>Trace</code> object</p> <p>Example:</p> <pre><code>from prela.core import Span\n\nspans = [span1, span2, span3]\ntrace = TraceLoader.from_spans(spans)\n</code></pre>"},{"location":"api/replay/#trace","title":"Trace","text":"<p>Represents a loaded trace with span tree structure.</p>"},{"location":"api/replay/#properties","title":"Properties","text":"<pre><code>@property\ndef trace_id(self) -&gt; str\n    \"\"\"Trace ID (from first span).\"\"\"\n\n@property\ndef spans(self) -&gt; list[Span]\n    \"\"\"All spans in trace.\"\"\"\n\n@property\ndef root_spans(self) -&gt; list[Span]\n    \"\"\"Root spans (no parent).\"\"\"\n</code></pre>"},{"location":"api/replay/#methods_1","title":"Methods","text":""},{"location":"api/replay/#walk_depth_first","title":"<code>walk_depth_first()</code>","text":"<p>Traverse spans in depth-first order.</p> <pre><code>def walk_depth_first(self) -&gt; list[Span]\n</code></pre> <p>Returns: Ordered list of spans</p> <p>Example:</p> <pre><code>trace = TraceLoader.from_file(\"trace.jsonl\")\nfor span in trace.walk_depth_first():\n    print(f\"{span.name} ({span.span_type})\")\n</code></pre>"},{"location":"api/replay/#data-classes","title":"Data Classes","text":""},{"location":"api/replay/#replayresult","title":"ReplayResult","text":"<p>Result of a replay execution.</p> <pre><code>from prela.replay import ReplayResult\n</code></pre>"},{"location":"api/replay/#attributes","title":"Attributes","text":"<pre><code>@dataclass\nclass ReplayResult:\n    trace_id: str\n    \"\"\"Trace ID.\"\"\"\n\n    spans: list[ReplayedSpan]\n    \"\"\"Replayed spans with outputs.\"\"\"\n\n    total_duration_ms: float\n    \"\"\"Total execution duration in milliseconds.\"\"\"\n\n    total_tokens: int\n    \"\"\"Total token usage across all spans.\"\"\"\n\n    total_cost_usd: float\n    \"\"\"Total estimated cost in USD.\"\"\"\n\n    metadata: dict[str, Any]\n    \"\"\"Additional metadata.\"\"\"\n</code></pre>"},{"location":"api/replay/#replayedspan","title":"ReplayedSpan","text":"<p>Individual span replay result.</p> <pre><code>from prela.replay import ReplayedSpan\n</code></pre>"},{"location":"api/replay/#attributes_1","title":"Attributes","text":"<pre><code>@dataclass\nclass ReplayedSpan:\n    span_id: str\n    \"\"\"Span ID.\"\"\"\n\n    name: str\n    \"\"\"Span name.\"\"\"\n\n    span_type: str\n    \"\"\"Span type (llm, tool, retrieval, etc).\"\"\"\n\n    output: str\n    \"\"\"Span output (response, result, etc).\"\"\"\n\n    duration_ms: float\n    \"\"\"Span duration in milliseconds.\"\"\"\n\n    tokens: int | None\n    \"\"\"Token usage (LLM spans only).\"\"\"\n\n    cost_usd: float | None\n    \"\"\"Estimated cost in USD (LLM spans only).\"\"\"\n\n    attributes: dict[str, Any]\n    \"\"\"Span attributes.\"\"\"\n\n    error: str | None\n    \"\"\"Error message if span failed.\"\"\"\n\n    retry_count: int\n    \"\"\"Number of retry attempts (0 if no retries).\"\"\"\n</code></pre> <p>Example: Checking Retry Counts</p> <pre><code>result = engine.replay_with_modifications(model=\"gpt-4o\")\n\n# Check which spans required retries\nfor span in result.spans:\n    if span.retry_count &gt; 0:\n        print(f\"{span.name} required {span.retry_count} retries\")\n</code></pre>"},{"location":"api/replay/#replaycomparison","title":"ReplayComparison","text":"<p>Comparison between two replay results.</p> <pre><code>from prela.replay import ReplayComparison\n</code></pre>"},{"location":"api/replay/#attributes_2","title":"Attributes","text":"<pre><code>@dataclass\nclass ReplayComparison:\n    original: ReplayResult\n    \"\"\"Original replay result.\"\"\"\n\n    modified: ReplayResult\n    \"\"\"Modified replay result.\"\"\"\n\n    differences: list[SpanDifference]\n    \"\"\"List of differences found.\"\"\"\n</code></pre>"},{"location":"api/replay/#methods_2","title":"Methods","text":""},{"location":"api/replay/#generate_summary","title":"<code>generate_summary()</code>","text":"<p>Generate human-readable comparison summary.</p> <pre><code>def generate_summary() -&gt; str\n</code></pre> <p>Returns: Formatted summary string</p> <p>Example:</p> <pre><code>comparison = compare_replays(original, modified)\nprint(comparison.generate_summary())\n</code></pre>"},{"location":"api/replay/#spandifference","title":"SpanDifference","text":"<p>Represents a difference between two span executions.</p> <pre><code>from prela.replay import SpanDifference\n</code></pre>"},{"location":"api/replay/#attributes_3","title":"Attributes","text":"<pre><code>@dataclass\nclass SpanDifference:\n    span_id: str\n    \"\"\"Span ID.\"\"\"\n\n    span_name: str\n    \"\"\"Span name.\"\"\"\n\n    field: str\n    \"\"\"Field that changed (output, tokens, cost, etc).\"\"\"\n\n    original_value: Any\n    \"\"\"Original value.\"\"\"\n\n    new_value: Any\n    \"\"\"New value.\"\"\"\n\n    semantic_similarity: float | None\n    \"\"\"Semantic similarity score (0.0-1.0) for text outputs.\"\"\"\n</code></pre>"},{"location":"api/replay/#module-prelareplaycomparison","title":"Module: <code>prela.replay.comparison</code>","text":"<p>Compare replay results with semantic analysis.</p>"},{"location":"api/replay/#replaycomparator","title":"ReplayComparator","text":"<p>Comparison engine with semantic similarity.</p> <pre><code>from prela.replay.comparison import ReplayComparator\n</code></pre>"},{"location":"api/replay/#constructor_1","title":"Constructor","text":"<pre><code>ReplayComparator(use_semantic_similarity: bool = True)\n</code></pre> <p>Parameters:</p> <ul> <li><code>use_semantic_similarity</code> (bool): Enable semantic text comparison</li> <li>Requires <code>sentence-transformers</code> package</li> <li>Gracefully degrades if not installed</li> </ul>"},{"location":"api/replay/#methods_3","title":"Methods","text":""},{"location":"api/replay/#compare","title":"<code>compare()</code>","text":"<p>Compare two replay results.</p> <pre><code>def compare(\n    original: ReplayResult,\n    modified: ReplayResult\n) -&gt; ReplayComparison\n</code></pre> <p>Parameters:</p> <ul> <li><code>original</code> (ReplayResult): Baseline result</li> <li><code>modified</code> (ReplayResult): Modified result</li> </ul> <p>Returns: <code>ReplayComparison</code> object</p> <p>Example:</p> <pre><code>comparator = ReplayComparator(use_semantic_similarity=True)\ncomparison = comparator.compare(original, modified)\n</code></pre>"},{"location":"api/replay/#cli-commands","title":"CLI Commands","text":""},{"location":"api/replay/#prela-replay","title":"<code>prela replay</code>","text":"<p>Replay traces from command line.</p> <pre><code>prela replay TRACE_FILE [OPTIONS]\n</code></pre>"},{"location":"api/replay/#arguments","title":"Arguments","text":"<ul> <li><code>TRACE_FILE</code>: Path to trace file (.json or .jsonl)</li> </ul>"},{"location":"api/replay/#options","title":"Options","text":"Option Description Default <code>--model TEXT</code> Override LLM model None <code>--temperature FLOAT</code> Set temperature (0.0-1.0) None <code>--system-prompt TEXT</code> Override system prompt None <code>--max-tokens INT</code> Set max output tokens None <code>--compare</code> Compare with original False <code>--output PATH</code> Save results to file None"},{"location":"api/replay/#examples","title":"Examples","text":"<pre><code># Exact replay\nprela replay trace.json\n\n# Modified replay\nprela replay trace.json --model gpt-4o\n\n# With comparison\nprela replay trace.json --model gpt-4o --compare\n\n# Save results\nprela replay trace.json --model gpt-4o --output result.json\n\n# Multiple parameters\nprela replay trace.json \\\n  --model claude-sonnet-4 \\\n  --temperature 0.7 \\\n  --compare\n</code></pre>"},{"location":"api/replay/#environment-variables","title":"Environment Variables","text":""},{"location":"api/replay/#prela_capture_for_replay","title":"<code>PRELA_CAPTURE_FOR_REPLAY</code>","text":"<p>Enable replay capture globally.</p> <pre><code>export PRELA_CAPTURE_FOR_REPLAY=true\n</code></pre> <p>Equivalent to:</p> <pre><code>prela.init(capture_for_replay=True)\n</code></pre>"},{"location":"api/replay/#type-hints","title":"Type Hints","text":"<p>All replay APIs include full type hints for IDE support:</p> <pre><code>from typing import Any\n\nfrom prela.replay import ReplayEngine, ReplayResult, compare_replays\nfrom prela.replay.loader import Trace, TraceLoader\n\ndef analyze_trace(file_path: str, new_model: str) -&gt; dict[str, Any]:\n    \"\"\"Analyze trace with new model.\"\"\"\n    trace: Trace = TraceLoader.from_file(file_path)\n    engine: ReplayEngine = ReplayEngine(trace)\n\n    original: ReplayResult = engine.replay_exact()\n    modified: ReplayResult = engine.replay_with_modifications(model=new_model)\n\n    comparison = compare_replays(original, modified)\n\n    return {\n        \"original_cost\": original.total_cost_usd,\n        \"modified_cost\": modified.total_cost_usd,\n        \"summary\": comparison.generate_summary(),\n    }\n</code></pre>"},{"location":"api/replay/#error-handling","title":"Error Handling","text":""},{"location":"api/replay/#common-exceptions","title":"Common Exceptions","text":""},{"location":"api/replay/#filenotfounderror","title":"<code>FileNotFoundError</code>","text":"<p>Raised when trace file doesn't exist:</p> <pre><code>try:\n    trace = TraceLoader.from_file(\"missing.jsonl\")\nexcept FileNotFoundError:\n    print(\"Trace file not found\")\n</code></pre>"},{"location":"api/replay/#valueerror","title":"<code>ValueError</code>","text":"<p>Raised when trace data is invalid:</p> <pre><code>try:\n    trace = TraceLoader.from_dict({\"invalid\": \"data\"})\nexcept ValueError as e:\n    print(f\"Invalid trace: {e}\")\n</code></pre>"},{"location":"api/replay/#apierror","title":"<code>APIError</code>","text":"<p>Raised when API calls fail during modified replay:</p> <pre><code>try:\n    result = engine.replay_with_modifications(model=\"gpt-4o\")\nexcept Exception as e:\n    print(f\"API error: {e}\")\n</code></pre>"},{"location":"api/replay/#performance-considerations","title":"Performance Considerations","text":""},{"location":"api/replay/#memory-usage","title":"Memory Usage","text":"<p>Replay engines store:</p> <ul> <li>Original trace spans: O(n)</li> <li>Replay results: O(n)</li> <li>Comparison differences: O(n)</li> </ul> <p>Recommendation: For large traces (&gt;10,000 spans), process in batches.</p>"},{"location":"api/replay/#semantic-similarity","title":"Semantic Similarity","text":"<p>Computing semantic similarity requires:</p> <ul> <li>Sentence-transformers model: ~90MB download (first use)</li> <li>Embedding computation: ~10-50ms per span</li> </ul> <p>Recommendation: Disable for quick comparisons without quality analysis:</p> <pre><code>comparator = ReplayComparator(use_semantic_similarity=False)\n</code></pre>"},{"location":"api/replay/#next-steps","title":"Next Steps","text":"<ul> <li>Replay Concepts: Understand replay fundamentals</li> <li>Replay Examples: Practical code examples</li> <li>CLI Reference: Command-line usage</li> </ul>"},{"location":"cli/CLI_SCREENSHOT_GUIDE/","title":"Prela CLI Screenshot Guide","text":"<p>Purpose: Visual demonstration guide for CLI features (FAQs, marketing, documentation)</p> <p>Status: Ready for screenshots</p>"},{"location":"cli/CLI_SCREENSHOT_GUIDE/#screenshot-checklist","title":"\ud83d\udcf8 Screenshot Checklist","text":"<p>Use this guide to capture professional screenshots of Prela CLI in action.</p>"},{"location":"cli/CLI_SCREENSHOT_GUIDE/#terminal-setup-for-screenshots","title":"Terminal Setup for Screenshots","text":"<pre><code># Recommended terminal settings for clean screenshots:\n# - Font: SF Mono, Menlo, or Monaco (14-16pt)\n# - Theme: Dark theme with good contrast\n# - Window size: 100 columns x 30 rows (standard)\n# - Clear terminal before each screenshot: clear or cmd+K\n</code></pre>"},{"location":"cli/CLI_SCREENSHOT_GUIDE/#1-getting-started","title":"1. Getting Started","text":""},{"location":"cli/CLI_SCREENSHOT_GUIDE/#screenshot-11-installation","title":"Screenshot 1.1: Installation","text":"<pre><code># Show installation process\npip install prela\n\n# Expected output:\n# Collecting prela\n# Downloading prela-0.1.0-py3-none-any.whl\n# Installing collected packages: prela\n# Successfully installed prela-0.1.0\n</code></pre>"},{"location":"cli/CLI_SCREENSHOT_GUIDE/#screenshot-12-version-check","title":"Screenshot 1.2: Version Check","text":"<pre><code>prela --version\n\n# Expected output:\n# Prela CLI v0.1.0\n</code></pre>"},{"location":"cli/CLI_SCREENSHOT_GUIDE/#screenshot-13-help-overview","title":"Screenshot 1.3: Help Overview","text":"<pre><code>prela --help\n\n# Expected output:\n# Prela - AI Agent Observability Platform CLI\n#\n# Usage: prela [OPTIONS] COMMAND [ARGS]...\n#\n# Commands:\n#   list      List recent traces\n#   show      Show detailed trace information\n#   search    Search traces by keyword\n#   replay    Replay a trace with modifications\n#   explore   Launch interactive trace explorer\n#   last      Show most recent trace\n#   errors    Show failed traces\n#   tail      Follow new traces in real-time\n#   eval      Run evaluation suites\n</code></pre>"},{"location":"cli/CLI_SCREENSHOT_GUIDE/#2-basic-commands","title":"2. Basic Commands","text":""},{"location":"cli/CLI_SCREENSHOT_GUIDE/#screenshot-21-list-traces-empty-state","title":"Screenshot 2.1: List Traces (Empty State)","text":"<pre><code># First run with no traces yet\nprela list\n\n# Expected output:\n# No traces found in ./traces\n#\n# To start tracing:\n#   1. Import prela in your code: import prela\n#   2. Initialize: prela.init(service_name=\"my-app\", exporter=\"file\")\n#   3. Run your AI agent\n#   4. Traces will appear here!\n</code></pre>"},{"location":"cli/CLI_SCREENSHOT_GUIDE/#screenshot-22-list-traces-with-data","title":"Screenshot 2.2: List Traces (With Data)","text":"<pre><code># After running some scenarios\nprela list\n\n# Expected output (formatted table):\n#                             Recent Traces (17 of 17)\n# \u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n# \u2503 Trace ID         \u2503 Root Span     \u2503 Duration \u2503 Status  \u2503 Spans \u2503 Time         \u2503\n# \u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n# \u2502 aa538cdd-d1d5-4b \u2502 reasoning_fl\u2026 \u2502   10.99s \u2502 success \u2502     7 \u2502 2026-01-30   \u2502\n# \u2502 427ef3a7-58a3-4a \u2502 rapid_reques\u2026 \u2502    9.03s \u2502 success \u2502     6 \u2502 2026-01-30   \u2502\n# \u2502 56cee896-936f-46 \u2502 reasoning_fl\u2026 \u2502    8.12s \u2502 error   \u2502     4 \u2502 2026-01-30   \u2502\n# \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"cli/CLI_SCREENSHOT_GUIDE/#screenshot-23-list-with-filters","title":"Screenshot 2.3: List with Filters","text":"<pre><code># Show filtering capabilities\nprela list --limit 5\n\n# Expected output:\n#                             Recent Traces (5 of 17)\n# [Same table format, but only 5 rows]\n</code></pre> <pre><code># Time-based filtering\nprela list --since 1h\n\n# Expected output:\n#                       Recent Traces (Last Hour: 12 of 17)\n# [Table with traces from last hour only]\n</code></pre>"},{"location":"cli/CLI_SCREENSHOT_GUIDE/#3-show-trace-details","title":"3. Show Trace Details","text":""},{"location":"cli/CLI_SCREENSHOT_GUIDE/#screenshot-31-compact-tree-view","title":"Screenshot 3.1: Compact Tree View","text":"<pre><code># Clean hierarchical view\nprela show aa538cdd-d1d5-4b --compact\n\n# Expected output:\n# Trace: aa538cdd-d1d5-4b1a-a5c6-2895041bd236\n#\n# reasoning_flow (agent) success 10.99s\n# \u251c\u2500\u2500 step_1_analyze (custom) success 3.97s\n# \u2502   \u2514\u2500\u2500 anthropic.messages.create (llm) success 3.97s\n# \u251c\u2500\u2500 step_2_solve (custom) success 2.35s\n# \u2502   \u2514\u2500\u2500 anthropic.messages.create (llm) success 2.35s\n# \u2514\u2500\u2500 step_3_verify (custom) success 4.67s\n#     \u2514\u2500\u2500 anthropic.messages.create (llm) success 4.67s\n#\n# \ud83d\udca1 Tip: Run without --compact to see full span details\n</code></pre>"},{"location":"cli/CLI_SCREENSHOT_GUIDE/#screenshot-32-full-trace-details","title":"Screenshot 3.2: Full Trace Details","text":"<pre><code># Show detailed span information\nprela show aa538cdd-d1d5-4b\n\n# Expected output:\n# Trace: aa538cdd-d1d5-4b1a-a5c6-2895041bd236\n# Started: 2026-01-30 03:15:42\n# Duration: 10.99s\n# Status: SUCCESS\n#\n# reasoning_flow (agent) success 10.99s\n# \u251c\u2500\u2500 step_1_analyze (custom) success 3.97s\n# \u2502   \u2514\u2500\u2500 anthropic.messages.create (llm) success 3.97s\n# \u251c\u2500\u2500 step_2_solve (custom) success 2.35s\n# \u2502   \u2514\u2500\u2500 anthropic.messages.create (llm) success 2.35s\n# \u2514\u2500\u2500 step_3_verify (custom) success 4.67s\n#     \u2514\u2500\u2500 anthropic.messages.create (llm) success 4.67s\n#\n# Span Details:\n#\n# reasoning_flow\n#   Span ID: root-span-123\n#   Type: agent\n#   Status: success\n#   Duration: 10.99s\n#   Attributes:\n#     task: mathematical_reasoning\n#     service.name: test-multi-step\n#\n# step_1_analyze\n#   Span ID: span-456\n#   Type: custom\n#   Status: success\n#   Duration: 3.97s\n#   Parent: reasoning_flow\n#   Attributes:\n#     result: To calculate 15 * 23, we can break it down...\n#\n# [... more span details ...]\n</code></pre>"},{"location":"cli/CLI_SCREENSHOT_GUIDE/#4-search-functionality","title":"4. Search Functionality","text":""},{"location":"cli/CLI_SCREENSHOT_GUIDE/#screenshot-41-search-by-keyword","title":"Screenshot 4.1: Search by Keyword","text":"<pre><code># Search across all traces\nprela search \"reasoning_flow\"\n\n# Expected output:\n# Found 3 traces matching 'reasoning_flow'\n#\n#                                 Search Results\n# \u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n# \u2503 Trace ID         \u2503 Root Span           \u2503 Matching Spans \u2503 Status \u2503\n# \u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n# \u2502 aa538cdd-d1d5-4b \u2502 reasoning_flow      \u2502              1 \u2502 success\u2502\n# \u2502 56cee896-936f-46 \u2502 reasoning_flow      \u2502              1 \u2502 error  \u2502\n# \u2502 8f2a1b34-c9d7-4e \u2502 reasoning_flow      \u2502              1 \u2502 success\u2502\n# \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"cli/CLI_SCREENSHOT_GUIDE/#screenshot-42-search-by-model-name","title":"Screenshot 4.2: Search by Model Name","text":"<pre><code># Find traces using specific model\nprela search \"claude-sonnet-4\"\n\n# Expected output:\n# Found 12 traces matching 'claude-sonnet-4'\n#\n# [Search results table showing all Claude Sonnet traces]\n</code></pre>"},{"location":"cli/CLI_SCREENSHOT_GUIDE/#screenshot-43-search-for-errors","title":"Screenshot 4.3: Search for Errors","text":"<pre><code># Quick error search\nprela search \"error\"\n\n# Expected output:\n# Found 5 traces matching 'error'\n#\n# [Search results table with error traces highlighted]\n</code></pre>"},{"location":"cli/CLI_SCREENSHOT_GUIDE/#5-convenience-shortcuts","title":"5. Convenience Shortcuts","text":""},{"location":"cli/CLI_SCREENSHOT_GUIDE/#screenshot-51-most-recent-trace","title":"Screenshot 5.1: Most Recent Trace","text":"<pre><code># One command to see latest execution\nprela last --compact\n\n# Expected output:\n# Showing most recent trace (aa538cdd-d1d5-4b...)\n#\n# reasoning_flow (agent) success 10.99s\n# \u251c\u2500\u2500 step_1_analyze (custom) success 3.97s\n# \u2502   \u2514\u2500\u2500 anthropic.messages.create (llm) success 3.97s\n# \u251c\u2500\u2500 step_2_solve (custom) success 2.35s\n# \u2502   \u2514\u2500\u2500 anthropic.messages.create (llm) success 2.35s\n# \u2514\u2500\u2500 step_3_verify (custom) success 4.67s\n#     \u2514\u2500\u2500 anthropic.messages.create (llm) success 4.67s\n#\n# \ud83d\udca1 Tip: Run without --compact to see full span details\n</code></pre>"},{"location":"cli/CLI_SCREENSHOT_GUIDE/#screenshot-52-failed-traces","title":"Screenshot 5.2: Failed Traces","text":"<pre><code># Instant error filtering\nprela errors\n\n# Expected output:\n#                             Failed Traces (3 errors)\n# \u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n# \u2503 Trace ID       \u2503 Root Span           \u2503 Duration \u2503 Spans \u2503 Time       \u2503\n# \u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n# \u2502 56cee896-936f  \u2502 reasoning_flow      \u2502    8.12s \u2502     4 \u2502 2026-01-30 \u2502\n# \u2502 615cecb0-2fe4  \u2502 rapid_requests      \u2502    2.15s \u2502     3 \u2502 2026-01-30 \u2502\n# \u2502 ae9ced4a-1c2f  \u2502 eval.case.test      \u2502    0.45s \u2502     2 \u2502 2026-01-30 \u2502\n# \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n#\n# \ud83d\udca1 Tip: Use 'prela show &lt;trace-id&gt;' to inspect a specific error\n</code></pre>"},{"location":"cli/CLI_SCREENSHOT_GUIDE/#screenshot-53-real-time-monitoring","title":"Screenshot 5.3: Real-Time Monitoring","text":"<pre><code># Follow new traces as they arrive\nprela tail --compact --interval 1\n\n# Expected output:\n# Following traces in ./traces (polling every 1s)\n# Press Ctrl+C to stop\n#\n# [New traces appear automatically as they're created]\n#\n# 2026-01-30 03:25:15  reasoning_flow (10.99s) \u2713\n# 2026-01-30 03:25:28  rapid_requests (9.03s) \u2713\n# 2026-01-30 03:25:42  eval.case.test (0.33s) \u2717\n</code></pre>"},{"location":"cli/CLI_SCREENSHOT_GUIDE/#6-interactive-features","title":"6. Interactive Features","text":""},{"location":"cli/CLI_SCREENSHOT_GUIDE/#screenshot-61-interactive-list-selection","title":"Screenshot 6.1: Interactive List Selection","text":"<pre><code># Numbered trace selection\nprela list --interactive\n\n# Expected output:\n#                    Recent Traces (5 of 17) - Select by number\n# \u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n# \u2503    # \u2503 Trace ID       \u2503 Root Span  \u2503 Duration \u2503 Status  \u2503 Spans \u2503 Time       \u2503\n# \u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n# \u2502    1 \u2502 aa538cdd-d1d5  \u2502 reasoning\u2026 \u2502   10.99s \u2502 success \u2502     7 \u2502 2026-01-30 \u2502\n# \u2502    2 \u2502 427ef3a7-58a3  \u2502 rapid_req\u2026 \u2502    9.03s \u2502 success \u2502     6 \u2502 2026-01-30 \u2502\n# \u2502    3 \u2502 56cee896-936f  \u2502 reasoning\u2026 \u2502    8.12s \u2502  error  \u2502     4 \u2502 2026-01-30 \u2502\n# \u2502    4 \u2502 615cecb0-2fe4  \u2502 rapid_req\u2026 \u2502    2.15s \u2502  error  \u2502     3 \u2502 2026-01-30 \u2502\n# \u2502    5 \u2502 ae9ced4a-1c2f  \u2502 eval.case  \u2502    0.33s \u2502  error  \u2502     2 \u2502 2026-01-30 \u2502\n# \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n#\n# Select trace (1-5), or 'q' to quit [q]: _\n</code></pre>"},{"location":"cli/CLI_SCREENSHOT_GUIDE/#screenshot-62-interactive-tui-full-screen","title":"Screenshot 6.2: Interactive TUI (Full Screen)","text":"<pre><code># Launch full-screen interactive explorer\nprela explore\n\n# Expected: Full-screen TUI with:\n# - Header: \"Prela Trace Explorer | k/j: Navigate | Enter: Select | Esc: Back | q: Quit\"\n# - Main area: DataTable with trace list\n# - Footer: Keyboard shortcuts help\n#\n# [Note: This requires an actual terminal screenshot of the Textual TUI]\n</code></pre>"},{"location":"cli/CLI_SCREENSHOT_GUIDE/#7-replay-engine","title":"7. Replay Engine","text":""},{"location":"cli/CLI_SCREENSHOT_GUIDE/#screenshot-71-basic-replay","title":"Screenshot 7.1: Basic Replay","text":"<pre><code># Replay exact execution\nprela replay test_traces/replay_test.jsonl\n\n# Expected output:\n# Loading trace from test_traces/replay_test.jsonl...\n# \u2713 Loaded trace f6358584-48eb-40c8-8c5e-49010950c9f2 with 1 spans\n#\n# Executing replay...\n# \u2713 Exact replay completed\n#\n# Replay Results:\n#   Trace ID: f6358584-48eb-40c8-8c5e-49010950c9f2\n#   Total Spans: 1\n#   Duration: 1957.3ms\n#   Tokens: 34\n#   Cost: $0.0002\n#   Success: \u2713\n#\n# Final Output:\n#   2 + 2 equals 4.\n</code></pre>"},{"location":"cli/CLI_SCREENSHOT_GUIDE/#screenshot-72-replay-with-model-override","title":"Screenshot 7.2: Replay with Model Override","text":"<pre><code># Change model and compare results\nprela replay test_traces/replay_test.jsonl --model claude-opus-4 --compare\n\n# Expected output:\n# Loading trace from test_traces/replay_test.jsonl...\n# \u2713 Loaded trace f6358584-48eb-40c8-8c5e-49010950c9f2 with 1 spans\n#\n# Executing replay...\n# \u2713 Modified replay completed (1 spans modified)\n#\n# Comparing with original execution...\n# sentence-transformers not available. Using fallback similarity metrics.\n#\n# Replay Comparison Summary\n# ==================================================\n# Total Spans: 1\n# Identical: 0 (0.0%)\n# Changed: 1 (100.0%)\n#\n# Cost: $0.0002 \u2192 $0.0008 (+$0.0006)\n# Tokens: 34 \u2192 42 (+8)\n#\n# Key Differences:\n#   \u2022 anthropic.messages.create (model)\n#   \u2022 anthropic.messages.create (output)\n#\n# Output Comparison:\n#   Original: \"2 + 2 equals 4.\"\n#   Modified: \"The sum of 2 and 2 is 4.\"\n#   Similarity: 0.82 (semantic match)\n</code></pre>"},{"location":"cli/CLI_SCREENSHOT_GUIDE/#screenshot-73-streaming-replay","title":"Screenshot 7.3: Streaming Replay","text":"<pre><code># Watch replay output in real-time\nprela replay test_traces/replay_test.jsonl --stream\n\n# Expected output:\n# Loading trace from test_traces/replay_test.jsonl...\n# \u2713 Loaded trace f6358584-48eb-40c8-8c5e-49010950c9f2 with 1 spans\n#\n# Executing replay...\n# Streaming enabled - showing real-time output:\n#\n# 2 + 2 equals 4.\n#\n# \u2713 Exact replay completed\n</code></pre>"},{"location":"cli/CLI_SCREENSHOT_GUIDE/#8-evaluation-framework","title":"8. Evaluation Framework","text":""},{"location":"cli/CLI_SCREENSHOT_GUIDE/#screenshot-81-run-eval-suite-python","title":"Screenshot 8.1: Run Eval Suite (Python)","text":"<pre><code># Show evaluation results\ncd test_scenarios\npython 06_evaluation.py\n\n# Expected output:\n# ============================================================\n# TEST SCENARIO 6: Evaluation Framework\n# ============================================================\n#\n# \u2713 Prela initialized with file exporter\n# \u2713 Traces will be saved to: ./test_traces\n#\n# \u2192 Defining test cases...\n#   \u2713 Created 3 test cases\n#   \u2713 Created eval suite: Math Agent Tests\n#\n# \u2192 Running evaluation...\n#\n# ============================================================\n# Evaluation Suite: Math Agent Tests\n# Started: 2026-01-30T03:41:54\n# Completed: 2026-01-30T03:41:54\n# Duration: 0.33s\n#\n# Total Cases: 3\n# Passed: 2 (66.7%)\n# Failed: 1\n# ============================================================\n#\n# Detailed Results:\n#   \u2713 Simple Addition (152.3ms)\n#   \u2713 Multiplication (189.7ms)\n#   \u2717 Explain Reasoning (98.1ms)\n#     \u2717 Semantic similarity below threshold (0.65 &lt; 0.70)\n</code></pre>"},{"location":"cli/CLI_SCREENSHOT_GUIDE/#9-workflow-comparisons-for-marketing","title":"9. Workflow Comparisons (For Marketing)","text":""},{"location":"cli/CLI_SCREENSHOT_GUIDE/#screenshot-91-before-prela-4-steps","title":"Screenshot 9.1: Before Prela (4 Steps)","text":"<pre><code># Traditional workflow without convenience commands\nprela list\n# [scan timestamps manually...]\n# [copy trace ID...]\nprela show aa538cdd-d1d5-4b1a-a5c6-2895041bd236\n\n# Caption: \"Old way: 4 steps, manual scanning, copy/paste\"\n</code></pre>"},{"location":"cli/CLI_SCREENSHOT_GUIDE/#screenshot-92-after-prela-1-step","title":"Screenshot 9.2: After Prela (1 Step)","text":"<pre><code># Modern workflow with convenience shortcuts\nprela last\n\n# Caption: \"New way: 1 command, instant results\"\n</code></pre>"},{"location":"cli/CLI_SCREENSHOT_GUIDE/#screenshot-93-error-debugging-before","title":"Screenshot 9.3: Error Debugging - Before","text":"<pre><code># Traditional error finding\nprela list\n# [scan visually for red 'error' status...]\n# [copy each error trace ID...]\nprela show 56cee896-936f-4612-8a42-3f1e2d5c7b89\nprela show 615cecb0-2fe4-4e91-b2c3-8a7d9e1f4c2b\n# [repeat for each error...]\n\n# Caption: \"Finding errors: Manual scanning and multiple commands\"\n</code></pre>"},{"location":"cli/CLI_SCREENSHOT_GUIDE/#screenshot-94-error-debugging-after","title":"Screenshot 9.4: Error Debugging - After","text":"<pre><code># Modern error debugging\nprela errors\n\n# Caption: \"Finding errors: One command, instant results\"\n</code></pre>"},{"location":"cli/CLI_SCREENSHOT_GUIDE/#10-integration-examples-for-docs","title":"10. Integration Examples (For Docs)","text":""},{"location":"cli/CLI_SCREENSHOT_GUIDE/#screenshot-101-simple-python-integration","title":"Screenshot 10.1: Simple Python Integration","text":"<pre><code># show_simple_integration.py\nimport prela\nfrom anthropic import Anthropic\n\n# One line to enable tracing\nprela.init(service_name=\"my-app\", exporter=\"file\")\n\n# All Claude calls automatically traced!\nclient = Anthropic()\nresponse = client.messages.create(\n    model=\"claude-sonnet-4-20250514\",\n    max_tokens=1024,\n    messages=[{\"role\": \"user\", \"content\": \"What is 2+2?\"}]\n)\n\nprint(f\"\u2713 Response: {response.content[0].text}\")\nprint(\"\u2713 Trace captured automatically!\")\nprint(\"\u2713 Run 'prela list' to view\")\n</code></pre>"},{"location":"cli/CLI_SCREENSHOT_GUIDE/#screenshot-102-multi-step-agent-integration","title":"Screenshot 10.2: Multi-Step Agent Integration","text":"<pre><code># show_multistep_integration.py\nimport prela\nfrom anthropic import Anthropic\n\ntracer = prela.init(service_name=\"reasoning-agent\", exporter=\"file\")\nclient = Anthropic()\n\n# Create hierarchical trace structure\nwith tracer.span(\"reasoning_flow\", span_type=prela.SpanType.AGENT):\n\n    with tracer.span(\"analyze\", span_type=prela.SpanType.CUSTOM):\n        response1 = client.messages.create(...)  # Automatically traced\n\n    with tracer.span(\"solve\", span_type=prela.SpanType.CUSTOM):\n        response2 = client.messages.create(...)  # Automatically traced\n\n    with tracer.span(\"verify\", span_type=prela.SpanType.CUSTOM):\n        response3 = client.messages.create(...)  # Automatically traced\n\nprint(\"\u2713 Multi-step trace captured with hierarchy!\")\nprint(\"\u2713 Run 'prela show &lt;trace-id&gt;' to see the tree\")\n</code></pre>"},{"location":"cli/CLI_SCREENSHOT_GUIDE/#screenshot-capture-instructions","title":"\ud83d\udccb Screenshot Capture Instructions","text":""},{"location":"cli/CLI_SCREENSHOT_GUIDE/#recommended-order","title":"Recommended Order:","text":"<ol> <li>Getting Started (Screenshots 1.1-1.3) - Show installation and help</li> <li>Basic Usage (Screenshots 2.1-2.2) - Empty state \u2192 populated list</li> <li>Core Features (Screenshots 3.1-5.3) - Show, search, shortcuts</li> <li>Interactive (Screenshots 6.1-6.2) - Interactive selection and TUI</li> <li>Advanced (Screenshots 7.1-8.1) - Replay and evaluation</li> <li>Comparisons (Screenshots 9.1-9.4) - Before/after workflows</li> </ol>"},{"location":"cli/CLI_SCREENSHOT_GUIDE/#terminal-setup","title":"Terminal Setup:","text":"<pre><code># Clean, professional screenshots\nexport PS1=\"$ \"  # Simple prompt\nclear            # Clear screen before each screenshot\n\n# Optimal window size\n# Width: 100 columns (fits on most screens)\n# Height: 30 rows (shows full tables)\n</code></pre>"},{"location":"cli/CLI_SCREENSHOT_GUIDE/#pro-tips","title":"Pro Tips:","text":"<ol> <li>Add delays for streaming: Use <code>sleep 1</code> to show incremental output</li> <li>Highlight key outputs: Circle or annotate important parts in post</li> <li>Show full commands: Include the <code>prela</code> command in each screenshot</li> <li>Consistent styling: Use same terminal theme throughout</li> <li>Add captions: Brief text below each screenshot explaining what it shows</li> </ol>"},{"location":"cli/CLI_SCREENSHOT_GUIDE/#marketing-captions","title":"\ud83c\udfa8 Marketing Captions","text":""},{"location":"cli/CLI_SCREENSHOT_GUIDE/#for-hero-section","title":"For Hero Section:","text":"<pre><code>\"See inside your AI agents\"\n\u2192 Screenshot: prela show --compact with clean tree structure\n\n\"Debug errors in seconds, not hours\"\n\u2192 Screenshot: prela errors with 3 error traces highlighted\n\n\"One line of code. Complete observability.\"\n\u2192 Screenshot: Python code showing prela.init() + automatic tracing\n</code></pre>"},{"location":"cli/CLI_SCREENSHOT_GUIDE/#for-features-section","title":"For Features Section:","text":"<pre><code>\"Beautiful CLI, not just functional\"\n\u2192 Screenshot: prela list with formatted table\n\n\"Interactive exploration when you need it\"\n\u2192 Screenshot: prela explore TUI full screen\n\n\"Replay any execution with different models\"\n\u2192 Screenshot: prela replay --model --compare showing side-by-side\n</code></pre>"},{"location":"cli/CLI_SCREENSHOT_GUIDE/#for-docs-faq","title":"For Docs FAQ:","text":"<pre><code>Q: \"How do I find my most recent trace?\"\nA: Run `prela last`\n\u2192 Screenshot: prela last --compact output\n\nQ: \"How do I debug failed executions?\"\nA: Run `prela errors`\n\u2192 Screenshot: prela errors table with helpful tip\n\nQ: \"Can I monitor traces in real-time?\"\nA: Run `prela tail`\n\u2192 Screenshot: prela tail --compact showing live updates\n</code></pre>"},{"location":"cli/CLI_SCREENSHOT_GUIDE/#ready-for-screenshots","title":"\u2705 Ready for Screenshots!","text":"<p>This guide provides clear, reproducible examples for all major CLI features. Each command shows expected output formatted exactly as it appears in the terminal.</p> <p>Next steps: 1. Set up clean terminal environment 2. Run test scenarios to populate traces 3. Capture screenshots following the recommended order 4. Add captions and annotations for marketing materials 5. Use in docs, README, website, and Product Hunt launch</p> <p>Estimated time: 30-45 minutes for all screenshots</p>"},{"location":"cli/commands/","title":"CLI Commands","text":"<p>Prela provides a command-line interface for managing traces and running evaluations.</p>"},{"location":"cli/commands/#installation","title":"Installation","text":"<p>The CLI is included with Prela:</p> <pre><code>pip install prela\nprela --help\n</code></pre>"},{"location":"cli/commands/#global-options","title":"Global Options","text":"<pre><code>prela --version          # Show version\nprela --help             # Show help\nprela COMMAND --help     # Command-specific help\n</code></pre>"},{"location":"cli/commands/#commands","title":"Commands","text":""},{"location":"cli/commands/#init","title":"init","text":"<p>Initialize Prela in a new project.</p> <pre><code>prela init [--service-name NAME] [--exporter TYPE]\n</code></pre> <p>Options: - <code>--service-name</code>: Service name (default: directory name) - <code>--exporter</code>: Exporter type (console, file)</p> <p>Example: <pre><code>prela init --service-name my-agent --exporter file\n</code></pre></p>"},{"location":"cli/commands/#list","title":"list","text":"<p>List available traces.</p> <pre><code>prela list [--directory DIR] [--service NAME] [--date DATE] [--limit N]\n</code></pre> <p>Options: - <code>--directory</code>: Traces directory (default: ./traces) - <code>--service</code>: Filter by service name - <code>--date</code>: Filter by date (YYYY-MM-DD) - <code>--limit</code>: Max results (default: 20)</p> <p>Example: <pre><code>prela list --service my-agent --date 2025-01-26\n</code></pre></p>"},{"location":"cli/commands/#show","title":"show","text":"<p>Display a specific trace.</p> <pre><code>prela show TRACE_ID [--directory DIR] [--format FORMAT]\n</code></pre> <p>Options: - <code>--directory</code>: Traces directory - <code>--format</code>: Output format (json, tree, default: tree)</p> <p>Example: <pre><code>prela show 550e8400-e29b-41d4-a716-446655440000\nprela show 550e8400-e29b-41d4-a716-446655440000 --format json\n</code></pre></p>"},{"location":"cli/commands/#search","title":"search","text":"<p>Search traces by attributes.</p> <pre><code>prela search [--directory DIR] [--service NAME] [--status STATUS] [--type TYPE] [--date DATE] [--limit N]\n</code></pre> <p>Options: - <code>--directory</code>: Traces directory - <code>--service</code>: Service name - <code>--status</code>: Span status (success, error, pending) - <code>--type</code>: Span type (agent, llm, tool, etc.) - <code>--date</code>: Date (YYYY-MM-DD) - <code>--limit</code>: Max results (default: 20)</p> <p>Example: <pre><code>prela search --service my-agent --status error --type llm\n</code></pre></p>"},{"location":"cli/commands/#eval-run","title":"eval run","text":"<p>Run evaluation suite.</p> <pre><code>prela eval run SUITE_FILE --agent AGENT_FILE [OPTIONS]\n</code></pre> <p>Options: - <code>--agent</code>: Path to agent module - <code>--format</code>: Output format (console, json, junit, default: console) - <code>--output</code>: Output file path - <code>--parallel</code>: Enable parallel execution - <code>--workers N</code>: Number of parallel workers (default: 4) - <code>--trace</code>: Enable tracing - <code>--exporter</code>: Trace exporter (console, file)</p> <p>Example: <pre><code>prela eval run tests.yaml --agent agent.py\nprela eval run tests.yaml --agent agent.py --parallel --workers 8\nprela eval run tests.yaml --agent agent.py --format junit --output results.xml\n</code></pre></p>"},{"location":"cli/commands/#export","title":"export","text":"<p>Export traces to different formats.</p> <pre><code>prela export [--directory DIR] [--output FILE] [--format FORMAT] [--service NAME] [--date DATE]\n</code></pre> <p>Options: - <code>--directory</code>: Traces directory - <code>--output</code>: Output file - <code>--format</code>: Format (json, jsonl, csv) - <code>--service</code>: Filter by service - <code>--date</code>: Filter by date</p> <p>Example: <pre><code>prela export --service my-agent --date 2025-01-26 --output export.json\n</code></pre></p>"},{"location":"cli/commands/#replay","title":"replay","text":"<p>Replay captured traces with modifications for testing and debugging.</p> <pre><code>prela replay TRACE_FILE [OPTIONS]\n</code></pre> <p>Options: - <code>--model</code>: Override model (e.g., gpt-4o, claude-sonnet-4) - <code>--temperature</code>: Override temperature - <code>--max-tokens</code>: Override max tokens - <code>--compare</code>: Compare with original execution - <code>--stream</code>: Enable streaming output - <code>--enable-tools</code>: Enable tool re-execution - <code>--enable-retrieval</code>: Enable retrieval re-execution - <code>--format</code>: Output format (json, tree, default: tree)</p> <p>Examples:</p> <p>Basic replay with different model: <pre><code>prela replay trace.json --model gpt-4o\n</code></pre></p> <p>Replay with comparison: <pre><code>prela replay trace.json --model claude-sonnet-4 --compare\n</code></pre></p> <p>Replay with streaming: <pre><code>prela replay trace.json --model gpt-4o --stream\n</code></pre></p> <p>Replay with tool execution: <pre><code>prela replay trace.json --enable-tools --compare\n</code></pre></p> <p>See also: - Replay Concepts - Replay Examples</p>"},{"location":"cli/commands/#configuration-file","title":"Configuration File","text":"<p>Create <code>.prela.yaml</code> for defaults:</p> <pre><code># .prela.yaml\nservice_name: my-agent\ntrace_directory: ./traces\nexporter: file\nsample_rate: 1.0\n</code></pre> <p>CLI options override config file.</p>"},{"location":"cli/commands/#next-steps","title":"Next Steps","text":"<ul> <li>See Configuration</li> <li>Learn about Exporters</li> </ul>"},{"location":"cli/configuration/","title":"CLI Configuration","text":"<p>Configure the Prela CLI using configuration files and environment variables.</p>"},{"location":"cli/configuration/#configuration-file","title":"Configuration File","text":"<p>Create <code>.prela.yaml</code> in your project root:</p> <pre><code># .prela.yaml\nservice_name: my-agent\ntrace_directory: ./traces\nexporter: file\nsample_rate: 1.0\nauto_instrument: true\ndebug: false\n\n# File exporter options\nfile_exporter:\n  max_file_size_mb: 100\n  rotate: true\n\n# Console exporter options\nconsole_exporter:\n  verbosity: normal\n  color: true\n  show_timestamps: true\n</code></pre>"},{"location":"cli/configuration/#environment-variables","title":"Environment Variables","text":"<p>Override config with environment variables:</p> <pre><code>export PRELA_SERVICE_NAME=my-agent\nexport PRELA_TRACE_DIR=./traces\nexport PRELA_EXPORTER=file\nexport PRELA_SAMPLE_RATE=1.0\nexport PRELA_AUTO_INSTRUMENT=true\nexport PRELA_DEBUG=false\n</code></pre>"},{"location":"cli/configuration/#precedence","title":"Precedence","text":"<ol> <li>CLI options (highest priority)</li> <li>Environment variables</li> <li>Configuration file</li> <li>Defaults (lowest priority)</li> </ol>"},{"location":"cli/configuration/#example-configurations","title":"Example Configurations","text":""},{"location":"cli/configuration/#development","title":"Development","text":"<pre><code># .prela.dev.yaml\nservice_name: dev-agent\nexporter: console\nsample_rate: 1.0\nconsole_exporter:\n  verbosity: verbose\n  color: true\ndebug: true\n</code></pre>"},{"location":"cli/configuration/#production","title":"Production","text":"<pre><code># .prela.prod.yaml\nservice_name: prod-agent\ntrace_directory: /var/log/traces\nexporter: file\nsample_rate: 0.1\nfile_exporter:\n  max_file_size_mb: 500\n  rotate: true\nauto_instrument: true\ndebug: false\n</code></pre>"},{"location":"cli/configuration/#testing","title":"Testing","text":"<pre><code># .prela.test.yaml\nservice_name: test-agent\nexporter: console\nsample_rate: 1.0\nconsole_exporter:\n  verbosity: minimal\n</code></pre>"},{"location":"cli/configuration/#loading-custom-config","title":"Loading Custom Config","text":"<pre><code># Use specific config file\nPRELA_CONFIG=.prela.prod.yaml prela list\n</code></pre>"},{"location":"cli/configuration/#next-steps","title":"Next Steps","text":"<ul> <li>See Commands</li> <li>Learn about Exporters</li> </ul>"},{"location":"concepts/context/","title":"Context Propagation","text":"<p>Context propagation is how Prela tracks the active span and trace across different parts of your application, including async code and thread pools.</p>"},{"location":"concepts/context/#overview","title":"Overview","text":"<p>Prela uses Python's <code>contextvars</code> module for thread-safe and async-safe context storage. This enables:</p> <ul> <li>Automatic span nesting: Child spans automatically link to parent spans</li> <li>Thread safety: Each thread has its own context</li> <li>Async support: Context preserved across <code>await</code> boundaries</li> <li>Baggage propagation: Carry metadata through the trace</li> </ul> <pre><code>graph LR\n    A[Main Thread] --&gt; B[Context]\n    B --&gt; C[Span Stack]\n    B --&gt; D[Baggage]\n    A --&gt; E[Worker Thread 1]\n    A --&gt; F[Worker Thread 2]\n    E --&gt; G[Copied Context]\n    F --&gt; H[Copied Context]</code></pre>"},{"location":"concepts/context/#how-context-works","title":"How Context Works","text":""},{"location":"concepts/context/#automatic-context-management","title":"Automatic Context Management","text":"<p>When using the tracer's context manager, context is handled automatically:</p> <pre><code>from prela import get_tracer, SpanType\n\n# Context automatically created and managed\nwith get_tracer().span(\"parent_operation\", SpanType.AGENT) as parent:\n    # parent is now the active span\n\n    with get_tracer().span(\"child_operation\", SpanType.TOOL) as child:\n        # child automatically becomes a child of parent\n        # child.parent_span_id == parent.span_id\n        pass\n\n    # parent is active again after child ends\n</code></pre>"},{"location":"concepts/context/#context-stack","title":"Context Stack","text":"<p>Prela maintains a stack of active spans in the current context:</p> <pre><code>from prela.core.context import get_current_span\n\nwith get_tracer().span(\"level_1\") as span1:\n    print(get_current_span().name)  # \"level_1\"\n\n    with get_tracer().span(\"level_2\") as span2:\n        print(get_current_span().name)  # \"level_2\"\n\n    print(get_current_span().name)  # \"level_1\" again\n</code></pre>"},{"location":"concepts/context/#thread-safety","title":"Thread Safety","text":""},{"location":"concepts/context/#the-problem","title":"The Problem","text":"<p>Python's <code>contextvars</code> don't automatically propagate to thread pool workers:</p> <pre><code>from concurrent.futures import ThreadPoolExecutor\n\nwith get_tracer().span(\"parent\") as parent:\n    def worker():\n        # This runs in a different thread\n        # Context is LOST - get_current_span() returns None\n        current = get_current_span()\n        print(current)  # None\n\n    with ThreadPoolExecutor() as executor:\n        executor.submit(worker)  # Context not propagated!\n</code></pre>"},{"location":"concepts/context/#the-solution-copy_context_to_thread","title":"The Solution: copy_context_to_thread","text":"<p>Prela provides <code>copy_context_to_thread()</code> to propagate context to worker threads:</p> <pre><code>from prela.core.context import copy_context_to_thread\nfrom concurrent.futures import ThreadPoolExecutor\n\nwith get_tracer().span(\"parent\") as parent:\n    @copy_context_to_thread\n    def worker():\n        # Context is preserved!\n        current = get_current_span()\n        print(current.name)  # \"parent\"\n\n        # Can create child spans\n        with get_tracer().span(\"worker_task\") as child:\n            # child.parent_span_id == parent.span_id\n            pass\n\n    with ThreadPoolExecutor() as executor:\n        # Wrap BEFORE submitting to pool\n        executor.submit(worker)\n</code></pre>"},{"location":"concepts/context/#important-wrap-inside-context","title":"Important: Wrap Inside Context","text":"<p>The function must be wrapped inside the context you want to propagate:</p> <pre><code># \u2705 CORRECT\nwith get_tracer().span(\"parent\"):\n    wrapped_func = copy_context_to_thread(my_function)\n    executor.submit(wrapped_func)\n\n# \u274c WRONG - wraps too early\nwrapped_func = copy_context_to_thread(my_function)\nwith get_tracer().span(\"parent\"):\n    executor.submit(wrapped_func)  # No context to capture!\n</code></pre>"},{"location":"concepts/context/#thread-pool-example","title":"Thread Pool Example","text":"<pre><code>from concurrent.futures import ThreadPoolExecutor, as_completed\nfrom prela import get_tracer\nfrom prela.core.context import copy_context_to_thread\n\ndef process_item(item_id):\n    \"\"\"Process a single item (runs in worker thread).\"\"\"\n    with get_tracer().span(f\"process_{item_id}\") as span:\n        span.set_attribute(\"item_id\", item_id)\n        # Processing logic\n        return f\"Result for {item_id}\"\n\n# Create parent span\nwith get_tracer().span(\"batch_processing\") as parent:\n    parent.set_attribute(\"batch_size\", 10)\n\n    # Wrap function INSIDE the parent span context\n    wrapped_process = copy_context_to_thread(process_item)\n\n    # Submit to thread pool\n    with ThreadPoolExecutor(max_workers=4) as executor:\n        futures = [\n            executor.submit(wrapped_process, i)\n            for i in range(10)\n        ]\n\n        # Collect results\n        for future in as_completed(futures):\n            result = future.result()\n            print(result)\n\n# All child spans are properly linked to parent!\n</code></pre>"},{"location":"concepts/context/#async-support","title":"Async Support","text":"<p>Context automatically propagates across <code>async</code>/<code>await</code> boundaries:</p> <pre><code>import asyncio\nfrom prela import get_tracer, SpanType\n\nasync def fetch_data(url):\n    \"\"\"Async function with tracing.\"\"\"\n    with get_tracer().span(\"fetch\", SpanType.RETRIEVAL) as span:\n        span.set_attribute(\"url\", url)\n        await asyncio.sleep(0.1)  # Simulate async work\n        return f\"Data from {url}\"\n\nasync def main():\n    with get_tracer().span(\"async_main\", SpanType.AGENT) as parent:\n        # Context preserved across await\n        result1 = await fetch_data(\"https://api1.com\")\n        result2 = await fetch_data(\"https://api2.com\")\n\n        # Both fetch spans are children of async_main\n        return [result1, result2]\n\n# Run\nasyncio.run(main())\n</code></pre>"},{"location":"concepts/context/#concurrent-async-tasks","title":"Concurrent Async Tasks","text":"<pre><code>async def process_urls(urls):\n    with get_tracer().span(\"process_batch\") as parent:\n        # Create concurrent tasks\n        tasks = [fetch_data(url) for url in urls]\n\n        # Each task maintains its own span context\n        results = await asyncio.gather(*tasks)\n\n        return results\n\n# All spans properly linked\nasyncio.run(process_urls([\"url1\", \"url2\", \"url3\"]))\n</code></pre>"},{"location":"concepts/context/#manual-context-management","title":"Manual Context Management","text":"<p>For advanced use cases, you can manually manage context:</p>"},{"location":"concepts/context/#creating-a-new-trace-context","title":"Creating a New Trace Context","text":"<pre><code>from prela.core.context import new_trace_context\n\n# Create isolated trace context\nwith new_trace_context() as ctx:\n    # ctx is a fresh TraceContext\n    # No parent spans, clean slate\n\n    with get_tracer().span(\"root_operation\") as root:\n        # This is a root span (no parent)\n        assert root.parent_span_id is None\n</code></pre>"},{"location":"concepts/context/#getting-current-context","title":"Getting Current Context","text":"<pre><code>from prela.core.context import get_trace_context, get_current_span\n\n# Get the entire context\nctx = get_trace_context()\nprint(ctx.trace_id)\nprint(ctx.baggage)\n\n# Get just the current span\nspan = get_current_span()\nif span:\n    print(f\"Currently in span: {span.name}\")\nelse:\n    print(\"No active span\")\n</code></pre>"},{"location":"concepts/context/#accessing-trace-id","title":"Accessing Trace ID","text":"<pre><code>from prela.core.context import get_current_trace_id\n\nwith get_tracer().span(\"operation\") as span:\n    trace_id = get_current_trace_id()\n    print(f\"Trace ID: {trace_id}\")\n\n    # Use trace ID for correlation\n    logger.info(\"Processing request\", extra={\"trace_id\": trace_id})\n</code></pre>"},{"location":"concepts/context/#baggage","title":"Baggage","text":"<p>Baggage allows you to carry key-value data through the entire trace:</p> <pre><code>from prela.core.context import get_trace_context\n\nwith get_tracer().span(\"root\") as root:\n    # Set baggage at any point\n    ctx = get_trace_context()\n    ctx.baggage[\"user_id\"] = \"user123\"\n    ctx.baggage[\"request_id\"] = \"req456\"\n\n    # Baggage is available in child spans\n    with get_tracer().span(\"child\") as child:\n        ctx = get_trace_context()\n        user_id = ctx.baggage.get(\"user_id\")  # \"user123\"\n        print(f\"Processing for user: {user_id}\")\n</code></pre>"},{"location":"concepts/context/#baggage-in-thread-pools","title":"Baggage in Thread Pools","text":"<p>Baggage is automatically propagated when using <code>copy_context_to_thread</code>:</p> <pre><code>from prela.core.context import copy_context_to_thread, get_trace_context\n\nwith get_tracer().span(\"parent\"):\n    # Set baggage\n    ctx = get_trace_context()\n    ctx.baggage[\"session_id\"] = \"session789\"\n\n    @copy_context_to_thread\n    def worker():\n        # Baggage available in worker thread\n        ctx = get_trace_context()\n        session_id = ctx.baggage.get(\"session_id\")\n        print(f\"Session: {session_id}\")  # \"session789\"\n\n    executor.submit(worker)\n</code></pre>"},{"location":"concepts/context/#context-boundaries","title":"Context Boundaries","text":""},{"location":"concepts/context/#when-context-is-preserved","title":"When Context is Preserved","text":"<p>\u2705 Context automatically propagates in these scenarios:</p> <ul> <li>Same thread: Sequential function calls</li> <li>Async/await: Across coroutine boundaries</li> <li>With copy_context_to_thread: Thread pool workers</li> <li>Nested spans: Child spans within parent spans</li> </ul>"},{"location":"concepts/context/#when-context-is-lost","title":"When Context is Lost","text":"<p>\u274c Context is not preserved in these scenarios:</p> <ul> <li>New threads: Without <code>copy_context_to_thread</code></li> <li>Process pools: multiprocessing doesn't share context</li> <li>Message queues: Background job systems (Celery, RQ)</li> <li>Different services: Requires distributed tracing headers</li> </ul> <pre><code>import multiprocessing\n\nwith get_tracer().span(\"parent\"):\n    def process_worker():\n        # Context is LOST in process pools\n        # multiprocessing creates new Python interpreter\n        current = get_current_span()\n        print(current)  # None\n\n    with multiprocessing.Pool() as pool:\n        pool.apply(process_worker)  # No context propagation\n</code></pre>"},{"location":"concepts/context/#distributed-tracing","title":"Distributed Tracing","text":"<p>For distributed systems spanning multiple services, you need to propagate trace context via headers or metadata.</p>"},{"location":"concepts/context/#extracting-context-for-propagation","title":"Extracting Context for Propagation","text":"<pre><code>from prela.core.context import get_trace_context\n\nwith get_tracer().span(\"api_request\") as span:\n    ctx = get_trace_context()\n\n    # Extract IDs for propagation\n    headers = {\n        \"X-Trace-ID\": ctx.trace_id,\n        \"X-Span-ID\": span.span_id,\n    }\n\n    # Send to downstream service\n    response = requests.get(\"https://api.example.com\", headers=headers)\n</code></pre>"},{"location":"concepts/context/#injecting-context-from-headers","title":"Injecting Context from Headers","text":"<pre><code>from prela import Span, SpanType\nfrom prela.core.context import new_trace_context\n\ndef handle_request(request):\n    # Extract trace context from incoming headers\n    trace_id = request.headers.get(\"X-Trace-ID\")\n    parent_span_id = request.headers.get(\"X-Span-ID\")\n\n    # Create span that continues the distributed trace\n    with get_tracer().span(\"handle_request\", SpanType.AGENT) as span:\n        # Manually set trace context\n        span.trace_id = trace_id\n        span.parent_span_id = parent_span_id\n\n        # Process request\n        result = process(request)\n        return result\n</code></pre>"},{"location":"concepts/context/#best-practices","title":"Best Practices","text":""},{"location":"concepts/context/#1-always-use-context-managers","title":"1. Always Use Context Managers","text":"<pre><code># \u2705 GOOD: Automatic context cleanup\nwith get_tracer().span(\"operation\") as span:\n    do_work()\n\n# \u274c BAD: Manual management is error-prone\nspan = tracer.start_span(\"operation\")\ntry:\n    do_work()\nfinally:\n    span.end()  # Easy to forget or miss in error paths\n</code></pre>"},{"location":"concepts/context/#2-wrap-thread-pool-functions-inside-context","title":"2. Wrap Thread Pool Functions Inside Context","text":"<pre><code># \u2705 GOOD: Wrap after entering context\nwith get_tracer().span(\"parent\"):\n    wrapped = copy_context_to_thread(worker_func)\n    executor.submit(wrapped)\n\n# \u274c BAD: Wrap before entering context\nwrapped = copy_context_to_thread(worker_func)\nwith get_tracer().span(\"parent\"):\n    executor.submit(wrapped)  # No context to capture!\n</code></pre>"},{"location":"concepts/context/#3-use-baggage-for-cross-cutting-concerns","title":"3. Use Baggage for Cross-Cutting Concerns","text":"<pre><code># Store user/request IDs in baggage for all child spans\nwith get_tracer().span(\"handle_request\") as span:\n    ctx = get_trace_context()\n    ctx.baggage[\"user_id\"] = request.user_id\n    ctx.baggage[\"request_id\"] = request.id\n\n    # All child spans can access these values\n    process_request(request)\n</code></pre>"},{"location":"concepts/context/#4-check-for-active-span-before-using","title":"4. Check for Active Span Before Using","text":"<pre><code>from prela.core.context import get_current_span\n\n# Safe access to current span\ncurrent = get_current_span()\nif current:\n    current.set_attribute(\"key\", \"value\")\nelse:\n    # No active tracing - graceful degradation\n    pass\n</code></pre>"},{"location":"concepts/context/#5-use-new_trace_context-for-root-operations","title":"5. Use new_trace_context for Root Operations","text":"<pre><code># Create isolated traces for background jobs\nwith new_trace_context():\n    with get_tracer().span(\"background_job\") as span:\n        # This is a root span, not linked to any previous trace\n        process_job()\n</code></pre>"},{"location":"concepts/context/#troubleshooting","title":"Troubleshooting","text":""},{"location":"concepts/context/#problem-child-spans-not-linking-to-parent","title":"Problem: Child spans not linking to parent","text":"<p>Symptom: <code>child.parent_span_id</code> is <code>None</code> when it should have a parent.</p> <p>Solution: Ensure you're using the tracer's span context manager:</p> <pre><code># \u2705 CORRECT\nwith get_tracer().span(\"parent\"):\n    with get_tracer().span(\"child\"):\n        pass  # child.parent_span_id is set\n\n# \u274c WRONG\nparent = Span(name=\"parent\")\nwith get_tracer().span(\"child\"):\n    pass  # child.parent_span_id is None (parent not in context)\n</code></pre>"},{"location":"concepts/context/#problem-context-lost-in-thread-pool","title":"Problem: Context lost in thread pool","text":"<p>Symptom: <code>get_current_span()</code> returns <code>None</code> in worker threads.</p> <p>Solution: Use <code>copy_context_to_thread</code>:</p> <pre><code>from prela.core.context import copy_context_to_thread\n\nwith get_tracer().span(\"parent\"):\n    @copy_context_to_thread\n    def worker():\n        span = get_current_span()  # Now works!\n\n    executor.submit(worker)\n</code></pre>"},{"location":"concepts/context/#problem-baggage-not-propagating","title":"Problem: Baggage not propagating","text":"<p>Symptom: Baggage set in parent not available in child.</p> <p>Solution: Ensure baggage is set before child span creation:</p> <pre><code>with get_tracer().span(\"parent\"):\n    ctx = get_trace_context()\n    ctx.baggage[\"key\"] = \"value\"  # Set BEFORE child span\n\n    with get_tracer().span(\"child\"):\n        # Baggage is now available\n        value = get_trace_context().baggage.get(\"key\")\n</code></pre>"},{"location":"concepts/context/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Sampling to control trace volume</li> <li>Explore Exporters for sending traces to backends</li> <li>See Custom Spans for advanced patterns</li> </ul>"},{"location":"concepts/exporters/","title":"Exporters","text":"<p>Exporters control where and how traces are sent after collection. Prela provides built-in exporters for common use cases and a flexible base class for custom implementations.</p>"},{"location":"concepts/exporters/#overview","title":"Overview","text":"<p>Exporters are responsible for:</p> <ul> <li>Serialization: Converting spans to wire format</li> <li>Transmission: Sending traces to backend systems</li> <li>Retry Logic: Handling transient failures</li> <li>Batching: Grouping spans for efficiency</li> </ul> <pre><code>graph LR\n    A[Span Collection] --&gt; B{Sampled?}\n    B --&gt;|Yes| C[Exporter]\n    B --&gt;|No| D[Discard]\n    C --&gt; E[Console]\n    C --&gt; F[File]\n    C --&gt; G[HTTP]\n    C --&gt; H[Custom]</code></pre>"},{"location":"concepts/exporters/#built-in-exporters","title":"Built-in Exporters","text":"Exporter Use Case Output <code>ConsoleExporter</code> Development, debugging Terminal output <code>FileExporter</code> Production, analysis JSONL files Custom Integration Your backend"},{"location":"concepts/exporters/#consoleexporter","title":"ConsoleExporter","text":"<p>Pretty-prints traces to the console. Perfect for development and debugging.</p>"},{"location":"concepts/exporters/#basic-usage","title":"Basic Usage","text":"<pre><code>from prela import init\n\ntracer = init(\n    service_name=\"my-app\",\n    exporter=\"console\"  # String shortcut\n)\n</code></pre>"},{"location":"concepts/exporters/#configuration","title":"Configuration","text":"<pre><code>from prela.exporters import ConsoleExporter\n\nexporter = ConsoleExporter(\n    verbosity=\"normal\",      # \"minimal\", \"normal\", or \"verbose\"\n    color=True,              # Enable colored output (requires rich library)\n    show_timestamps=True,    # Show span start/end times\n    show_attributes=True,    # Show span attributes\n    show_events=True,        # Show span events\n    indent=2                 # JSON indentation for verbose mode\n)\n\ntracer = init(service_name=\"my-app\", exporter=exporter)\n</code></pre>"},{"location":"concepts/exporters/#verbosity-levels","title":"Verbosity Levels","text":""},{"location":"concepts/exporters/#minimal","title":"Minimal","text":"<p>Shows only span names and status:</p> <pre><code>\u2713 anthropic.messages.create (1.2s)\n\u2713 process_query (1.5s)\n</code></pre>"},{"location":"concepts/exporters/#normal-default","title":"Normal (Default)","text":"<p>Adds key attributes and timing:</p> <pre><code>\u2713 anthropic.messages.create (1.234s)\n  model: claude-sonnet-4-20250514\n  tokens: 150 \u2192 89\n  latency: 1234ms\n</code></pre>"},{"location":"concepts/exporters/#verbose","title":"Verbose","text":"<p>Full JSON output with all details:</p> <pre><code>{\n  \"span_id\": \"550e8400-e29b-41d4-a716-446655440000\",\n  \"trace_id\": \"123e4567-e89b-12d3-a456-426614174000\",\n  \"name\": \"anthropic.messages.create\",\n  \"span_type\": \"llm\",\n  \"status\": \"success\",\n  \"attributes\": {\n    \"llm.model\": \"claude-sonnet-4-20250514\",\n    \"llm.input_tokens\": 150,\n    \"llm.output_tokens\": 89\n  },\n  \"events\": [...],\n  \"duration_ms\": 1234.5\n}\n</code></pre>"},{"location":"concepts/exporters/#example","title":"Example","text":"<pre><code>from prela import init\n\n# Development: verbose output\ntracer = init(\n    service_name=\"dev-app\",\n    exporter=\"console\",\n    verbosity=\"verbose\",\n    color=True\n)\n\n# Demo: minimal output\ntracer = init(\n    service_name=\"demo-app\",\n    exporter=\"console\",\n    verbosity=\"minimal\"\n)\n</code></pre>"},{"location":"concepts/exporters/#fileexporter","title":"FileExporter","text":"<p>Writes traces to JSONL files on disk. Production-ready with rotation and organization.</p>"},{"location":"concepts/exporters/#basic-usage_1","title":"Basic Usage","text":"<pre><code>from prela import init\n\ntracer = init(\n    service_name=\"my-app\",\n    exporter=\"file\",\n    directory=\"./traces\"  # Output directory\n)\n</code></pre>"},{"location":"concepts/exporters/#configuration_1","title":"Configuration","text":"<pre><code>from prela.exporters import FileExporter\n\nexporter = FileExporter(\n    directory=\"./traces\",           # Base directory\n    format=\"jsonl\",                  # Output format (currently only jsonl)\n    max_file_size_mb=100,           # Max file size before rotation\n    rotate=True,                    # Enable file rotation\n    service_name=\"my-app\"           # Service name for organization\n)\n\ntracer = init(service_name=\"my-app\", exporter=exporter)\n</code></pre>"},{"location":"concepts/exporters/#file-organization","title":"File Organization","text":"<p>Traces are organized by service and date:</p> <pre><code>traces/\n\u251c\u2500\u2500 my-app/\n\u2502   \u251c\u2500\u2500 2025-01-26/\n\u2502   \u2502   \u251c\u2500\u2500 trace_550e8400.jsonl\n\u2502   \u2502   \u251c\u2500\u2500 trace_661f9511.jsonl\n\u2502   \u2502   \u2514\u2500\u2500 trace_772fa622.jsonl\n\u2502   \u2514\u2500\u2500 2025-01-27/\n\u2502       \u2514\u2500\u2500 trace_883gb733.jsonl\n\u2514\u2500\u2500 another-service/\n    \u2514\u2500\u2500 2025-01-26/\n        \u2514\u2500\u2500 trace_994hc844.jsonl\n</code></pre>"},{"location":"concepts/exporters/#file-format","title":"File Format","text":"<p>Each file contains one JSON object per line (JSONL):</p> <pre><code>{\"span_id\":\"550e8400...\",\"trace_id\":\"123e4567...\",\"name\":\"llm_call\",...}\n{\"span_id\":\"661f9511...\",\"trace_id\":\"123e4567...\",\"name\":\"tool_call\",...}\n{\"span_id\":\"772fa622...\",\"trace_id\":\"123e4567...\",\"name\":\"retrieval\",...}\n</code></pre>"},{"location":"concepts/exporters/#file-rotation","title":"File Rotation","text":"<p>When <code>rotate=True</code>, files automatically rotate at <code>max_file_size_mb</code>:</p> <pre><code>exporter = FileExporter(\n    directory=\"./traces\",\n    max_file_size_mb=50,  # Rotate at 50MB\n    rotate=True\n)\n\n# Creates numbered files:\n# trace_550e8400.jsonl       (original)\n# trace_550e8400.jsonl.1     (rotated)\n# trace_550e8400.jsonl.2     (rotated again)\n</code></pre>"},{"location":"concepts/exporters/#thread-safety","title":"Thread Safety","text":"<p><code>FileExporter</code> is thread-safe and can be used from multiple threads:</p> <pre><code>from concurrent.futures import ThreadPoolExecutor\nfrom prela import init, get_tracer\n\ntracer = init(service_name=\"app\", exporter=\"file\")\n\ndef worker(i):\n    with get_tracer().span(f\"task_{i}\") as span:\n        # Safe: multiple threads writing to same exporter\n        span.set_attribute(\"worker_id\", i)\n\nwith ThreadPoolExecutor(max_workers=10) as executor:\n    executor.map(worker, range(100))\n</code></pre>"},{"location":"concepts/exporters/#example_1","title":"Example","text":"<pre><code>from prela import init\n\n# Production configuration\ntracer = init(\n    service_name=\"production-api\",\n    exporter=\"file\",\n    directory=\"/var/log/traces\",\n    max_file_size_mb=100,\n    rotate=True\n)\n\n# Staging: smaller files\ntracer = init(\n    service_name=\"staging-api\",\n    exporter=\"file\",\n    directory=\"./traces\",\n    max_file_size_mb=10,\n    rotate=True\n)\n</code></pre>"},{"location":"concepts/exporters/#custom-exporters","title":"Custom Exporters","text":"<p>Create custom exporters for integration with your backend.</p>"},{"location":"concepts/exporters/#baseexporter-interface","title":"BaseExporter Interface","text":"<pre><code>from prela.exporters import BaseExporter, ExportResult\nfrom prela import Span\n\nclass MyExporter(BaseExporter):\n    \"\"\"Custom exporter for my backend.\"\"\"\n\n    def export(self, spans: list[Span]) -&gt; ExportResult:\n        \"\"\"Export spans to backend.\n\n        Args:\n            spans: List of spans to export\n\n        Returns:\n            ExportResult.SUCCESS: Export succeeded\n            ExportResult.FAILURE: Permanent failure, don't retry\n            ExportResult.RETRY: Transient failure, retry later\n        \"\"\"\n        try:\n            # Serialize spans\n            data = [span.to_dict() for span in spans]\n\n            # Send to backend\n            response = self.send_to_backend(data)\n\n            if response.status_code == 200:\n                return ExportResult.SUCCESS\n            elif response.status_code &gt;= 500:\n                # Transient error, retry\n                return ExportResult.RETRY\n            else:\n                # Permanent error, don't retry\n                return ExportResult.FAILURE\n\n        except Exception as e:\n            # Unexpected error, retry\n            print(f\"Export failed: {e}\")\n            return ExportResult.RETRY\n\n    def send_to_backend(self, data):\n        # Your backend integration here\n        pass\n</code></pre>"},{"location":"concepts/exporters/#using-custom-exporter","title":"Using Custom Exporter","text":"<pre><code>from prela import init\n\n# Initialize with custom exporter\ntracer = init(\n    service_name=\"my-app\",\n    exporter=MyExporter()\n)\n</code></pre>"},{"location":"concepts/exporters/#batchexporter","title":"BatchExporter","text":"<p>For high-volume scenarios, extend <code>BatchExporter</code> for automatic batching and retry logic:</p> <pre><code>from prela.exporters.base import BatchExporter, ExportResult\nfrom prela import Span\nimport requests\n\nclass HTTPExporter(BatchExporter):\n    \"\"\"Export traces via HTTP.\"\"\"\n\n    def __init__(self, endpoint: str, api_key: str, batch_size: int = 100):\n        super().__init__(\n            batch_size=batch_size,\n            timeout_ms=5000,           # 5 second timeout\n            max_retries=3,             # Retry up to 3 times\n            initial_backoff_ms=100,    # Start with 100ms backoff\n            max_backoff_ms=10000       # Cap at 10 seconds\n        )\n        self.endpoint = endpoint\n        self.api_key = api_key\n\n    def _do_export(self, spans: list[Span]) -&gt; ExportResult:\n        \"\"\"Implement actual export logic.\"\"\"\n        try:\n            response = requests.post(\n                self.endpoint,\n                json=[span.to_dict() for span in spans],\n                headers={\"Authorization\": f\"Bearer {self.api_key}\"},\n                timeout=self.timeout_ms / 1000\n            )\n\n            if response.status_code == 200:\n                return ExportResult.SUCCESS\n            elif response.status_code &gt;= 500:\n                return ExportResult.RETRY\n            else:\n                return ExportResult.FAILURE\n\n        except requests.Timeout:\n            return ExportResult.RETRY\n        except requests.ConnectionError:\n            return ExportResult.RETRY\n        except Exception:\n            return ExportResult.FAILURE\n\n\n# Use with batching\ntracer = init(\n    service_name=\"my-app\",\n    exporter=HTTPExporter(\n        endpoint=\"https://api.example.com/traces\",\n        api_key=\"your-api-key\",\n        batch_size=50  # Send in batches of 50\n    )\n)\n</code></pre>"},{"location":"concepts/exporters/#export-results","title":"Export Results","text":"<p>Exporters return one of three results:</p>"},{"location":"concepts/exporters/#success","title":"SUCCESS","text":"<p>Export completed successfully. Span data is safe.</p> <pre><code>def export(self, spans):\n    send_to_backend(spans)\n    return ExportResult.SUCCESS\n</code></pre>"},{"location":"concepts/exporters/#failure","title":"FAILURE","text":"<p>Permanent failure. Don't retry (e.g., invalid data, auth failure).</p> <pre><code>def export(self, spans):\n    response = send_to_backend(spans)\n    if response.status_code == 401:\n        # Auth failure, don't retry\n        return ExportResult.FAILURE\n    return ExportResult.SUCCESS\n</code></pre>"},{"location":"concepts/exporters/#retry","title":"RETRY","text":"<p>Transient failure. Will retry with exponential backoff.</p> <pre><code>def export(self, spans):\n    try:\n        send_to_backend(spans)\n        return ExportResult.SUCCESS\n    except TimeoutError:\n        # Network issue, retry\n        return ExportResult.RETRY\n</code></pre>"},{"location":"concepts/exporters/#retry-logic","title":"Retry Logic","text":"<p><code>BatchExporter</code> implements exponential backoff:</p> <pre><code># Retry schedule with default settings:\n# Attempt 1: immediate\n# Attempt 2: after 100ms\n# Attempt 3: after 200ms\n# Attempt 4: after 400ms\n# ...up to max_backoff_ms\n\nexporter = BatchExporter(\n    initial_backoff_ms=100,   # Start backoff\n    max_backoff_ms=10000,     # Cap backoff\n    max_retries=5             # Max attempts\n)\n</code></pre>"},{"location":"concepts/exporters/#multiple-exporters","title":"Multiple Exporters","text":"<p>Send traces to multiple destinations:</p> <pre><code>from prela.exporters import BaseExporter, ExportResult\n\nclass MultiExporter(BaseExporter):\n    \"\"\"Export to multiple backends.\"\"\"\n\n    def __init__(self, *exporters: BaseExporter):\n        self.exporters = exporters\n\n    def export(self, spans):\n        results = [exp.export(spans) for exp in self.exporters]\n\n        # Return RETRY if any exporter wants to retry\n        if ExportResult.RETRY in results:\n            return ExportResult.RETRY\n\n        # Return FAILURE if any exporter failed\n        if ExportResult.FAILURE in results:\n            return ExportResult.FAILURE\n\n        return ExportResult.SUCCESS\n\n\n# Use multiple exporters\nfrom prela.exporters import ConsoleExporter, FileExporter\n\ntracer = init(\n    service_name=\"my-app\",\n    exporter=MultiExporter(\n        ConsoleExporter(verbosity=\"minimal\"),\n        FileExporter(directory=\"./traces\")\n    )\n)\n</code></pre>"},{"location":"concepts/exporters/#best-practices","title":"Best Practices","text":""},{"location":"concepts/exporters/#1-use-consoleexporter-in-development","title":"1. Use ConsoleExporter in Development","text":"<pre><code># Development\ntracer = init(\n    service_name=\"dev-app\",\n    exporter=\"console\",\n    verbosity=\"verbose\"\n)\n</code></pre>"},{"location":"concepts/exporters/#2-use-fileexporter-in-production","title":"2. Use FileExporter in Production","text":"<pre><code># Production\ntracer = init(\n    service_name=\"prod-app\",\n    exporter=\"file\",\n    directory=\"/var/log/traces\",\n    max_file_size_mb=100,\n    rotate=True\n)\n</code></pre>"},{"location":"concepts/exporters/#3-implement-proper-error-handling","title":"3. Implement Proper Error Handling","text":"<pre><code>class RobustExporter(BaseExporter):\n    def export(self, spans):\n        try:\n            # Export logic\n            return ExportResult.SUCCESS\n        except ValueError:\n            # Bad data, don't retry\n            return ExportResult.FAILURE\n        except (TimeoutError, ConnectionError):\n            # Network issue, retry\n            return ExportResult.RETRY\n        except Exception as e:\n            # Unknown error, log and fail\n            logger.error(f\"Export failed: {e}\")\n            return ExportResult.FAILURE\n</code></pre>"},{"location":"concepts/exporters/#4-set-appropriate-timeouts","title":"4. Set Appropriate Timeouts","text":"<pre><code>class TimeoutAwareExporter(BatchExporter):\n    def __init__(self):\n        super().__init__(\n            timeout_ms=5000,  # 5 second total timeout\n            max_retries=3     # Retry up to 3 times\n        )\n\n    def _do_export(self, spans):\n        # Implementation with timeout enforcement\n        pass\n</code></pre>"},{"location":"concepts/exporters/#5-monitor-export-success","title":"5. Monitor Export Success","text":"<pre><code>class MonitoredExporter(BaseExporter):\n    def __init__(self, base_exporter):\n        self.base_exporter = base_exporter\n        self.success_count = 0\n        self.failure_count = 0\n\n    def export(self, spans):\n        result = self.base_exporter.export(spans)\n\n        if result == ExportResult.SUCCESS:\n            self.success_count += 1\n        else:\n            self.failure_count += 1\n\n        # Log metrics\n        logger.info(\n            f\"Export stats: {self.success_count} success, \"\n            f\"{self.failure_count} failures\"\n        )\n\n        return result\n</code></pre>"},{"location":"concepts/exporters/#troubleshooting","title":"Troubleshooting","text":""},{"location":"concepts/exporters/#problem-traces-not-appearing","title":"Problem: Traces not appearing","text":"<p>Check exporter configuration:</p> <pre><code>from prela import get_tracer\n\ntracer = get_tracer()\nprint(f\"Exporter: {tracer.exporter}\")\nprint(f\"Sampler: {tracer.sampler}\")\n</code></pre>"},{"location":"concepts/exporters/#problem-file-exporter-creating-too-many-files","title":"Problem: File exporter creating too many files","text":"<p>Increase file size limit:</p> <pre><code>exporter = FileExporter(\n    directory=\"./traces\",\n    max_file_size_mb=500,  # Larger files\n    rotate=True\n)\n</code></pre>"},{"location":"concepts/exporters/#problem-export-failures","title":"Problem: Export failures","text":"<p>Enable debug logging:</p> <pre><code>export PRELA_DEBUG=true\n</code></pre> <pre><code>import logging\nlogging.basicConfig(level=logging.DEBUG)\n\ntracer = init(service_name=\"app\", exporter=\"file\")\n# Logs will show export attempts and failures\n</code></pre>"},{"location":"concepts/exporters/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Sampling to control trace volume</li> <li>See Production Setup for deployment patterns</li> <li>Explore Custom Spans for advanced usage</li> </ul>"},{"location":"concepts/replay-advanced/","title":"Advanced Replay Features","text":"<p>Prela's replay engine includes advanced features for handling transient failures, computing similarity without heavy dependencies, and re-executing tools and retrievals for comprehensive testing.</p>"},{"location":"concepts/replay-advanced/#overview","title":"Overview","text":"<p>Advanced replay capabilities enable:</p> <ul> <li>API Retry Logic - Automatic recovery from transient API failures</li> <li>Semantic Similarity Fallback - Text comparison without 500MB dependencies</li> <li>Tool Re-execution - Execute tools during replay with safety controls</li> <li>Retrieval Re-execution - Query vector databases for fresh results</li> </ul> <p>These features make replay more robust, flexible, and production-ready.</p>"},{"location":"concepts/replay-advanced/#api-retry-logic","title":"API Retry Logic","text":""},{"location":"concepts/replay-advanced/#exponential-backoff","title":"Exponential Backoff","text":"<p>The replay engine automatically retries failed API calls using exponential backoff:</p> <pre><code>from prela.replay import ReplayEngine, TraceLoader\n\ntrace = TraceLoader.from_file(\"trace.json\")\nengine = ReplayEngine(\n    trace,\n    max_retries=3,              # Maximum retry attempts (default: 3)\n    retry_initial_delay=1.0,    # Initial delay in seconds (default: 1.0)\n    retry_max_delay=60.0,       # Maximum delay cap (default: 60.0)\n    retry_exponential_base=2.0, # Exponential base (default: 2.0)\n)\n\nresult = engine.replay_with_modifications(model=\"gpt-4o\")\n</code></pre> <p>Retry Pattern: - Attempt 0: No delay (initial request) - Attempt 1: 1.0s delay (2^0 \u00d7 1.0) - Attempt 2: 2.0s delay (2^1 \u00d7 1.0) - Attempt 3: 4.0s delay (2^2 \u00d7 1.0) - Capped at <code>retry_max_delay</code> (60s default)</p>"},{"location":"concepts/replay-advanced/#retryable-errors","title":"Retryable Errors","text":"<p>The engine automatically retries these error types:</p> <p>HTTP Status Codes: - <code>429</code> - Rate limit exceeded - <code>503</code> - Service temporarily unavailable - <code>502</code> - Bad gateway</p> <p>Exception Types: - Timeout errors (connection timeout, read timeout) - Connection errors (network issues) - API responses containing \"try again\" messages</p> <p>Non-Retryable Errors: - <code>401</code> - Authentication errors (fail immediately) - <code>403</code> - Permission errors (fail immediately) - <code>400</code> - Bad request errors (fail immediately)</p>"},{"location":"concepts/replay-advanced/#retry-count-tracking","title":"Retry Count Tracking","text":"<p>Each replayed span includes retry count information:</p> <pre><code>result = engine.replay_with_modifications(model=\"gpt-4o\")\n\nfor span in result.spans:\n    if span.retry_count &gt; 0:\n        print(f\"{span.name} required {span.retry_count} retries\")\n        # Example output:\n        # openai.chat.completions.create required 2 retries\n</code></pre> <p>Use Cases: - Monitor API reliability - Identify rate limit issues - Optimize retry configuration - Debug transient failures</p>"},{"location":"concepts/replay-advanced/#configuration-examples","title":"Configuration Examples","text":"<p>Aggressive Retries (Development): <pre><code>engine = ReplayEngine(\n    trace,\n    max_retries=5,           # More attempts\n    retry_initial_delay=0.5, # Faster retries\n    retry_max_delay=30.0,    # Lower cap\n)\n</code></pre></p> <p>Conservative Retries (Production): <pre><code>engine = ReplayEngine(\n    trace,\n    max_retries=2,           # Fewer attempts\n    retry_initial_delay=2.0, # Slower retries\n    retry_max_delay=120.0,   # Higher cap\n)\n</code></pre></p> <p>No Retries: <pre><code>engine = ReplayEngine(\n    trace,\n    max_retries=0,  # Disable retries\n)\n</code></pre></p>"},{"location":"concepts/replay-advanced/#semantic-similarity-fallback","title":"Semantic Similarity Fallback","text":""},{"location":"concepts/replay-advanced/#overview_1","title":"Overview","text":"<p>Replay comparison uses semantic similarity to compare original vs replayed outputs. By default, this requires <code>sentence-transformers</code> (~500MB). The fallback system enables comparison without this dependency.</p>"},{"location":"concepts/replay-advanced/#fallback-strategy","title":"Fallback Strategy","text":"<p>Three-tier fallback when <code>sentence-transformers</code> is unavailable:</p> <p>Tier 1: Exact Match (Fastest) <pre><code>if original_text == replayed_text:\n    return 1.0  # 100% similarity\n</code></pre></p> <p>Tier 2: difflib SequenceMatcher (Primary) <pre><code>import difflib\nratio = difflib.SequenceMatcher(None, original_text, replayed_text).ratio()\n# Returns 0.0-1.0 based on edit distance\n</code></pre></p> <p>Tier 3: Jaccard Word Similarity (Secondary) <pre><code>words1 = set(original_text.lower().split())\nwords2 = set(replayed_text.lower().split())\nintersection = len(words1 &amp; words2)\nunion = len(words1 | words2)\nreturn intersection / union if union &gt; 0 else 0.0\n</code></pre></p>"},{"location":"concepts/replay-advanced/#performance-comparison","title":"Performance Comparison","text":"Method Installation Size Speed Accuracy sentence-transformers ~500MB 10-50ms High (0.9+ for similar) difflib (fallback) 0MB (built-in) 1-5ms Medium (0.7+ for similar) Jaccard (fallback) 0MB (built-in) &lt;1ms Low (0.5+ for similar)"},{"location":"concepts/replay-advanced/#difflib-behavior","title":"difflib Behavior","text":"<p>Exact match: <pre><code>original = \"Hello world\"\nreplayed = \"Hello world\"\nsimilarity = 1.0  # 100%\n</code></pre></p> <p>Case change: <pre><code>original = \"Hello World\"\nreplayed = \"hello world\"\nsimilarity = 0.82  # 82%\n</code></pre></p> <p>Minor edit: <pre><code>original = \"The quick brown fox\"\nreplayed = \"The quick red fox\"\nsimilarity = 0.85  # 85%\n</code></pre></p> <p>Word reorder: <pre><code>original = \"cat dog bird\"\nreplayed = \"dog bird cat\"\nsimilarity = 0.67  # 67%\n</code></pre></p> <p>Completely different: <pre><code>original = \"Hello world\"\nreplayed = \"Goodbye universe\"\nsimilarity = 0.15  # 15%\n</code></pre></p>"},{"location":"concepts/replay-advanced/#availability-flags","title":"Availability Flags","text":"<p>Comparison results include flags indicating which method was used:</p> <pre><code>comparison = engine.compare_replay(original_result, replayed_result)\n\nprint(f\"Semantic similarity available: {comparison.semantic_similarity_available}\")\nprint(f\"Model used: {comparison.semantic_similarity_model}\")\n# Output (with sentence-transformers):\n# Semantic similarity available: True\n# Model used: all-MiniLM-L6-v2\n\n# Output (without sentence-transformers):\n# Semantic similarity available: False\n# Model used: None\n</code></pre>"},{"location":"concepts/replay-advanced/#installation-options","title":"Installation Options","text":"<p>Minimal (fallback only): <pre><code>pip install prela\n# Uses difflib + Jaccard, no heavy dependencies\n# Fast installation, 0 additional storage\n</code></pre></p> <p>Full (best accuracy): <pre><code>pip install prela[similarity]\n# Downloads sentence-transformers (~500MB first time)\n# Better accuracy for semantic comparison\n</code></pre></p>"},{"location":"concepts/replay-advanced/#when-to-use-each-method","title":"When to Use Each Method","text":"<p>Use Fallback (difflib) When: - Installation size matters (containers, edge devices) - Comparing structured output (JSON, code) - Fast installation required (CI/CD) - Exact or near-exact matches expected</p> <p>Use sentence-transformers When: - Comparing natural language text - Semantic meaning matters more than exact wording - High accuracy required - Storage/bandwidth not constrained</p>"},{"location":"concepts/replay-advanced/#tool-re-execution","title":"Tool Re-execution","text":""},{"location":"concepts/replay-advanced/#overview_2","title":"Overview","text":"<p>Instead of replaying cached tool outputs, you can re-execute tools during replay to test with fresh data.</p>"},{"location":"concepts/replay-advanced/#basic-usage","title":"Basic Usage","text":"<pre><code>from prela.replay import ReplayEngine, TraceLoader\n\n# Define tool functions\ndef calculator(expression: str) -&gt; str:\n    \"\"\"Safe calculator tool.\"\"\"\n    return str(eval(expression))\n\ndef search_api(query: str) -&gt; str:\n    \"\"\"Search API tool.\"\"\"\n    import requests\n    response = requests.get(f\"https://api.example.com/search?q={query}\")\n    return response.json()\n\n# Create tool registry\ntool_registry = {\n    \"calculator\": calculator,\n    \"search_api\": search_api,\n}\n\ntrace = TraceLoader.from_file(\"trace.json\")\nengine = ReplayEngine(trace, tool_registry=tool_registry, enable_tool_execution=True)\n\nresult = engine.replay_with_modifications(model=\"gpt-4o\")\n</code></pre>"},{"location":"concepts/replay-advanced/#safety-controls","title":"Safety Controls","text":"<p>Allowlist (Recommended): <pre><code>engine = ReplayEngine(\n    trace,\n    tool_registry=tool_registry,\n    enable_tool_execution={\n        \"allowlist\": [\"calculator\", \"search_api\"],  # Only these tools\n    }\n)\n</code></pre></p> <p>Blocklist: <pre><code>engine = ReplayEngine(\n    trace,\n    tool_registry=tool_registry,\n    enable_tool_execution={\n        \"blocklist\": [\"delete_file\", \"send_email\"],  # Block dangerous tools\n    }\n)\n</code></pre></p> <p>All Tools: <pre><code>engine = ReplayEngine(\n    trace,\n    tool_registry=tool_registry,\n    enable_tool_execution=True,  # Enable all tools in registry\n)\n</code></pre></p>"},{"location":"concepts/replay-advanced/#priority-system","title":"Priority System","text":"<p>When a tool call is encountered, the engine uses this priority:</p> <ol> <li>Mocks (highest priority) - If mock provided via <code>tool_mocks</code> parameter</li> <li>Execution (medium priority) - If enabled and tool in registry</li> <li>Cached (lowest priority) - Original output from trace</li> </ol> <pre><code>engine = ReplayEngine(\n    trace,\n    tool_registry={\"calculator\": calculator_fn},\n    enable_tool_execution=True,\n    tool_mocks={\"calculator\": \"42\"},  # Mock overrides execution\n)\n</code></pre>"},{"location":"concepts/replay-advanced/#error-handling","title":"Error Handling","text":"<p>Tool errors are captured safely:</p> <pre><code>def risky_tool(input: str) -&gt; str:\n    \"\"\"Tool that might fail.\"\"\"\n    if input == \"error\":\n        raise ValueError(\"Invalid input\")\n    return f\"Processed: {input}\"\n\ntool_registry = {\"risky_tool\": risky_tool}\n\nengine = ReplayEngine(\n    trace,\n    tool_registry=tool_registry,\n    enable_tool_execution=True,\n)\n\nresult = engine.replay_with_modifications(model=\"gpt-4o\")\n\n# Errors captured in span status, replay continues\nfor span in result.spans:\n    if span.span_type == \"tool\" and span.status == \"error\":\n        print(f\"Tool {span.name} failed: {span.attributes.get('error.message')}\")\n</code></pre>"},{"location":"concepts/replay-advanced/#use-cases","title":"Use Cases","text":"<p>Integration Testing: <pre><code># Test with real APIs\ntool_registry = {\n    \"github_api\": github.search_repositories,\n    \"slack_api\": slack.post_message,\n}\n\nengine = ReplayEngine(\n    trace,\n    tool_registry=tool_registry,\n    enable_tool_execution={\"allowlist\": [\"github_api\"]},  # Only test GitHub\n)\n</code></pre></p> <p>Regression Testing: <pre><code># Compare cached vs fresh results\nresult1 = engine.replay_with_modifications(enable_tool_execution=False)  # Use cached\nresult2 = engine.replay_with_modifications(enable_tool_execution=True)   # Re-execute\n\ncomparison = engine.compare_replay(result1, result2)\nprint(f\"Tool output consistency: {comparison.output_similarity}\")\n</code></pre></p> <p>Controlled Testing: <pre><code># Test subset of tools\nengine = ReplayEngine(\n    trace,\n    tool_registry=all_tools,\n    enable_tool_execution={\n        \"allowlist\": [\"safe_tool_1\", \"safe_tool_2\"],\n        \"blocklist\": [\"dangerous_tool\"],  # Extra safety\n    }\n)\n</code></pre></p>"},{"location":"concepts/replay-advanced/#retrieval-re-execution","title":"Retrieval Re-execution","text":""},{"location":"concepts/replay-advanced/#overview_3","title":"Overview","text":"<p>Re-execute vector database queries during replay to test with current data:</p> <pre><code>from prela.replay import ReplayEngine, TraceLoader\nimport chromadb\n\n# Initialize vector database client\nclient = chromadb.Client()\ncollection = client.get_or_create_collection(\"my_docs\")\n\ntrace = TraceLoader.from_file(\"trace.json\")\nengine = ReplayEngine(\n    trace,\n    retrieval_client=client,\n    enable_retrieval_execution=True,\n)\n\nresult = engine.replay_with_modifications(model=\"gpt-4o\")\n</code></pre>"},{"location":"concepts/replay-advanced/#supported-vector-databases","title":"Supported Vector Databases","text":"<p>ChromaDB (Fully Implemented): <pre><code>import chromadb\n\nclient = chromadb.Client()\ncollection = client.get_or_create_collection(\"docs\")\n\nengine = ReplayEngine(\n    trace,\n    retrieval_client=client,\n    enable_retrieval_execution=True,\n)\n</code></pre></p> <p>Pinecone (Placeholder): <pre><code>import pinecone\n\npinecone.init(api_key=\"...\")\nindex = pinecone.Index(\"my-index\")\n\nengine = ReplayEngine(\n    trace,\n    retrieval_client=index,\n    enable_retrieval_execution=True,\n)\n</code></pre></p> <p>Qdrant (Placeholder): <pre><code>from qdrant_client import QdrantClient\n\nclient = QdrantClient(url=\"http://localhost:6333\")\n\nengine = ReplayEngine(\n    trace,\n    retrieval_client=client,\n    enable_retrieval_execution=True,\n)\n</code></pre></p> <p>Weaviate (Placeholder): <pre><code>import weaviate\n\nclient = weaviate.Client(url=\"http://localhost:8080\")\n\nengine = ReplayEngine(\n    trace,\n    retrieval_client=client,\n    enable_retrieval_execution=True,\n)\n</code></pre></p>"},{"location":"concepts/replay-advanced/#query-override","title":"Query Override","text":"<p>Override the original query:</p> <pre><code>engine = ReplayEngine(\n    trace,\n    retrieval_client=client,\n    enable_retrieval_execution=True,\n    retrieval_query_override=\"Updated query text\",\n)\n\n# All retrieval spans will use the new query\nresult = engine.replay_with_modifications(model=\"gpt-4o\")\n</code></pre>"},{"location":"concepts/replay-advanced/#priority-system_1","title":"Priority System","text":"<p>When a retrieval operation is encountered:</p> <ol> <li>Execution (if enabled and client provided) - Query vector database</li> <li>Cached (fallback) - Original documents from trace</li> </ol> <pre><code># Test with fresh data\nresult1 = engine.replay_with_modifications(enable_retrieval_execution=True)\n\n# Test with cached data\nresult2 = engine.replay_with_modifications(enable_retrieval_execution=False)\n\n# Compare consistency\ncomparison = engine.compare_replay(result1, result2)\n</code></pre>"},{"location":"concepts/replay-advanced/#use-cases_1","title":"Use Cases","text":"<p>Data Freshness Testing: <pre><code># Verify agent works with current data\nengine = ReplayEngine(\n    trace,\n    retrieval_client=chroma_client,\n    enable_retrieval_execution=True,\n)\n\nresult = engine.replay_with_modifications(model=\"gpt-4o\")\nprint(f\"Documents retrieved: {len(result.spans[0].documents)}\")\n</code></pre></p> <p>RAG Pipeline Testing: <pre><code># Test retrieval \u2192 generation pipeline\nengine = ReplayEngine(\n    trace,\n    retrieval_client=chroma_client,\n    enable_retrieval_execution=True,\n    enable_tool_execution=True,  # Also re-execute tools\n)\n\nresult = engine.replay_with_modifications(\n    model=\"gpt-4o\",\n    temperature=0.0,  # Deterministic generation\n)\n</code></pre></p> <p>Query Sensitivity Testing: <pre><code># Test different query variations\nqueries = [\n    \"original query\",\n    \"rephrased query\",\n    \"shorter query\",\n]\n\nresults = []\nfor query in queries:\n    engine = ReplayEngine(\n        trace,\n        retrieval_client=client,\n        enable_retrieval_execution=True,\n        retrieval_query_override=query,\n    )\n    results.append(engine.replay_with_modifications(model=\"gpt-4o\"))\n\n# Compare outputs across query variations\n</code></pre></p>"},{"location":"concepts/replay-advanced/#combining-advanced-features","title":"Combining Advanced Features","text":""},{"location":"concepts/replay-advanced/#complete-example","title":"Complete Example","text":"<pre><code>from prela.replay import ReplayEngine, TraceLoader\nimport chromadb\n\n# Define tools\ndef calculator(expr: str) -&gt; str:\n    return str(eval(expr))\n\ndef search_api(query: str) -&gt; str:\n    import requests\n    return requests.get(f\"https://api.example.com/search?q={query}\").json()\n\n# Initialize vector database\nchroma_client = chromadb.Client()\ncollection = chroma_client.get_or_create_collection(\"docs\")\n\n# Load trace\ntrace = TraceLoader.from_file(\"trace.json\")\n\n# Create engine with all advanced features\nengine = ReplayEngine(\n    trace,\n    # API retry configuration\n    max_retries=3,\n    retry_initial_delay=1.0,\n    retry_max_delay=60.0,\n    # Tool re-execution\n    tool_registry={\"calculator\": calculator, \"search_api\": search_api},\n    enable_tool_execution={\"allowlist\": [\"calculator\"]},\n    # Retrieval re-execution\n    retrieval_client=chroma_client,\n    enable_retrieval_execution=True,\n)\n\n# Replay with modifications\nresult = engine.replay_with_modifications(\n    model=\"gpt-4o\",\n    temperature=0.7,\n)\n\n# Analyze results\nprint(f\"Replayed {len(result.spans)} spans\")\nprint(f\"Retries required: {sum(s.retry_count for s in result.spans)}\")\nprint(f\"Tools executed: {sum(1 for s in result.spans if s.span_type == 'tool')}\")\nprint(f\"Retrievals executed: {sum(1 for s in result.spans if s.span_type == 'retrieval')}\")\n\n# Compare with original (using fallback similarity)\ncomparison = engine.compare_replay(\n    original_result=trace,\n    replayed_result=result,\n)\n\nprint(f\"\\nSemantic similarity available: {comparison.semantic_similarity_available}\")\nprint(f\"Similarity model: {comparison.semantic_similarity_model or 'difflib (fallback)'}\")\nprint(f\"Output similarity: {comparison.output_similarity:.2%}\")\n</code></pre>"},{"location":"concepts/replay-advanced/#best-practices","title":"Best Practices","text":""},{"location":"concepts/replay-advanced/#1-start-with-defaults","title":"1. Start with Defaults","text":"<p>Use default retry configuration unless you have specific needs:</p> <pre><code>engine = ReplayEngine(trace)  # max_retries=3, exponential backoff\n</code></pre>"},{"location":"concepts/replay-advanced/#2-use-allowlists-for-tool-execution","title":"2. Use Allowlists for Tool Execution","text":"<p>Always specify which tools are safe to execute:</p> <pre><code>engine = ReplayEngine(\n    trace,\n    tool_registry=all_tools,\n    enable_tool_execution={\"allowlist\": [\"safe_tool_1\", \"safe_tool_2\"]},\n)\n</code></pre>"},{"location":"concepts/replay-advanced/#3-monitor-retry-counts","title":"3. Monitor Retry Counts","text":"<p>Track which spans require retries:</p> <pre><code>retry_spans = [s for s in result.spans if s.retry_count &gt; 0]\nif retry_spans:\n    print(f\"Warning: {len(retry_spans)} spans required retries\")\n</code></pre>"},{"location":"concepts/replay-advanced/#4-fallback-is-usually-sufficient","title":"4. Fallback is Usually Sufficient","text":"<p>Use <code>difflib</code> fallback unless semantic understanding is critical:</p> <pre><code># No need to install sentence-transformers for most use cases\npip install prela  # Fallback is fast and accurate enough\n</code></pre>"},{"location":"concepts/replay-advanced/#5-combine-features-carefully","title":"5. Combine Features Carefully","text":"<p>Enable only features you need:</p> <pre><code># Development: Enable everything\nengine = ReplayEngine(\n    trace,\n    max_retries=5,\n    enable_tool_execution=True,\n    enable_retrieval_execution=True,\n)\n\n# Production: Conservative settings\nengine = ReplayEngine(\n    trace,\n    max_retries=2,\n    enable_tool_execution={\"allowlist\": [\"read_only_tool\"]},\n    enable_retrieval_execution=False,  # Use cached data\n)\n</code></pre>"},{"location":"concepts/replay-advanced/#troubleshooting","title":"Troubleshooting","text":""},{"location":"concepts/replay-advanced/#issue-retries-not-working","title":"Issue: Retries Not Working","text":"<p>Symptoms: API calls fail immediately without retrying.</p> <p>Solutions: 1. Check error type is retryable:    <pre><code># 429, 503, 502 are retryable\n# 401, 403, 400 are not\n</code></pre></p> <ol> <li>Verify max_retries &gt; 0:    <pre><code>engine = ReplayEngine(trace, max_retries=3)  # Not 0\n</code></pre></li> </ol>"},{"location":"concepts/replay-advanced/#issue-tool-execution-failing","title":"Issue: Tool Execution Failing","text":"<p>Symptoms: Tools not executing during replay.</p> <p>Solutions: 1. Verify tool in registry:    <pre><code>print(tool_registry.keys())  # Check tool name matches\n</code></pre></p> <ol> <li> <p>Check allowlist/blocklist:    <pre><code>enable_tool_execution={\"allowlist\": [\"tool_name\"]}  # Exact match required\n</code></pre></p> </li> <li> <p>Ensure tool function signature is correct:    <pre><code>def my_tool(input: str) -&gt; str:  # Must accept string, return string\n    return result\n</code></pre></p> </li> </ol>"},{"location":"concepts/replay-advanced/#issue-retrieval-client-not-working","title":"Issue: Retrieval Client Not Working","text":"<p>Symptoms: Retrieval not re-executing, using cached data.</p> <p>Solutions: 1. Verify client type is supported:    <pre><code># ChromaDB fully supported\n# Others are placeholders\n</code></pre></p> <ol> <li>Check enable_retrieval_execution is True:    <pre><code>engine = ReplayEngine(\n    trace,\n    retrieval_client=client,\n    enable_retrieval_execution=True,  # Must be True\n)\n</code></pre></li> </ol>"},{"location":"concepts/replay-advanced/#next-steps","title":"Next Steps","text":"<ul> <li>Replay Multi-Agent Examples - Replay with CrewAI, AutoGen, LangGraph, Swarm</li> <li>Replay with Tools Examples - Tool re-execution patterns</li> <li>Basic Replay - Core replay concepts</li> <li>CLI Replay Commands - Command-line replay interface</li> </ul>"},{"location":"concepts/replay/","title":"Deterministic Replay","text":"<p>Re-execute captured traces with parameter modifications and compare results.</p>"},{"location":"concepts/replay/#what-is-replay","title":"What is Replay?","text":"<p>Replay enables you to re-execute previously captured traces in two modes:</p> <ol> <li>Exact Replay: Deterministic re-execution using cached data (no API calls, identical results)</li> <li>Modified Replay: Re-execution with parameter changes (makes real API calls for modified spans)</li> </ol> <p>This is powerful for:</p> <ul> <li>A/B Testing: Compare different models (GPT-4 vs Claude)</li> <li>Parameter Tuning: Test temperature, max_tokens, system prompts</li> <li>Regression Testing: Ensure new versions produce similar outputs</li> <li>Cost Optimization: Experiment with cheaper models</li> <li>Debugging: Reproduce issues from production traces</li> </ul>"},{"location":"concepts/replay/#how-replay-works","title":"How Replay Works","text":""},{"location":"concepts/replay/#replay-capture-automatic","title":"Replay Capture (Automatic)","text":"<p>When you enable replay capture, Prela automatically records additional data:</p> <pre><code>import prela\n\n# Enable replay capture\ntracer = prela.init(\n    service_name=\"my-agent\",\n    exporter=\"file\",\n    file_path=\"traces.jsonl\",\n    capture_for_replay=True  # \u2190 Enable replay\n)\n</code></pre> <p>Captured data includes:</p> <ul> <li>LLM Requests: Model, temperature, max_tokens, system_prompt, messages</li> <li>LLM Responses: Full response text, token usage, finish_reason</li> <li>Tool Calls: Function names, arguments, results</li> <li>Retrieval Operations: Queries, documents, similarity scores</li> <li>Agent State: Memory, context, configuration</li> </ul>"},{"location":"concepts/replay/#replay-execution","title":"Replay Execution","text":"<p>Load and replay a trace:</p> <pre><code>from prela.replay import ReplayEngine\nfrom prela.replay.loader import TraceLoader\n\n# Load trace from file\ntrace = TraceLoader.from_file(\"traces.jsonl\")\n\n# Create replay engine\nengine = ReplayEngine(trace)\n\n# Exact replay (no API calls)\nresult = engine.replay_exact()\n\nprint(f\"Duration: {result.total_duration_ms}ms\")\nprint(f\"Tokens: {result.total_tokens}\")\nprint(f\"Cost: ${result.total_cost_usd:.4f}\")\n</code></pre>"},{"location":"concepts/replay/#replay-modes","title":"Replay Modes","text":""},{"location":"concepts/replay/#1-exact-replay","title":"1. Exact Replay","text":"<p>Re-execute using captured data without API calls:</p> <pre><code># Deterministic, fast, free\nresult = engine.replay_exact()\n</code></pre> <p>Characteristics:</p> <ul> <li>\u2705 Deterministic: Always produces identical results</li> <li>\u2705 Fast: ~1ms per span (no network calls)</li> <li>\u2705 Free: No API costs</li> <li>\u2705 Offline: Works without API access</li> </ul> <p>Use Cases:</p> <ul> <li>Verify trace completeness</li> <li>Measure baseline performance</li> <li>Test comparison engine</li> <li>Debugging without costs</li> </ul>"},{"location":"concepts/replay/#2-modified-replay","title":"2. Modified Replay","text":"<p>Re-execute with parameter changes (makes real API calls):</p> <pre><code># Change model and temperature\nresult = engine.replay_with_modifications(\n    model=\"gpt-4o\",\n    temperature=0.7\n)\n</code></pre> <p>Available Modifications:</p> Parameter Description Example <code>model</code> Change LLM model <code>\"gpt-4o\"</code>, <code>\"claude-sonnet-4\"</code> <code>temperature</code> Adjust randomness <code>0.0</code> (deterministic) to <code>1.0</code> (creative) <code>system_prompt</code> Override system instructions <code>\"You are a helpful assistant\"</code> <code>max_tokens</code> Change output length limit <code>512</code>, <code>1024</code>, <code>4096</code> <code>mock_tool_responses</code> Override tool outputs <code>{\"search\": {\"results\": [...]}}</code> <code>mock_retrieval_results</code> Override retrieval results <code>{\"query\": {\"documents\": [...]}}</code> <p>Selective Re-execution:</p> <p>Only modified spans make real API calls. Unmodified spans use cached data:</p> <pre><code># Only LLM spans with gpt-4 \u2192 gpt-4o will call API\nresult = engine.replay_with_modifications(model=\"gpt-4o\")\n\n# If original trace used 3 LLM calls:\n# - 3 API calls are made (one per modified span)\n# - Tool calls use cached data\n# - Retrieval uses cached data\n</code></pre>"},{"location":"concepts/replay/#comparing-replays","title":"Comparing Replays","text":"<p>Compare two replay results to see differences:</p> <pre><code>from prela.replay import compare_replays\n\n# Exact replay (baseline)\noriginal = engine.replay_exact()\n\n# Modified replay (experiment)\nmodified = engine.replay_with_modifications(\n    model=\"gpt-4o\",\n    temperature=0.5\n)\n\n# Compare\ncomparison = compare_replays(original, modified)\n\n# Print summary\nprint(comparison.generate_summary())\n</code></pre>"},{"location":"concepts/replay/#comparison-output","title":"Comparison Output","text":"<pre><code>Replay Comparison Summary\n========================\n\nTotal Spans: 5\nSpans with Differences: 3\n\nChanges:\n- Output differences: 3 spans\n- Token changes: 2 spans (+150 tokens)\n- Cost changes: 2 spans (+$0.0045)\n- Duration changes: 3 spans (+234ms)\n\nSpan-by-Span Differences:\n\n1. anthropic.messages.create\n   \u2717 Output changed (semantic similarity: 85.3%)\n   \u2717 Tokens: 450 \u2192 600 (+150)\n   \u2717 Cost: $0.0090 \u2192 $0.0135 (+$0.0045)\n   \u2717 Duration: 823ms \u2192 1057ms (+234ms)\n\n2. langchain.tool.search\n   \u2713 No differences (cached data used)\n\n3. anthropic.messages.create\n   \u2717 Output changed (semantic similarity: 92.1%)\n   \u2713 Tokens unchanged (cached)\n   \u2717 Duration: 756ms \u2192 891ms (+135ms)\n</code></pre>"},{"location":"concepts/replay/#difference-types","title":"Difference Types","text":"Difference Description When It Appears Output Response text changed Modified LLM spans Input Request changed Modified prompts Tokens Token usage changed Model change, output length change Cost API cost changed Model change, token change Duration Execution time changed Real API calls vs cached Status Success \u2192 Error (or vice versa) API failures, timeouts Semantic Similarity Cosine similarity of embeddings Requires sentence-transformers"},{"location":"concepts/replay/#semantic-similarity","title":"Semantic Similarity","text":"<p>Compare text outputs semantically (requires optional dependency):</p> <pre><code>pip install sentence-transformers\n</code></pre> <p>The comparison engine uses embeddings to measure similarity:</p> <pre><code>comparison = compare_replays(original, modified)\n\nfor diff in comparison.differences:\n    if diff.semantic_similarity:\n        if diff.semantic_similarity &gt; 0.9:\n            print(f\"{diff.span_name}: Highly similar ({diff.semantic_similarity:.1%})\")\n        elif diff.semantic_similarity &gt; 0.7:\n            print(f\"{diff.span_name}: Moderately similar ({diff.semantic_similarity:.1%})\")\n        else:\n            print(f\"{diff.span_name}: Low similarity ({diff.semantic_similarity:.1%})\")\n</code></pre> <p>Interpretation:</p> <ul> <li>&gt; 90%: Nearly identical meaning (paraphrases)</li> <li>70-90%: Similar concepts, different wording</li> <li>50-70%: Related but divergent responses</li> <li>&lt; 50%: Significantly different outputs</li> </ul>"},{"location":"concepts/replay/#automatic-retry-logic","title":"Automatic Retry Logic","text":"<p>Prela automatically retries failed API calls with exponential backoff.</p>"},{"location":"concepts/replay/#how-it-works","title":"How It Works","text":"<p>When an API call fails with a transient error (rate limit, timeout, connection issue), Prela:</p> <ol> <li>Detects if error is retryable</li> <li>Waits with exponential backoff</li> <li>Retries up to configured maximum</li> <li>Tracks retry count per span</li> </ol> <p>Retryable Errors:</p> <ul> <li>HTTP 429 (Rate Limit)</li> <li>HTTP 503 (Service Unavailable)</li> <li>HTTP 502 (Bad Gateway)</li> <li>Connection timeouts</li> <li>Network errors</li> </ul> <p>Non-Retryable Errors:</p> <ul> <li>Authentication failures (401, 403)</li> <li>Invalid requests (400)</li> <li>Not found (404)</li> </ul>"},{"location":"concepts/replay/#configuration","title":"Configuration","text":"<pre><code>from prela.replay import ReplayEngine\n\n# Default: 3 retries, 1s initial delay, 60s max\nengine = ReplayEngine(trace)\n\n# Custom: More aggressive retry for flaky networks\nengine = ReplayEngine(\n    trace,\n    max_retries=5,\n    retry_initial_delay=2.0,\n    retry_max_delay=120.0,\n    retry_exponential_base=2.0,\n)\n\n# Fast-fail: Minimal retries\nengine = ReplayEngine(\n    trace,\n    max_retries=1,\n    retry_initial_delay=0.5,\n)\n</code></pre>"},{"location":"concepts/replay/#exponential-backoff","title":"Exponential Backoff","text":"<p>Delays double with each retry (capped at max_delay):</p> <ul> <li>Attempt 0: No delay (initial request)</li> <li>Attempt 1: 1.0s delay</li> <li>Attempt 2: 2.0s delay</li> <li>Attempt 3: 4.0s delay</li> <li>Attempt 4: 8.0s delay (capped at max_delay)</li> </ul>"},{"location":"concepts/replay/#monitoring-retries","title":"Monitoring Retries","text":"<pre><code>result = engine.replay_with_modifications(model=\"gpt-4o\")\n\n# Check which spans required retries\nfor span in result.spans:\n    if span.retry_count &gt; 0:\n        print(f\"\u26a0\ufe0f  {span.name} required {span.retry_count} retries\")\n</code></pre>"},{"location":"concepts/replay/#semantic-similarity-fallback","title":"Semantic Similarity Fallback","text":"<p>Prela provides intelligent fallback when <code>sentence-transformers</code> is unavailable.</p>"},{"location":"concepts/replay/#fallback-strategy","title":"Fallback Strategy","text":"<p>Without sentence-transformers (fallback):</p> <ol> <li>Exact Match (fastest) - Returns 1.0 for identical strings</li> <li>difflib.SequenceMatcher (primary) - Edit distance-based similarity (0.0-1.0)</li> <li>Jaccard Word Similarity (secondary) - Word overlap measurement (0.0-1.0)</li> </ol> <p>With sentence-transformers (best accuracy):</p> <ul> <li>Uses <code>all-MiniLM-L6-v2</code> embedding model</li> <li>Computes cosine similarity between embeddings</li> <li>Better for paraphrasing and semantic equivalence</li> </ul>"},{"location":"concepts/replay/#performance-comparison","title":"Performance Comparison","text":"Method Speed Accuracy Use Case Exact match Instant Perfect for identical text Quick check difflib ~1-5ms Good for typos, minor edits General use Jaccard ~1-5ms Good for word reordering Paraphrasing Embeddings ~10-50ms Best for semantic similarity Production"},{"location":"concepts/replay/#difflib-accuracy","title":"difflib Accuracy","text":"<pre><code># Same text, different case\n\"Hello World\" vs \"hello world\" \u2192 0.82 (82%)\n\n# Minor edit\n\"brown fox\" vs \"red fox\" \u2192 0.85 (85%)\n\n# Word reorder\n\"cat dog bird\" vs \"dog bird cat\" \u2192 0.67 (67%)\n\n# Completely different\n\"apple\" vs \"orange\" \u2192 0.0 (0%)\n</code></pre>"},{"location":"concepts/replay/#when-to-install-sentence-transformers","title":"When to Install sentence-transformers","text":"<p>Install for production use when you need:</p> <ul> <li>Paraphrase detection (\"quick\" vs \"fast\")</li> <li>Semantic equivalence (\"start\" vs \"begin\")</li> <li>High accuracy requirements</li> </ul> <pre><code>pip install prela[similarity]  # Installs sentence-transformers (~500MB)\n</code></pre>"},{"location":"concepts/replay/#checking-availability","title":"Checking Availability","text":"<pre><code>from prela.replay import compare_replays\n\ncomparison = compare_replays(original, modified)\n\nif comparison.semantic_similarity_available:\n    print(f\"Using embeddings: {comparison.semantic_similarity_model}\")\nelse:\n    print(\"Using fallback: difflib + Jaccard\")\n</code></pre>"},{"location":"concepts/replay/#tool-re-execution","title":"Tool Re-execution","text":"<p>Re-execute tools during replay instead of using cached data.</p>"},{"location":"concepts/replay/#3-tier-priority-system","title":"3-Tier Priority System","text":"<p>For tool spans, Prela uses this priority order:</p> <ol> <li>Mock responses (highest) - Always used if provided</li> <li>Real execution - Used if enabled, mocks not provided</li> <li>Cached data (default) - Original captured output</li> </ol> <p>This prevents accidental execution while allowing controlled testing.</p>"},{"location":"concepts/replay/#basic-usage","title":"Basic Usage","text":"<pre><code># Define tool functions\ndef my_calculator(input_data):\n    return {\"result\": input_data[\"a\"] + input_data[\"b\"]}\n\ndef my_search(input_data):\n    # Actual search implementation\n    return {\"results\": [...]}\n\n# Create tool registry\ntool_registry = {\n    \"calculator\": my_calculator,\n    \"search\": my_search,\n}\n\n# Re-execute tools\nresult = engine.replay_with_modifications(\n    enable_tool_execution=True,\n    tool_registry=tool_registry,\n)\n</code></pre>"},{"location":"concepts/replay/#safety-controls","title":"Safety Controls","text":"<p>Allowlist (only execute specific tools):</p> <pre><code>result = engine.replay_with_modifications(\n    enable_tool_execution=True,\n    tool_execution_allowlist=[\"calculator\", \"search\"],  # Only these\n    tool_registry=tool_registry,\n)\n</code></pre> <p>Blocklist (never execute specific tools):</p> <pre><code>result = engine.replay_with_modifications(\n    enable_tool_execution=True,\n    tool_execution_blocklist=[\"delete_file\", \"shutdown\"],  # Block these\n    tool_registry=tool_registry,\n)\n</code></pre> <p>Note: Blocklist takes precedence over allowlist.</p>"},{"location":"concepts/replay/#use-cases","title":"Use Cases","text":"<p>Testing with Different Tool Implementations:</p> <pre><code># Original used production API\n# Replay with mock API for testing\ndef mock_api(input_data):\n    return {\"status\": \"success\", \"data\": \"test\"}\n\ntool_registry = {\"api_call\": mock_api}\n\nresult = engine.replay_with_modifications(\n    enable_tool_execution=True,\n    tool_execution_allowlist=[\"api_call\"],\n    tool_registry=tool_registry,\n)\n</code></pre> <p>Debugging with Fresh Data:</p> <pre><code># Re-run search tool to see if results changed\ndef fresh_search(input_data):\n    # Query current database\n    return {...}\n\nresult = engine.replay_with_modifications(\n    enable_tool_execution=True,\n    tool_execution_allowlist=[\"search\"],\n    tool_registry={\"search\": fresh_search},\n)\n</code></pre>"},{"location":"concepts/replay/#retrieval-re-execution","title":"Retrieval Re-execution","text":"<p>Re-query vector databases during replay to test with updated data.</p>"},{"location":"concepts/replay/#3-tier-priority-system_1","title":"3-Tier Priority System","text":"<p>For retrieval spans, Prela uses this priority order:</p> <ol> <li>Mock results (highest) - Always used if provided</li> <li>Real execution - Re-queries if enabled, mocks not provided</li> <li>Cached data (default) - Original retrieved documents</li> </ol>"},{"location":"concepts/replay/#supported-vector-databases","title":"Supported Vector Databases","text":"<ul> <li>\u2705 ChromaDB - Fully implemented</li> <li>\u26a0\ufe0f Pinecone - Requires embedding model (placeholder)</li> <li>\u26a0\ufe0f Qdrant - Requires embedding model (placeholder)</li> <li>\u26a0\ufe0f Weaviate - Requires class name (placeholder)</li> </ul>"},{"location":"concepts/replay/#chromadb-example","title":"ChromaDB Example","text":"<pre><code>import chromadb\n\n# Setup ChromaDB client\nclient = chromadb.Client()\ncollection = client.create_collection(\"my_docs\")\n\n# Add some documents\ncollection.add(\n    documents=[\"Updated document 1\", \"Updated document 2\"],\n    ids=[\"1\", \"2\"],\n)\n\n# Re-query with current data\nresult = engine.replay_with_modifications(\n    enable_retrieval_execution=True,\n    retrieval_client=collection,\n)\n</code></pre>"},{"location":"concepts/replay/#query-override","title":"Query Override","text":"<p>Change retrieval query during replay:</p> <pre><code># Original query: \"What is Python?\"\n# Test with different query\nresult = engine.replay_with_modifications(\n    enable_retrieval_execution=True,\n    retrieval_client=collection,\n    retrieval_query_override=\"What is JavaScript?\",\n)\n</code></pre>"},{"location":"concepts/replay/#use-cases_1","title":"Use Cases","text":"<p>Testing with Updated Vector Store:</p> <pre><code># Original trace used old embeddings\n# Replay with newly indexed documents\nnew_client = chromadb.Client()\n# ... add updated documents ...\n\nresult = engine.replay_with_modifications(\n    enable_retrieval_execution=True,\n    retrieval_client=new_client,\n)\n</code></pre> <p>A/B Testing Retrieval Strategies:</p> <pre><code>queries = [\n    \"Direct question\",\n    \"Rephrased question\",\n    \"Keywords only\",\n]\n\nresults = {}\nfor query in queries:\n    results[query] = engine.replay_with_modifications(\n        enable_retrieval_execution=True,\n        retrieval_client=client,\n        retrieval_query_override=query,\n    )\n</code></pre>"},{"location":"concepts/replay/#cost-estimation","title":"Cost Estimation","text":"<p>Estimate costs without making API calls:</p> <pre><code># Load trace\ntrace = TraceLoader.from_file(\"traces.jsonl\")\nengine = ReplayEngine(trace)\n\n# Estimate cost of replay with different model\nresult = engine.replay_exact()  # No API calls\n\nprint(f\"Original cost: ${result.total_cost_usd:.4f}\")\nprint(f\"Total tokens: {result.total_tokens}\")\n\n# Estimate new cost by inspecting token usage\n# (Actual API call costs will vary slightly)\n</code></pre> <p>Supported Models:</p> <ul> <li>OpenAI: gpt-4, gpt-4o, gpt-3.5-turbo, o1-preview, o1-mini</li> <li>Anthropic: claude-3-opus, claude-3-sonnet, claude-3-haiku, claude-sonnet-4</li> </ul>"},{"location":"concepts/replay/#loading-traces","title":"Loading Traces","text":""},{"location":"concepts/replay/#from-file","title":"From File","text":"<pre><code>from prela.replay.loader import TraceLoader\n\n# JSON file (single trace)\ntrace = TraceLoader.from_file(\"trace.json\")\n\n# JSONL file (picks first trace)\ntrace = TraceLoader.from_file(\"traces.jsonl\")\n</code></pre>"},{"location":"concepts/replay/#from-dictionary","title":"From Dictionary","text":"<pre><code># From exported trace dict\ntrace_dict = {\n    \"trace_id\": \"abc-123\",\n    \"spans\": [...]\n}\ntrace = TraceLoader.from_dict(trace_dict)\n</code></pre>"},{"location":"concepts/replay/#from-span-list","title":"From Span List","text":"<pre><code># From list of Span objects\nfrom prela.core import Span\n\nspans = [span1, span2, span3]\ntrace = TraceLoader.from_spans(spans)\n</code></pre>"},{"location":"concepts/replay/#cli-usage","title":"CLI Usage","text":"<p>Replay traces from the command line:</p> <pre><code># Exact replay\nprela replay trace.json\n\n# Modified replay with comparison\nprela replay trace.json --model gpt-4o --compare\n\n# Override multiple parameters\nprela replay trace.json \\\n  --model claude-sonnet-4 \\\n  --temperature 0.7 \\\n  --system-prompt \"You are an expert assistant\" \\\n  --output result.json\n\n# Save comparison report\nprela replay trace.json --model gpt-4o --compare --output comparison.json\n</code></pre> <p>CLI Options:</p> Option Description Example <code>--model</code> Override model <code>--model gpt-4o</code> <code>--temperature</code> Set temperature <code>--temperature 0.7</code> <code>--system-prompt</code> Override system prompt <code>--system-prompt \"Be concise\"</code> <code>--max-tokens</code> Set max tokens <code>--max-tokens 1024</code> <code>--compare</code> Compare with original <code>--compare</code> <code>--output</code> Save result to file <code>--output result.json</code>"},{"location":"concepts/replay/#architecture","title":"Architecture","text":""},{"location":"concepts/replay/#trace-tree-reconstruction","title":"Trace Tree Reconstruction","text":"<p>Traces are loaded and organized into a tree structure:</p> <pre><code>graph TD\n    A[Root Span: Agent] --&gt; B[Span: LLM Call]\n    A --&gt; C[Span: Tool Call]\n    C --&gt; D[Span: LLM Call]\n    A --&gt; E[Span: Final LLM Call]\n\n    style A fill:#4F46E5\n    style B fill:#6366F1\n    style C fill:#818CF8\n    style D fill:#A5B4FC\n    style E fill:#6366F1</code></pre> <p>Depth-First Execution:</p> <p>Spans are replayed in depth-first order to match original execution:</p> <ol> <li>Root span starts</li> <li>First child executes completely (including its children)</li> <li>Second child executes completely</li> <li>And so on...</li> </ol> <p>This ensures parent-child dependencies are respected.</p>"},{"location":"concepts/replay/#selective-api-calls","title":"Selective API Calls","text":"<p>The replay engine determines which spans need real API calls:</p> <pre><code>def _span_needs_modification(span):\n    if span.span_type != SpanType.LLM:\n        return False  # Only LLM spans can be modified\n\n    if modifications.get(\"model\") and span.model != modifications[\"model\"]:\n        return True  # Model changed\n\n    if modifications.get(\"temperature\") and span.temperature != modifications[\"temperature\"]:\n        return True  # Temperature changed\n\n    # ... check other parameters\n\n    return False  # Use cached data\n</code></pre> <p>Optimization:</p> <ul> <li>Only modified spans call APIs</li> <li>Cached data used for unchanged spans</li> <li>Significant cost and latency savings</li> </ul>"},{"location":"concepts/replay/#best-practices","title":"Best Practices","text":""},{"location":"concepts/replay/#1-enable-replay-selectively","title":"1. Enable Replay Selectively","text":"<p>Only enable replay capture when needed:</p> <pre><code># Production: No replay (minimal overhead)\nprela.init(service_name=\"prod\", capture_for_replay=False)\n\n# Development: Enable replay\nprela.init(service_name=\"dev\", capture_for_replay=True)\n</code></pre>"},{"location":"concepts/replay/#2-use-exact-replay-first","title":"2. Use Exact Replay First","text":"<p>Always start with exact replay to verify completeness:</p> <pre><code>engine = ReplayEngine(trace)\nresult = engine.replay_exact()\n\nif not result.spans:\n    print(\"Warning: Trace has no replay data\")\n    return\n</code></pre>"},{"location":"concepts/replay/#3-compare-semantically","title":"3. Compare Semantically","text":"<p>Use semantic similarity for meaningful comparisons:</p> <pre><code>comparison = compare_replays(original, modified)\n\nfor diff in comparison.differences:\n    if diff.field == \"output\" and diff.semantic_similarity:\n        if diff.semantic_similarity &lt; 0.7:\n            print(f\"\u26a0\ufe0f  Significant divergence in {diff.span_name}\")\n</code></pre>"},{"location":"concepts/replay/#4-batch-experiments","title":"4. Batch Experiments","text":"<p>Test multiple configurations efficiently:</p> <pre><code>models = [\"gpt-4\", \"gpt-4o\", \"claude-sonnet-4\"]\ntemperatures = [0.0, 0.5, 1.0]\n\nresults = {}\nfor model in models:\n    for temp in temperatures:\n        key = f\"{model}_temp{temp}\"\n        results[key] = engine.replay_with_modifications(\n            model=model,\n            temperature=temp\n        )\n\n# Compare all results\nfor key, result in results.items():\n    print(f\"{key}: ${result.total_cost_usd:.4f}, {result.total_tokens} tokens\")\n</code></pre>"},{"location":"concepts/replay/#5-store-comparisons","title":"5. Store Comparisons","text":"<p>Save comparison reports for analysis:</p> <pre><code>comparison = compare_replays(original, modified)\n\n# Save to JSON\nimport json\nwith open(\"comparison.json\", \"w\") as f:\n    json.dump({\n        \"summary\": comparison.generate_summary(),\n        \"differences\": [\n            {\n                \"span\": diff.span_name,\n                \"field\": diff.field,\n                \"similarity\": diff.semantic_similarity\n            }\n            for diff in comparison.differences\n        ]\n    }, f, indent=2)\n</code></pre>"},{"location":"concepts/replay/#limitations","title":"Limitations","text":""},{"location":"concepts/replay/#1-tool-side-effects","title":"1. Tool Side Effects","text":"<p>Tools with side effects cannot be replayed safely:</p> <pre><code># \u274c Cannot replay safely\ndef send_email(to, subject, body):\n    smtp.send(to, subject, body)  # Side effect!\n\n# \u2705 Use mocking for side effects\nresult = engine.replay_with_modifications(\n    mock_tool_responses={\n        \"send_email\": {\"status\": \"sent\", \"message_id\": \"mock-123\"}\n    }\n)\n</code></pre>"},{"location":"concepts/replay/#2-streaming-responses","title":"2. Streaming Responses","text":"<p>Streaming is supported with real-time output:</p> <pre><code>from prela.replay import ReplayEngine\nfrom prela.replay.loader import TraceLoader\n\n# Load trace\ntrace = TraceLoader.from_file(\"trace.json\")\nengine = ReplayEngine(trace)\n\n# Replay with streaming enabled\ndef on_chunk(chunk: str):\n    print(chunk, end=\"\", flush=True)\n\nresult = engine.replay_with_modifications(\n    model=\"gpt-4o\",\n    stream=True,\n    stream_callback=on_chunk\n)\n</code></pre> <p>Note: Streaming works for both OpenAI and Anthropic models during replay execution.</p>"},{"location":"concepts/replay/#3-vendor-support","title":"3. Vendor Support","text":"<p>Currently supports OpenAI and Anthropic only:</p> <ul> <li>\u2705 OpenAI (gpt-, o1-)</li> <li>\u2705 Anthropic (claude-*)</li> <li>\u274c Other vendors (use exact replay only)</li> </ul>"},{"location":"concepts/replay/#performance","title":"Performance","text":""},{"location":"concepts/replay/#exact-replay","title":"Exact Replay","text":"<ul> <li>Speed: ~1ms per span</li> <li>Memory: O(n) where n = number of spans</li> <li>Cost: $0 (no API calls)</li> </ul>"},{"location":"concepts/replay/#modified-replay","title":"Modified Replay","text":"<ul> <li>Speed: Depends on API latency</li> <li>Memory: O(n) + API response buffers</li> <li>Cost: Only modified LLM spans charged</li> </ul>"},{"location":"concepts/replay/#comparison","title":"Comparison","text":"<ul> <li>Speed: ~10-50ms per comparison (with embeddings)</li> <li>Memory: O(n) for difference list</li> <li>Accuracy: 70%+ similarity for semantically similar texts</li> </ul>"},{"location":"concepts/replay/#api-reference","title":"API Reference","text":"<p>See Replay API Documentation for detailed API reference.</p>"},{"location":"concepts/replay/#next-steps","title":"Next Steps","text":"<ul> <li>Replay Examples: Practical examples and recipes</li> <li>CLI Commands: Command-line replay reference</li> <li>API Reference: Complete API documentation</li> </ul>"},{"location":"concepts/sampling/","title":"Sampling","text":"<p>Sampling controls which traces are captured and exported. In production systems with high traffic, sampling every trace can be prohibitively expensive. Prela provides multiple sampling strategies to balance observability with performance.</p>"},{"location":"concepts/sampling/#overview","title":"Overview","text":"<p>Sampling decisions are made at the trace level (not individual spans). Once a trace is sampled, all spans in that trace are captured. This ensures complete trace data for sampled requests.</p> <pre><code>graph TD\n    A[Incoming Request] --&gt; B{Should Sample?}\n    B --&gt;|Yes| C[Create Trace]\n    B --&gt;|No| D[Skip Tracing]\n    C --&gt; E[Capture All Spans]\n    C --&gt; F[Export Trace]\n    D --&gt; G[No Overhead]</code></pre>"},{"location":"concepts/sampling/#sampling-strategies","title":"Sampling Strategies","text":"<p>Prela provides four built-in samplers:</p> Sampler Use Case Overhead <code>AlwaysOnSampler</code> Development, debugging Highest <code>AlwaysOffSampler</code> Disable tracing None <code>ProbabilitySampler</code> Production (probabilistic) Medium <code>RateLimitingSampler</code> Production (rate limiting) Low"},{"location":"concepts/sampling/#alwaysonsampler","title":"AlwaysOnSampler","text":"<p>Samples every trace. Useful for development and low-traffic environments.</p> <pre><code>from prela import init\nfrom prela.core.sampler import AlwaysOnSampler\n\ntracer = init(\n    service_name=\"my-app\",\n    sampler=AlwaysOnSampler()\n)\n\n# Or use sample_rate=1.0\ntracer = init(service_name=\"my-app\", sample_rate=1.0)\n</code></pre> <p>Characteristics: - \u2705 Complete observability - \u2705 Simple and predictable - \u274c Not suitable for high-traffic production - \u274c Can generate massive trace volumes</p>"},{"location":"concepts/sampling/#alwaysoffsampler","title":"AlwaysOffSampler","text":"<p>Never samples traces. Useful for temporarily disabling tracing.</p> <pre><code>from prela import init\nfrom prela.core.sampler import AlwaysOffSampler\n\ntracer = init(\n    service_name=\"my-app\",\n    sampler=AlwaysOffSampler()\n)\n\n# Or use sample_rate=0.0\ntracer = init(service_name=\"my-app\", sample_rate=0.0)\n</code></pre> <p>Characteristics: - \u2705 Zero overhead - \u2705 Instant disable without code changes - \u274c No observability</p> <p>Environment Variable: <pre><code># Disable tracing via environment\nexport PRELA_SAMPLE_RATE=0.0\n</code></pre></p>"},{"location":"concepts/sampling/#probabilitysampler","title":"ProbabilitySampler","text":"<p>Samples traces with a given probability (0.0 to 1.0). Uses deterministic hashing to ensure consistent sampling decisions for the same trace ID.</p> <pre><code>from prela import init\nfrom prela.core.sampler import ProbabilitySampler\n\n# Sample 10% of traces\ntracer = init(\n    service_name=\"my-app\",\n    sampler=ProbabilitySampler(rate=0.1)\n)\n\n# Or use sample_rate parameter\ntracer = init(service_name=\"my-app\", sample_rate=0.1)\n</code></pre>"},{"location":"concepts/sampling/#how-it-works","title":"How It Works","text":"<ol> <li>Compute MD5 hash of trace ID</li> <li>Convert hash to float (0.0 to 1.0)</li> <li>Sample if hash value &lt; sampling rate</li> </ol> <p>Key Property: The same trace ID always produces the same sampling decision across all services in a distributed system.</p> <pre><code>from prela.core.sampler import ProbabilitySampler\n\nsampler = ProbabilitySampler(rate=0.1)\n\ntrace_id = \"550e8400-e29b-41d4-a716-446655440000\"\nresult1 = sampler.should_sample(trace_id)  # True or False\nresult2 = sampler.should_sample(trace_id)  # Same as result1\n\n# Deterministic: same trace_id = same decision\nassert result1 == result2\n</code></pre>"},{"location":"concepts/sampling/#configuration","title":"Configuration","text":"<pre><code># Sample 1% of traces (recommended for high-traffic production)\nProbabilitySampler(rate=0.01)\n\n# Sample 25% of traces (moderate traffic)\nProbabilitySampler(rate=0.25)\n\n# Sample 50% of traces (development/staging)\nProbabilitySampler(rate=0.5)\n</code></pre> <p>Characteristics: - \u2705 Predictable sampling rate - \u2705 Deterministic (consistent across services) - \u2705 Low computational overhead - \u274c Can miss rare events - \u274c Not aware of system load</p>"},{"location":"concepts/sampling/#ratelimitingsampler","title":"RateLimitingSampler","text":"<p>Limits traces to a maximum rate (traces per second). Uses a token bucket algorithm.</p> <pre><code>from prela import init\nfrom prela.core.sampler import RateLimitingSampler\n\n# Allow max 10 traces per second\ntracer = init(\n    service_name=\"my-app\",\n    sampler=RateLimitingSampler(traces_per_second=10.0)\n)\n</code></pre>"},{"location":"concepts/sampling/#how-it-works_1","title":"How It Works","text":"<p>Token bucket algorithm: 1. Bucket holds tokens (max = <code>traces_per_second</code>) 2. Tokens regenerate at <code>traces_per_second</code> rate 3. Each trace consumes 1 token 4. If bucket empty, trace is rejected</p> <pre><code>graph LR\n    A[Token Bucket] --&gt;|Has tokens?| B{Sample}\n    B --&gt;|Yes| C[Consume token]\n    B --&gt;|No| D[Reject]\n    E[Token Regeneration] --&gt;|traces_per_second| A</code></pre>"},{"location":"concepts/sampling/#configuration_1","title":"Configuration","text":"<pre><code># Low rate limit (high-traffic production)\nRateLimitingSampler(traces_per_second=1.0)\n\n# Medium rate limit\nRateLimitingSampler(traces_per_second=10.0)\n\n# High rate limit (staging)\nRateLimitingSampler(traces_per_second=100.0)\n</code></pre> <p>Characteristics: - \u2705 Guarantees max trace volume - \u2705 Load-aware (rejects when busy) - \u2705 Thread-safe - \u274c May reject important traces during bursts - \u274c Not deterministic (depends on timing)</p>"},{"location":"concepts/sampling/#choosing-a-sampler","title":"Choosing a Sampler","text":""},{"location":"concepts/sampling/#development","title":"Development","text":"<p>Use <code>AlwaysOnSampler</code> for complete visibility:</p> <pre><code>tracer = init(service_name=\"dev-app\", sample_rate=1.0)\n</code></pre>"},{"location":"concepts/sampling/#stagingqa","title":"Staging/QA","text":"<p>Use <code>ProbabilitySampler</code> with moderate rate:</p> <pre><code>tracer = init(service_name=\"staging-app\", sample_rate=0.5)\n</code></pre>"},{"location":"concepts/sampling/#production-low-traffic","title":"Production (Low Traffic)","text":"<p>Use <code>ProbabilitySampler</code> with 10-25% sampling:</p> <pre><code>tracer = init(service_name=\"prod-app\", sample_rate=0.1)\n</code></pre>"},{"location":"concepts/sampling/#production-high-traffic","title":"Production (High Traffic)","text":"<p>Use <code>RateLimitingSampler</code> to cap trace volume:</p> <pre><code>from prela.core.sampler import RateLimitingSampler\n\ntracer = init(\n    service_name=\"prod-app\",\n    sampler=RateLimitingSampler(traces_per_second=10.0)\n)\n</code></pre>"},{"location":"concepts/sampling/#production-very-high-traffic","title":"Production (Very High Traffic)","text":"<p>Combine low probability with rate limiting:</p> <pre><code>from prela.core.sampler import ProbabilitySampler\n\n# Sample 1% of traces (typical for large-scale systems)\ntracer = init(service_name=\"prod-app\", sample_rate=0.01)\n</code></pre>"},{"location":"concepts/sampling/#environment-based-configuration","title":"Environment-Based Configuration","text":"<p>Use environment variables for runtime configuration:</p> <pre><code># Set sampling rate\nexport PRELA_SAMPLE_RATE=0.1\n\n# Disable tracing\nexport PRELA_SAMPLE_RATE=0.0\n\n# Enable full tracing\nexport PRELA_SAMPLE_RATE=1.0\n</code></pre> <p>Python code: <pre><code>import os\nfrom prela import init\n\n# Reads from PRELA_SAMPLE_RATE environment variable\ntracer = init(service_name=\"my-app\")\n\n# Override environment variable\ntracer = init(service_name=\"my-app\", sample_rate=0.5)\n</code></pre></p>"},{"location":"concepts/sampling/#custom-samplers","title":"Custom Samplers","text":"<p>Implement custom sampling logic by extending <code>BaseSampler</code>:</p> <pre><code>from prela.core.sampler import BaseSampler\n\nclass UserBasedSampler(BaseSampler):\n    \"\"\"Sample based on user ID.\"\"\"\n\n    def __init__(self, vip_users: set[str]):\n        self.vip_users = vip_users\n\n    def should_sample(self, trace_id: str) -&gt; bool:\n        # Always sample VIP users\n        from prela.core.context import get_trace_context\n        ctx = get_trace_context()\n\n        user_id = ctx.baggage.get(\"user_id\")\n        if user_id in self.vip_users:\n            return True\n\n        # Sample 1% of other users\n        import hashlib\n        hash_value = int.from_bytes(\n            hashlib.md5(trace_id.encode()).digest()[:8],\n            byteorder=\"big\"\n        )\n        return (hash_value / (2**64 - 1)) &lt; 0.01\n\n\n# Use custom sampler\ntracer = init(\n    service_name=\"my-app\",\n    sampler=UserBasedSampler(vip_users={\"user123\", \"user456\"})\n)\n</code></pre>"},{"location":"concepts/sampling/#head-based-vs-tail-based-sampling","title":"Head-Based vs Tail-Based Sampling","text":""},{"location":"concepts/sampling/#head-based-sampling-prelas-approach","title":"Head-Based Sampling (Prela's Approach)","text":"<p>Sampling decision made before trace execution:</p> <pre><code># Decision made here (before span creation)\nwith get_tracer().span(\"operation\") as span:\n    # If sampled, span is captured\n    # If not sampled, span is not created\n    do_work()\n</code></pre> <p>Advantages: - \u2705 Low overhead (unsampled traces skipped) - \u2705 Simple implementation - \u2705 Predictable resource usage</p> <p>Disadvantages: - \u274c May miss important traces (errors, slow requests) - \u274c Decision made without runtime context</p>"},{"location":"concepts/sampling/#tail-based-sampling-not-currently-supported","title":"Tail-Based Sampling (Not Currently Supported)","text":"<p>Sampling decision made after trace execution based on properties:</p> <p>Advantages: - \u2705 Can sample errors and slow requests - \u2705 Context-aware decisions</p> <p>Disadvantages: - \u274c Higher overhead (all traces captured initially) - \u274c Requires buffering - \u274c Complex implementation</p>"},{"location":"concepts/sampling/#sampling-best-practices","title":"Sampling Best Practices","text":""},{"location":"concepts/sampling/#1-start-with-high-sampling-reduce-gradually","title":"1. Start with High Sampling, Reduce Gradually","text":"<pre><code># Development\ntracer = init(service_name=\"app\", sample_rate=1.0)\n\n# Staging\ntracer = init(service_name=\"app\", sample_rate=0.5)\n\n# Production (initial)\ntracer = init(service_name=\"app\", sample_rate=0.25)\n\n# Production (stable)\ntracer = init(service_name=\"app\", sample_rate=0.05)\n</code></pre>"},{"location":"concepts/sampling/#2-monitor-sampling-rate-effectiveness","title":"2. Monitor Sampling Rate Effectiveness","text":"<pre><code># Log sampling decisions for analysis\nimport logging\n\nclass LoggingSampler(BaseSampler):\n    def __init__(self, base_sampler: BaseSampler):\n        self.base_sampler = base_sampler\n        self.logger = logging.getLogger(__name__)\n\n    def should_sample(self, trace_id: str) -&gt; bool:\n        result = self.base_sampler.should_sample(trace_id)\n        self.logger.debug(f\"Trace {trace_id}: sampled={result}\")\n        return result\n</code></pre>"},{"location":"concepts/sampling/#3-use-different-rates-for-different-services","title":"3. Use Different Rates for Different Services","text":"<pre><code># Critical user-facing service: higher sampling\nuser_api_tracer = init(service_name=\"user-api\", sample_rate=0.5)\n\n# Background worker: lower sampling\nworker_tracer = init(service_name=\"worker\", sample_rate=0.01)\n</code></pre>"},{"location":"concepts/sampling/#4-consider-business-impact","title":"4. Consider Business Impact","text":"<pre><code>class BusinessAwareSampler(BaseSampler):\n    def should_sample(self, trace_id: str) -&gt; bool:\n        ctx = get_trace_context()\n\n        # Always sample premium customers\n        if ctx.baggage.get(\"customer_tier\") == \"premium\":\n            return True\n\n        # Always sample payment operations\n        if ctx.baggage.get(\"operation_type\") == \"payment\":\n            return True\n\n        # Sample 1% of others\n        return ProbabilitySampler(0.01).should_sample(trace_id)\n</code></pre>"},{"location":"concepts/sampling/#5-document-your-sampling-strategy","title":"5. Document Your Sampling Strategy","text":"<pre><code># Production sampling configuration\n# - 1% baseline sampling for cost control\n# - Premium customers: 100% sampling for SLA monitoring\n# - Payment flows: 100% sampling for compliance\n# - Expected volume: ~100 traces/sec at peak (from 10K req/sec)\n\ntracer = init(\n    service_name=\"production-api\",\n    sampler=BusinessAwareSampler()\n)\n</code></pre>"},{"location":"concepts/sampling/#performance-impact","title":"Performance Impact","text":""},{"location":"concepts/sampling/#sampling-overhead","title":"Sampling Overhead","text":"Sampler CPU Overhead Memory Overhead AlwaysOnSampler Negligible Negligible AlwaysOffSampler None None ProbabilitySampler ~10\u03bcs per decision Negligible RateLimitingSampler ~5\u03bcs per decision Negligible"},{"location":"concepts/sampling/#unsampled-traces","title":"Unsampled Traces","text":"<p>When a trace is not sampled, Prela: - \u274c Does not create span objects - \u274c Does not capture attributes/events - \u274c Does not export data - \u2705 Overhead: ~10\u03bcs for sampling decision</p> <pre><code># With 0.01 sampling rate:\n# - 99% of requests: ~10\u03bcs overhead (sampling decision only)\n# - 1% of requests: Full tracing overhead\n</code></pre>"},{"location":"concepts/sampling/#troubleshooting","title":"Troubleshooting","text":""},{"location":"concepts/sampling/#problem-too-few-traces","title":"Problem: Too few traces","text":"<p>Symptom: Not seeing enough data in production.</p> <p>Solution: Increase sampling rate temporarily:</p> <pre><code># Temporary increase\nexport PRELA_SAMPLE_RATE=0.5\n</code></pre>"},{"location":"concepts/sampling/#problem-too-many-traces","title":"Problem: Too many traces","text":"<p>Symptom: Overwhelming trace volume, high storage costs.</p> <p>Solution: Decrease sampling rate or use rate limiting:</p> <pre><code>from prela.core.sampler import RateLimitingSampler\n\n# Cap at 10 traces/second\ntracer = init(sampler=RateLimitingSampler(traces_per_second=10.0))\n</code></pre>"},{"location":"concepts/sampling/#problem-missing-important-traces","title":"Problem: Missing important traces","text":"<p>Symptom: Errors or slow requests not captured.</p> <p>Solution: Implement custom sampler to always sample errors:</p> <pre><code>class ErrorAwareSampler(BaseSampler):\n    def should_sample(self, trace_id: str) -&gt; bool:\n        # Start with low baseline\n        if ProbabilitySampler(0.01).should_sample(trace_id):\n            return True\n\n        # Check if error occurred (set via baggage)\n        ctx = get_trace_context()\n        if ctx.baggage.get(\"has_error\"):\n            return True\n\n        return False\n</code></pre>"},{"location":"concepts/sampling/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Exporters for sending traces to backends</li> <li>Explore Context Propagation for distributed tracing</li> <li>See Production Setup for deployment patterns</li> </ul>"},{"location":"concepts/spans/","title":"Spans","text":"<p>Spans are the fundamental building blocks of distributed tracing in Prela. A span represents a unit of work in your AI agent system, such as an LLM call, tool invocation, or custom operation.</p>"},{"location":"concepts/spans/#overview","title":"Overview","text":"<p>Each span captures:</p> <ul> <li>Identity: Unique span ID and trace ID for correlation</li> <li>Timing: Start and end timestamps with nanosecond precision</li> <li>Metadata: Attributes, events, and status information</li> <li>Hierarchy: Parent-child relationships for nested operations</li> </ul> <pre><code>graph TD\n    A[Root Span: Agent Request] --&gt; B[Span: LLM Call]\n    A --&gt; C[Span: Tool Execution]\n    C --&gt; D[Span: Database Query]\n    B --&gt; E[Span: Token Generation]</code></pre>"},{"location":"concepts/spans/#span-structure","title":"Span Structure","text":"<pre><code>from prela import Span, SpanType, SpanStatus\n\nspan = Span(\n    span_id=\"550e8400-e29b-41d4-a716-446655440000\",\n    trace_id=\"123e4567-e89b-12d3-a456-426614174000\",\n    parent_span_id=None,  # Root span\n    name=\"agent.process_request\",\n    span_type=SpanType.AGENT,\n    status=SpanStatus.SUCCESS,\n    attributes={\n        \"user_id\": \"user123\",\n        \"request_type\": \"chat\"\n    }\n)\n</code></pre>"},{"location":"concepts/spans/#span-types","title":"Span Types","text":"<p>Prela provides semantic span types for different operations:</p>"},{"location":"concepts/spans/#spantypeagent","title":"SpanType.AGENT","text":"<p>Represents agent-level operations (reasoning, planning, orchestration).</p> <pre><code>from prela import get_tracer, SpanType\n\nwith get_tracer().span(\"process_query\", SpanType.AGENT) as span:\n    span.set_attribute(\"query\", \"What's the weather?\")\n    # Agent logic here\n</code></pre>"},{"location":"concepts/spans/#spantypellm","title":"SpanType.LLM","text":"<p>Represents LLM API calls (automatically created by instrumentation).</p> <pre><code># Auto-instrumented LLM calls create LLM spans\nfrom anthropic import Anthropic\n\nclient = Anthropic()\nresponse = client.messages.create(...)  # Creates SpanType.LLM\n</code></pre>"},{"location":"concepts/spans/#spantypetool","title":"SpanType.TOOL","text":"<p>Represents tool or function calls made by the agent.</p> <pre><code>with get_tracer().span(\"search_database\", SpanType.TOOL) as span:\n    span.set_attribute(\"tool.name\", \"database_search\")\n    span.set_attribute(\"tool.input\", query)\n    result = search(query)\n    span.set_attribute(\"tool.output\", result)\n</code></pre>"},{"location":"concepts/spans/#spantyperetrieval","title":"SpanType.RETRIEVAL","text":"<p>Represents retrieval operations (vector search, RAG, knowledge base queries).</p> <pre><code>with get_tracer().span(\"vector_search\", SpanType.RETRIEVAL) as span:\n    span.set_attribute(\"retrieval.query\", text)\n    span.set_attribute(\"retrieval.top_k\", 10)\n    docs = vector_db.search(text, k=10)\n    span.set_attribute(\"retrieval.num_results\", len(docs))\n</code></pre>"},{"location":"concepts/spans/#spantypeembedding","title":"SpanType.EMBEDDING","text":"<p>Represents embedding generation operations.</p> <pre><code>with get_tracer().span(\"generate_embedding\", SpanType.EMBEDDING) as span:\n    span.set_attribute(\"embedding.model\", \"text-embedding-ada-002\")\n    span.set_attribute(\"embedding.input\", text)\n    embedding = model.embed(text)\n    span.set_attribute(\"embedding.dimensions\", len(embedding))\n</code></pre>"},{"location":"concepts/spans/#spantypecustom","title":"SpanType.CUSTOM","text":"<p>Represents custom operations specific to your application.</p> <pre><code>with get_tracer().span(\"custom_operation\", SpanType.CUSTOM) as span:\n    span.set_attribute(\"operation.type\", \"data_processing\")\n    # Custom logic\n</code></pre>"},{"location":"concepts/spans/#span-lifecycle","title":"Span Lifecycle","text":"<p>Spans follow a strict lifecycle:</p> <pre><code>stateDiagram-v2\n    [*] --&gt; Created\n    Created --&gt; Running: start()\n    Running --&gt; Ended: end()\n    Ended --&gt; [*]\n\n    note right of Running\n        Can add attributes,\n        events, update status\n    end note\n\n    note right of Ended\n        Immutable - no changes\n        allowed after end()\n    end note</code></pre>"},{"location":"concepts/spans/#1-creation","title":"1. Creation","text":"<pre><code>from prela import Span, SpanType\nfrom datetime import datetime, timezone\n\nspan = Span(\n    name=\"my_operation\",\n    span_type=SpanType.CUSTOM,\n    started_at=datetime.now(timezone.utc)\n)\n</code></pre>"},{"location":"concepts/spans/#2-running-mutable","title":"2. Running (Mutable)","text":"<p>During execution, you can add attributes and events:</p> <pre><code># Add attributes\nspan.set_attribute(\"user_id\", \"user123\")\nspan.set_attribute(\"model\", \"claude-sonnet-4\")\n\n# Add events\nspan.add_event(\"checkpoint_reached\", {\"step\": 1})\nspan.add_event(\"cache_hit\", {\"key\": \"abc123\"})\n</code></pre>"},{"location":"concepts/spans/#3-completion-immutable","title":"3. Completion (Immutable)","text":"<p>Once ended, spans become immutable:</p> <pre><code>span.end()  # Status defaults to SUCCESS\n\n# After end(), modifications raise RuntimeError\ntry:\n    span.set_attribute(\"foo\", \"bar\")\nexcept RuntimeError as e:\n    print(e)  # \"Span has ended and is immutable\"\n</code></pre>"},{"location":"concepts/spans/#error-handling","title":"Error Handling","text":"<pre><code>try:\n    # Operation that might fail\n    result = risky_operation()\n    span.set_attribute(\"result\", result)\nexcept Exception as e:\n    span.set_status(SpanStatus.ERROR, str(e))\n    raise\nfinally:\n    span.end()\n</code></pre>"},{"location":"concepts/spans/#span-status","title":"Span Status","text":"<p>Spans have one of three statuses:</p> Status Description Use Case <code>PENDING</code> Operation in progress Default when span is created <code>SUCCESS</code> Operation completed successfully Default when <code>end()</code> is called <code>ERROR</code> Operation failed Set manually when exceptions occur <pre><code>from prela import SpanStatus\n\n# Manual status control\nspan.set_status(SpanStatus.SUCCESS)\nspan.set_status(SpanStatus.ERROR, \"Connection timeout after 30s\")\n\n# Check current status\nif span.status == SpanStatus.ERROR:\n    print(f\"Error: {span.status_message}\")\n</code></pre>"},{"location":"concepts/spans/#attributes","title":"Attributes","text":"<p>Attributes are key-value metadata attached to spans. They provide context about the operation.</p>"},{"location":"concepts/spans/#setting-attributes","title":"Setting Attributes","text":"<pre><code># Single attribute\nspan.set_attribute(\"user_id\", \"user123\")\n\n# Multiple attributes\nspan.set_attribute(\"model\", \"gpt-4\")\nspan.set_attribute(\"temperature\", 0.7)\nspan.set_attribute(\"max_tokens\", 1000)\n\n# Complex values (automatically serialized)\nspan.set_attribute(\"metadata\", {\"key\": \"value\"})\n</code></pre>"},{"location":"concepts/spans/#recommended-attribute-conventions","title":"Recommended Attribute Conventions","text":"<p>For LLM operations: <pre><code>span.set_attribute(\"llm.vendor\", \"anthropic\")\nspan.set_attribute(\"llm.model\", \"claude-sonnet-4-20250514\")\nspan.set_attribute(\"llm.temperature\", 1.0)\nspan.set_attribute(\"llm.input_tokens\", 100)\nspan.set_attribute(\"llm.output_tokens\", 50)\n</code></pre></p> <p>For tool operations: <pre><code>span.set_attribute(\"tool.name\", \"web_search\")\nspan.set_attribute(\"tool.input\", query)\nspan.set_attribute(\"tool.output\", results)\n</code></pre></p> <p>For retrieval operations: <pre><code>span.set_attribute(\"retrieval.query\", text)\nspan.set_attribute(\"retrieval.top_k\", 5)\nspan.set_attribute(\"retrieval.num_results\", 3)\n</code></pre></p>"},{"location":"concepts/spans/#events","title":"Events","text":"<p>Events are timestamped occurrences during span execution.</p> <pre><code>from prela import SpanEvent\nfrom datetime import datetime, timezone\n\n# Add event with automatic timestamp\nspan.add_event(\"cache_miss\", {\"cache_key\": \"abc123\"})\n\n# Add event with explicit timestamp\nspan.add_event(\n    \"retry_attempted\",\n    {\"attempt\": 2, \"delay_ms\": 1000}\n)\n\n# Access events\nfor event in span.events:\n    print(f\"{event.timestamp}: {event.name}\")\n    print(f\"  Attributes: {event.attributes}\")\n</code></pre> <p>Common event types: - <code>llm.request</code>: LLM request details - <code>llm.response</code>: LLM response details - <code>tool.call</code>: Tool invocation - <code>cache.hit</code> / <code>cache.miss</code>: Cache access - <code>retry.attempted</code>: Retry logic</p>"},{"location":"concepts/spans/#context-manager-usage","title":"Context Manager Usage","text":"<p>The recommended way to use spans is with context managers:</p> <pre><code>from prela import get_tracer, SpanType\n\n# Basic usage\nwith get_tracer().span(\"my_operation\") as span:\n    span.set_attribute(\"key\", \"value\")\n    # Operation here\n    # span.end() called automatically\n\n# With span type\nwith get_tracer().span(\"llm_call\", SpanType.LLM) as span:\n    # LLM operation\n    pass\n\n# Nested spans\nwith get_tracer().span(\"parent_op\", SpanType.AGENT) as parent:\n    parent.set_attribute(\"stage\", \"planning\")\n\n    with get_tracer().span(\"child_op\", SpanType.TOOL) as child:\n        child.set_attribute(\"tool\", \"calculator\")\n        # Child operation\n</code></pre>"},{"location":"concepts/spans/#serialization","title":"Serialization","text":"<p>Spans can be serialized to/from dictionaries for storage or transmission:</p> <pre><code># Serialize to dict\nspan_dict = span.to_dict()\n# {\n#     \"span_id\": \"...\",\n#     \"trace_id\": \"...\",\n#     \"name\": \"my_operation\",\n#     \"span_type\": \"custom\",\n#     \"status\": \"success\",\n#     ...\n# }\n\n# Deserialize from dict\nfrom prela import Span\nreconstructed = Span.from_dict(span_dict)\n</code></pre>"},{"location":"concepts/spans/#performance-considerations","title":"Performance Considerations","text":"<p>Prela spans are designed for high performance:</p> <ul> <li>Memory Efficient: Uses <code>__slots__</code> to reduce memory overhead</li> <li>Lazy Serialization: Only serializes when exported</li> <li>Immutability: Ended spans can be safely shared across threads</li> <li>Minimal Overhead: Typical span creation adds &lt;100 microseconds</li> </ul> <pre><code># Good: Minimal attributes for high-volume operations\nwith get_tracer().span(\"fast_op\") as span:\n    span.set_attribute(\"id\", request_id)\n\n# Avoid: Too many attributes in tight loops\n# for i in range(10000):\n#     with get_tracer().span(f\"item_{i}\") as span:\n#         span.set_attribute(\"index\", i)\n#         span.set_attribute(\"data\", large_dict)  # Expensive\n</code></pre>"},{"location":"concepts/spans/#best-practices","title":"Best Practices","text":""},{"location":"concepts/spans/#1-use-semantic-span-types","title":"1. Use Semantic Span Types","text":"<pre><code># Good\nwith get_tracer().span(\"query_db\", SpanType.RETRIEVAL) as span:\n    ...\n\n# Less useful\nwith get_tracer().span(\"query_db\", SpanType.CUSTOM) as span:\n    ...\n</code></pre>"},{"location":"concepts/spans/#2-add-relevant-attributes","title":"2. Add Relevant Attributes","text":"<pre><code># Good: Useful for debugging and analysis\nspan.set_attribute(\"user_id\", user_id)\nspan.set_attribute(\"model\", model_name)\nspan.set_attribute(\"latency_ms\", duration)\n\n# Bad: Too much detail\nspan.set_attribute(\"entire_request_body\", massive_json)\n</code></pre>"},{"location":"concepts/spans/#3-use-events-for-milestones","title":"3. Use Events for Milestones","text":"<pre><code>span.add_event(\"validation_started\")\n# Validation logic\nspan.add_event(\"validation_completed\", {\"errors\": 0})\n</code></pre>"},{"location":"concepts/spans/#4-always-use-context-managers","title":"4. Always Use Context Managers","text":"<pre><code># Good: Automatic cleanup\nwith get_tracer().span(\"operation\") as span:\n    do_work()\n\n# Bad: Manual lifecycle management\nspan = get_tracer().start_span(\"operation\")\ntry:\n    do_work()\nfinally:\n    span.end()  # Easy to forget!\n</code></pre>"},{"location":"concepts/spans/#5-set-error-status-on-exceptions","title":"5. Set Error Status on Exceptions","text":"<pre><code>with get_tracer().span(\"risky_op\") as span:\n    try:\n        risky_operation()\n    except Exception as e:\n        span.set_status(SpanStatus.ERROR, str(e))\n        raise\n</code></pre>"},{"location":"concepts/spans/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Context Propagation for distributed tracing</li> <li>Explore Sampling to control trace volume</li> <li>See Exporters for sending traces to backends</li> </ul>"},{"location":"concepts/tracing/","title":"Distributed Tracing","text":"<p>Understanding how Prela traces AI agent execution.</p>"},{"location":"concepts/tracing/#what-is-tracing","title":"What is Tracing?","text":"<p>Tracing tracks the flow of execution through your application. Each operation (LLM call, tool invocation, agent step) creates a span that records:</p> <ul> <li>When it started and ended</li> <li>What inputs and outputs it had</li> <li>Whether it succeeded or failed</li> <li>How long it took</li> </ul> <p>Spans are organized into traces - a tree of related operations representing a single request or workflow.</p>"},{"location":"concepts/tracing/#anatomy-of-a-trace","title":"Anatomy of a Trace","text":"<pre><code>graph TD\n    A[Root Span: Agent Execution] --&gt; B[Span: LLM Call #1]\n    A --&gt; C[Span: Tool: Search]\n    C --&gt; D[Span: LLM Call #2]\n    A --&gt; E[Span: Tool: Calculator]\n    E --&gt; F[Span: LLM Call #3]\n\n    style A fill:#4F46E5\n    style B fill:#6366F1\n    style C fill:#818CF8\n    style D fill:#A5B4FC\n    style E fill:#818CF8\n    style F fill:#A5B4FC</code></pre> <p>Each span has:</p> <ul> <li>Trace ID: Unique identifier for the entire trace</li> <li>Span ID: Unique identifier for this span</li> <li>Parent Span ID: Links to parent span (if not root)</li> <li>Name: Human-readable operation name</li> <li>Type: AGENT, LLM, TOOL, RETRIEVAL, EMBEDDING, CUSTOM</li> <li>Status: PENDING, SUCCESS, ERROR</li> <li>Timestamps: Start and end times</li> <li>Duration: Calculated from timestamps</li> <li>Attributes: Key-value metadata</li> <li>Events: Timestamped events during execution</li> </ul>"},{"location":"concepts/tracing/#how-prela-traces-work","title":"How Prela Traces Work","text":""},{"location":"concepts/tracing/#1-initialization","title":"1. Initialization","text":"<p>When you call <code>prela.init()</code>:</p> <pre><code>import prela\n\nprela.init(service_name=\"my-agent\")\n</code></pre> <p>Prela:</p> <ol> <li>Creates a global tracer</li> <li>Auto-instruments installed LLM SDKs (OpenAI, Anthropic, etc.)</li> <li>Sets up context propagation for threads and async</li> </ol>"},{"location":"concepts/tracing/#2-automatic-span-creation","title":"2. Automatic Span Creation","text":"<p>When you call an LLM API:</p> <pre><code>from anthropic import Anthropic\n\nclient = Anthropic()\nresponse = client.messages.create(\n    model=\"claude-sonnet-4-20250514\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello\"}]\n)\n</code></pre> <p>Behind the scenes:</p> <pre><code>sequenceDiagram\n    participant App\n    participant Instrumentation\n    participant Tracer\n    participant LLM API\n\n    App-&gt;&gt;Instrumentation: client.messages.create()\n    Note over Instrumentation: Intercepted by wrapper\n    Instrumentation-&gt;&gt;Tracer: Create span\n    Tracer-&gt;&gt;Tracer: Generate trace_id, span_id\n    Instrumentation-&gt;&gt;Instrumentation: Record request attributes\n    Instrumentation-&gt;&gt;LLM API: Forward request\n    LLM API--&gt;&gt;Instrumentation: Response\n    Instrumentation-&gt;&gt;Instrumentation: Record response attributes\n    Instrumentation-&gt;&gt;Tracer: End span\n    Tracer-&gt;&gt;Tracer: Calculate duration\n    Tracer-&gt;&gt;Tracer: Export (if root span)\n    Instrumentation--&gt;&gt;App: Return response</code></pre>"},{"location":"concepts/tracing/#3-context-propagation","title":"3. Context Propagation","text":"<p>Prela uses Python's <code>contextvars</code> to track the current span:</p> <pre><code>import prela\n\ntracer = prela.get_tracer()\n\n# Root span\nwith tracer.span(\"agent_execution\"):\n    # Child span (automatically linked)\n    with tracer.span(\"tool_call\"):\n        # Grandchild span (automatically linked)\n        llm_response = client.messages.create(...)\n</code></pre> <p>Resulting trace tree:</p> <pre><code>agent_execution\n\u251c\u2500\u2500 tool_call\n\u2502   \u2514\u2500\u2500 anthropic.messages.create\n</code></pre>"},{"location":"concepts/tracing/#4-thread-safety","title":"4. Thread Safety","text":"<p>Prela is safe for multi-threaded applications:</p> <pre><code>from concurrent.futures import ThreadPoolExecutor\n\ndef process_query(query):\n    # Each thread has its own trace context\n    with tracer.span(\"process_query\"):\n        response = client.messages.create(\n            model=\"claude-sonnet-4-20250514\",\n            messages=[{\"role\": \"user\", \"content\": query}]\n        )\n        return response\n\nwith ThreadPoolExecutor(max_workers=4) as executor:\n    results = executor.map(process_query, queries)\n</code></pre>"},{"location":"concepts/tracing/#5-async-support","title":"5. Async Support","text":"<p>Prela works with async/await:</p> <pre><code>import asyncio\nfrom anthropic import AsyncAnthropic\n\nasync def process_async(query):\n    client = AsyncAnthropic()\n\n    # Automatic tracing in async context\n    with tracer.span(\"async_process\"):\n        response = await client.messages.create(\n            model=\"claude-sonnet-4-20250514\",\n            messages=[{\"role\": \"user\", \"content\": query}]\n        )\n        return response\n\nawait asyncio.gather(*[process_async(q) for q in queries])\n</code></pre>"},{"location":"concepts/tracing/#span-attributes","title":"Span Attributes","text":"<p>Spans capture rich metadata:</p>"},{"location":"concepts/tracing/#service-attributes","title":"Service Attributes","text":"<ul> <li><code>service.name</code>: Your service name (from <code>prela.init()</code>)</li> </ul>"},{"location":"concepts/tracing/#llm-attributes","title":"LLM Attributes","text":"<ul> <li><code>llm.vendor</code>: Provider (openai, anthropic)</li> <li><code>llm.model</code>: Model name</li> <li><code>llm.temperature</code>: Temperature parameter</li> <li><code>llm.max_tokens</code>: Max tokens parameter</li> <li><code>llm.input_tokens</code>: Tokens in request</li> <li><code>llm.output_tokens</code>: Tokens in response</li> <li><code>llm.total_tokens</code>: Sum of input/output</li> <li><code>llm.latency_ms</code>: Response latency</li> <li><code>llm.finish_reason</code>: Completion reason</li> <li><code>llm.stop_reason</code>: Stop reason</li> </ul>"},{"location":"concepts/tracing/#tool-attributes","title":"Tool Attributes","text":"<ul> <li><code>tool.name</code>: Tool name</li> <li><code>tool.arguments</code>: Tool arguments (JSON)</li> </ul>"},{"location":"concepts/tracing/#error-attributes","title":"Error Attributes","text":"<ul> <li><code>error.type</code>: Exception class name</li> <li><code>error.message</code>: Exception message</li> <li><code>error.stacktrace</code>: Full stack trace</li> </ul>"},{"location":"concepts/tracing/#span-events","title":"Span Events","text":"<p>Events are timestamped logs within a span:</p> <pre><code>{\n  \"name\": \"llm.request\",\n  \"timestamp\": \"2025-01-26T10:00:00.123456Z\",\n  \"attributes\": {\n    \"messages\": [{\"role\": \"user\", \"content\": \"Hello\"}]\n  }\n}\n</code></pre> <p>Common events:</p> <ul> <li><code>llm.request</code>: LLM request sent</li> <li><code>llm.response</code>: LLM response received</li> <li><code>tool.call</code>: Tool invoked</li> <li><code>tool.result</code>: Tool returned result</li> </ul>"},{"location":"concepts/tracing/#best-practices","title":"Best Practices","text":""},{"location":"concepts/tracing/#1-meaningful-span-names","title":"1. Meaningful Span Names","text":"<p>Use descriptive, consistent names:</p> <pre><code># Good\nwith tracer.span(\"fetch_user_data\"):\n    ...\n\nwith tracer.span(\"generate_response\"):\n    ...\n\n# Bad\nwith tracer.span(\"function1\"):\n    ...\n\nwith tracer.span(\"do_stuff\"):\n    ...\n</code></pre>"},{"location":"concepts/tracing/#2-appropriate-span-granularity","title":"2. Appropriate Span Granularity","text":"<p>Don't create spans for trivial operations:</p> <pre><code># Good: One span for the whole process\nwith tracer.span(\"process_document\"):\n    text = extract_text(doc)\n    summary = summarize(text)\n    return summary\n\n# Bad: Too granular\nwith tracer.span(\"process_document\"):\n    with tracer.span(\"extract_text\"):\n        text = extract_text(doc)\n    with tracer.span(\"summarize\"):\n        summary = summarize(text)\n    return summary\n</code></pre>"},{"location":"concepts/tracing/#3-add-context-with-attributes","title":"3. Add Context with Attributes","text":"<p>Enrich spans with relevant metadata:</p> <pre><code>with tracer.span(\"query_database\") as span:\n    span.set_attribute(\"query\", sql_query)\n    span.set_attribute(\"database\", db_name)\n    result = execute_query(sql_query)\n    span.set_attribute(\"rows_returned\", len(result))\n</code></pre>"},{"location":"concepts/tracing/#4-record-events-for-milestones","title":"4. Record Events for Milestones","text":"<pre><code>with tracer.span(\"agent_workflow\") as span:\n    span.add_event(\"planning_started\")\n    plan = create_plan()\n\n    span.add_event(\"execution_started\")\n    result = execute_plan(plan)\n\n    span.add_event(\"validation_started\")\n    validate(result)\n</code></pre>"},{"location":"concepts/tracing/#5-handle-errors-properly","title":"5. Handle Errors Properly","text":"<p>Errors are automatically captured, but you can add context:</p> <pre><code>with tracer.span(\"risky_operation\") as span:\n    try:\n        result = dangerous_function()\n    except ValueError as e:\n        span.set_attribute(\"error.context\", \"Invalid input format\")\n        raise  # Re-raise to mark span as error\n</code></pre>"},{"location":"concepts/tracing/#trace-lifecycle","title":"Trace Lifecycle","text":"<pre><code>stateDiagram-v2\n    [*] --&gt; Created: tracer.span()\n    Created --&gt; Active: __enter__()\n    Active --&gt; Active: set_attribute()\n    Active --&gt; Active: add_event()\n    Active --&gt; Ended: __exit__()\n    Ended --&gt; Exported: export()\n    Exported --&gt; [*]\n\n    Active --&gt; Error: Exception\n    Error --&gt; Ended: __exit__()</code></pre> <ol> <li>Created: <code>tracer.span()</code> called</li> <li>Active: Context entered, span can be modified</li> <li>Ended: Context exited, span immutable</li> <li>Exported: Sent to exporter (only root spans)</li> </ol>"},{"location":"concepts/tracing/#performance-considerations","title":"Performance Considerations","text":""},{"location":"concepts/tracing/#overhead","title":"Overhead","text":"<p>Prela adds minimal overhead:</p> <ul> <li>Span creation: ~0.1ms</li> <li>Attribute setting: ~0.01ms per attribute</li> <li>Event recording: ~0.01ms per event</li> <li>Export: Happens asynchronously (root spans only)</li> </ul> <p>Total: ~0.5-2ms per span</p>"},{"location":"concepts/tracing/#sampling","title":"Sampling","text":"<p>Reduce overhead in high-traffic systems:</p> <pre><code>prela.init(\n    service_name=\"high-traffic-agent\",\n    sample_rate=0.01  # Sample 1% of traces\n)\n</code></pre> <p>See Sampling for details.</p>"},{"location":"concepts/tracing/#next-steps","title":"Next Steps","text":"<ul> <li>Spans - Deep dive into span internals</li> <li>Context Propagation - Thread and async context</li> <li>Sampling - Control trace volume</li> <li>Exporters - Where traces go</li> </ul>"},{"location":"demos/P2_1_3_DEMO_GUIDE/","title":"P2.1.3: One-Click Debug Flow - Demo Guide","text":"<p>Feature: AI-Powered Error Explanations Status: Phase 1 Complete \u2705 Demo Time: &lt; 2 minutes</p>"},{"location":"demos/P2_1_3_DEMO_GUIDE/#what-to-show","title":"What to Show","text":""},{"location":"demos/P2_1_3_DEMO_GUIDE/#1-error-detection-before","title":"1. Error Detection (Before)","text":"<pre><code>\u274c Agent 'researcher' failed\n\nError: OpenAI rate limit exceeded (429)\nSpan: openai.chat.completions.create\n</code></pre>"},{"location":"demos/P2_1_3_DEMO_GUIDE/#2-error-detection-after-with-ai","title":"2. Error Detection (After - with AI)","text":"<pre><code>\u274c Rate Limit Exceeded\n\nError: OpenAI rate limit exceeded (429)\n\n\u2728 AI Analysis                           \ud83d\udd50 &lt; 1 minute\n\nWhy this happened:\nYour application hit OpenAI's rate limit because you're \nsending too many requests in a short time period. This \ncommonly happens during testing or when scaling up.\n\nHow to fix it:\n1. Wait 30 seconds for the rate limit to reset\n2. Switch to gpt-4o-mini (83% cheaper with higher limits)\n3. Implement exponential backoff in your retry logic\n\n\u25bc Similar patterns (3)\n</code></pre>"},{"location":"demos/P2_1_3_DEMO_GUIDE/#demo-script","title":"Demo Script","text":""},{"location":"demos/P2_1_3_DEMO_GUIDE/#step-1-trigger-an-error-30-seconds","title":"Step 1: Trigger an Error (30 seconds)","text":"<pre><code># demo_error.py\nimport prela\nfrom openai import OpenAI\n\nprela.init(service_name=\"demo\", exporter=\"file\", file_path=\"traces.jsonl\")\n\nclient = OpenAI(api_key=\"invalid-key-for-demo\")  # \u2190 Will fail\n\ntry:\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": \"Hello\"}]\n    )\nexcept Exception as e:\n    print(f\"Error occurred: {e}\")\n\n# Trace saved to traces.jsonl with error\n</code></pre>"},{"location":"demos/P2_1_3_DEMO_GUIDE/#step-2-view-in-dashboard-10-seconds","title":"Step 2: View in Dashboard (10 seconds)","text":"<ol> <li>Open Prela dashboard</li> <li>Navigate to Traces page</li> <li>See red \"\u26a0\ufe0f\" indicator on failed trace</li> <li>Click to view details</li> </ol>"},{"location":"demos/P2_1_3_DEMO_GUIDE/#step-3-see-ai-explanation-5-seconds","title":"Step 3: See AI Explanation (5 seconds)","text":"<ol> <li>Scroll to error span</li> <li>AI explanation automatically loads</li> <li>See \"Why this happened\" section</li> <li>See \"How to fix it\" steps</li> <li>Expand \"Similar patterns\"</li> </ol>"},{"location":"demos/P2_1_3_DEMO_GUIDE/#step-4-one-click-fix-10-seconds","title":"Step 4: One-Click Fix (10 seconds)","text":"<ol> <li>Click \"Try This Fix\" button</li> <li>Replay starts with suggested model</li> <li>See side-by-side comparison</li> <li>Success! \u2705</li> </ol> <p>Total Demo Time: 55 seconds (under 1 minute!)</p>"},{"location":"demos/P2_1_3_DEMO_GUIDE/#key-messages","title":"Key Messages","text":""},{"location":"demos/P2_1_3_DEMO_GUIDE/#for-developers","title":"For Developers:","text":"<ul> <li>\"No more guessing why your agent failed\"</li> <li>\"AI tells you exactly what to do in plain English\"</li> <li>\"Fix errors in under 30 seconds\"</li> </ul>"},{"location":"demos/P2_1_3_DEMO_GUIDE/#for-product-managers","title":"For Product Managers:","text":"<ul> <li>\"Reduce debugging time by 80%\"</li> <li>\"Junior developers can fix errors without senior help\"</li> <li>\"Clear ROI: Less time debugging = more time shipping\"</li> </ul>"},{"location":"demos/P2_1_3_DEMO_GUIDE/#for-decision-makers","title":"For Decision Makers:","text":"<ul> <li>\"Langfuse shows you WHAT failed\"</li> <li>\"Prela shows you WHY and HOW TO FIX\"</li> <li>\"First observability platform with AI debugging\"</li> </ul>"},{"location":"demos/P2_1_3_DEMO_GUIDE/#competitive-comparison","title":"Competitive Comparison","text":""},{"location":"demos/P2_1_3_DEMO_GUIDE/#langfuse","title":"Langfuse:","text":"<pre><code>\u274c Span failed with status: error\nError: RateLimitError\nStatus Code: 429\n\n[End of information]\n</code></pre> <p>User Action: Search Google, read docs, ask ChatGPT, trial-and-error</p> <p>Time to Fix: 10-30 minutes</p>"},{"location":"demos/P2_1_3_DEMO_GUIDE/#prela","title":"Prela:","text":"<pre><code>\u274c Rate Limit Exceeded\n\n\u2728 AI Analysis: Your application hit OpenAI's rate limit...\n\nHow to fix it:\n1. Wait 30 seconds\n2. Switch to gpt-4o-mini\n3. Implement exponential backoff\n\n[Try This Fix] \u2190 One click\n</code></pre> <p>User Action: Click button</p> <p>Time to Fix: &lt; 30 seconds \u2705</p>"},{"location":"demos/P2_1_3_DEMO_GUIDE/#technical-highlights","title":"Technical Highlights","text":""},{"location":"demos/P2_1_3_DEMO_GUIDE/#backend","title":"Backend:","text":"<ul> <li>GPT-4o-mini for natural language explanations</li> <li>Pattern matching for error categories</li> <li>Confidence scores for recommendations</li> <li>Cost: ~$0.0002 per explanation</li> </ul>"},{"location":"demos/P2_1_3_DEMO_GUIDE/#frontend","title":"Frontend:","text":"<ul> <li>Automatic explanation loading</li> <li>Beautiful loading skeleton</li> <li>Collapsible sections</li> <li>Mobile-responsive design</li> </ul>"},{"location":"demos/P2_1_3_DEMO_GUIDE/#performance","title":"Performance:","text":"<ul> <li>API response: 600-1600ms</li> <li>Frontend rendering: &lt; 50ms</li> <li>Total user wait: &lt; 2 seconds</li> <li>Goal: &lt; 30 seconds to fix \u2705</li> </ul>"},{"location":"demos/P2_1_3_DEMO_GUIDE/#demo-environment-setup","title":"Demo Environment Setup","text":""},{"location":"demos/P2_1_3_DEMO_GUIDE/#prerequisites","title":"Prerequisites:","text":"<pre><code>export OPENAI_API_KEY=sk-...  # For explanations\nexport CLICKHOUSE_URL=...\nexport REDIS_URL=...\n</code></pre>"},{"location":"demos/P2_1_3_DEMO_GUIDE/#run-backend","title":"Run Backend:","text":"<pre><code>cd backend/services/api-gateway\nuvicorn app.main:app --reload --port 8000\n</code></pre>"},{"location":"demos/P2_1_3_DEMO_GUIDE/#run-frontend","title":"Run Frontend:","text":"<pre><code>cd frontend\nnpm run dev\n</code></pre>"},{"location":"demos/P2_1_3_DEMO_GUIDE/#generate-test-errors","title":"Generate Test Errors:","text":"<pre><code>cd sdk/examples\npython demo_errors.py  # Creates various error types\n</code></pre>"},{"location":"demos/P2_1_3_DEMO_GUIDE/#screenshots-to-take","title":"Screenshots to Take","text":""},{"location":"demos/P2_1_3_DEMO_GUIDE/#1-error-list-view","title":"1. Error List View","text":"<ul> <li>Show trace list with red \"\u26a0\ufe0f\" indicators</li> <li>Highlight failed traces stand out visually</li> </ul>"},{"location":"demos/P2_1_3_DEMO_GUIDE/#2-error-detail-before-ai","title":"2. Error Detail - Before AI","text":"<ul> <li>Show basic error message</li> <li>Highlight lack of context</li> </ul>"},{"location":"demos/P2_1_3_DEMO_GUIDE/#3-error-detail-with-ai","title":"3. Error Detail - With AI","text":"<ul> <li>Show AI explanation section</li> <li>Highlight \"Why\" and \"What to do\" sections</li> <li>Show estimated fix time badge</li> </ul>"},{"location":"demos/P2_1_3_DEMO_GUIDE/#4-loading-state","title":"4. Loading State","text":"<ul> <li>Show pulse animation</li> <li>Demonstrate smooth UX</li> </ul>"},{"location":"demos/P2_1_3_DEMO_GUIDE/#5-similar-patterns","title":"5. Similar Patterns","text":"<ul> <li>Show collapsible section</li> <li>Demonstrate additional context</li> </ul>"},{"location":"demos/P2_1_3_DEMO_GUIDE/#6-side-by-side-comparison-phase-2","title":"6. Side-by-Side Comparison (Phase 2)","text":"<ul> <li>Show original vs fixed</li> <li>Highlight differences</li> <li>Show cost savings</li> </ul>"},{"location":"demos/P2_1_3_DEMO_GUIDE/#talking-points","title":"Talking Points","text":""},{"location":"demos/P2_1_3_DEMO_GUIDE/#problem","title":"Problem:","text":"<p>\"Debugging agent failures is frustrating. You see an error, but don't know why it happened or how to fix it. You waste hours searching docs, asking ChatGPT, and trying different things.\"</p>"},{"location":"demos/P2_1_3_DEMO_GUIDE/#solution","title":"Solution:","text":"<p>\"Prela uses AI to explain errors in plain English. Within seconds of seeing an error, you know exactly why it happened and what to do about it.\"</p>"},{"location":"demos/P2_1_3_DEMO_GUIDE/#demo","title":"Demo:","text":"<p>\"Watch this: I trigger an error [run demo], navigate to the trace [click], and instantly see an AI-generated explanation telling me why this happened and how to fix it. One click, and it's resolved.\"</p>"},{"location":"demos/P2_1_3_DEMO_GUIDE/#differentiation","title":"Differentiation:","text":"<p>\"Langfuse shows you what failed. Prela shows you why and how to fix it. We're not just observability - we're intelligent debugging.\"</p>"},{"location":"demos/P2_1_3_DEMO_GUIDE/#customer-quotes-future","title":"Customer Quotes (Future)","text":"<p>\"Before Prela, debugging agent failures took 20-30 minutes. Now it takes 30 seconds. This is a game-changer.\" - Engineering Lead, AI Startup</p> <p>\"The AI explanations are incredibly accurate. It's like having a senior engineer review every error.\" - Staff Engineer, Fortune 500</p> <p>\"We reduced our mean time to resolution by 80%. Prela paid for itself in the first week.\" - VP Engineering, SaaS Company</p>"},{"location":"demos/P2_1_3_DEMO_GUIDE/#call-to-action","title":"Call to Action","text":""},{"location":"demos/P2_1_3_DEMO_GUIDE/#for-demo","title":"For Demo:","text":"<p>\"Want to try it yourself? Sign up for free at prela.app - no credit card required.\"</p>"},{"location":"demos/P2_1_3_DEMO_GUIDE/#for-launch","title":"For Launch:","text":"<p>\"We're launching Prela 2.0 with AI debugging. Join the waitlist for early access.\"</p>"},{"location":"demos/P2_1_3_DEMO_GUIDE/#for-sales","title":"For Sales:","text":"<p>\"Schedule a 15-minute demo to see how Prela can reduce your debugging time by 80%.\"</p>"},{"location":"demos/P2_1_3_DEMO_GUIDE/#next-demo-phase-2","title":"Next Demo (Phase 2)","text":""},{"location":"demos/P2_1_3_DEMO_GUIDE/#one-click-replay-with-comparison","title":"One-Click Replay with Comparison:","text":"<ol> <li>See error + AI explanation</li> <li>Click \"Debug with Replay\"</li> <li>Pre-filled fix suggestions</li> <li>Click \"Run Comparison\"</li> <li>Side-by-side results</li> <li>Copy code snippet</li> <li>Total time: &lt; 30 seconds</li> </ol> <p>Coming Soon: Phase 2 implementation (ReplayDebugModal + ComparisonView)</p>"},{"location":"evals/assertions/","title":"Assertions","text":"<p>Assertions validate agent outputs against expected behavior. Prela provides 10 built-in assertion types across three categories.</p>"},{"location":"evals/assertions/#structural-assertions","title":"Structural Assertions","text":""},{"location":"evals/assertions/#containsassertion","title":"ContainsAssertion","text":"<p>Checks if output contains specific text.</p> <pre><code>from prela.evals.assertions import ContainsAssertion\n\nassertion = ContainsAssertion(text=\"success\", case_sensitive=False)\nresult = assertion.evaluate(output=\"Operation completed successfully!\")\nprint(result.passed)  # True\n</code></pre> <p>Configuration: <pre><code>{\n    \"type\": \"contains\",\n    \"value\": \"expected text\",\n    \"case_sensitive\": False  # Optional, default: False\n}\n</code></pre></p>"},{"location":"evals/assertions/#notcontainsassertion","title":"NotContainsAssertion","text":"<p>Checks if output does NOT contain specific text.</p> <pre><code>from prela.evals.assertions import NotContainsAssertion\n\nassertion = NotContainsAssertion(text=\"error\", case_sensitive=True)\nresult = assertion.evaluate(output=\"All tests passed!\")\nprint(result.passed)  # True\n</code></pre> <p>Configuration: <pre><code>{\n    \"type\": \"not_contains\",\n    \"value\": \"forbidden text\",\n    \"case_sensitive\": True  # Optional\n}\n</code></pre></p>"},{"location":"evals/assertions/#regexassertion","title":"RegexAssertion","text":"<p>Matches output against a regex pattern.</p> <pre><code>from prela.evals.assertions import RegexAssertion\n\nassertion = RegexAssertion(pattern=r\"\\\\d{3}-\\\\d{3}-\\\\d{4}\")\nresult = assertion.evaluate(output=\"Call me at 555-123-4567\")\nprint(result.passed)  # True\n</code></pre> <p>Configuration: <pre><code>{\n    \"type\": \"regex\",\n    \"pattern\": r\"\\\\d{3}-\\\\d{3}-\\\\d{4}\"\n}\n</code></pre></p>"},{"location":"evals/assertions/#lengthassertion","title":"LengthAssertion","text":"<p>Validates output length is within bounds.</p> <pre><code>from prela.evals.assertions import LengthAssertion\n\nassertion = LengthAssertion(min_length=10, max_length=100)\nresult = assertion.evaluate(output=\"This is a medium length response.\")\nprint(result.passed)  # True\n</code></pre> <p>Configuration: <pre><code>{\n    \"type\": \"length\",\n    \"min\": 10,    # Optional\n    \"max\": 100    # Optional\n}\n</code></pre></p>"},{"location":"evals/assertions/#jsonvalidassertion","title":"JSONValidAssertion","text":"<p>Validates output is valid JSON.</p> <pre><code>from prela.evals.assertions import JSONValidAssertion\n\nassertion = JSONValidAssertion()\nresult = assertion.evaluate(output='{\"key\": \"value\"}')\nprint(result.passed)  # True\n</code></pre> <p>Configuration: <pre><code>{\"type\": \"json_valid\"}\n</code></pre></p>"},{"location":"evals/assertions/#latencyassertion","title":"LatencyAssertion","text":"<p>Validates response time is under threshold.</p> <pre><code>from prela.evals.assertions import LatencyAssertion\nfrom prela.core.span import Span, SpanType\nfrom datetime import datetime, timezone, timedelta\n\nassertion = LatencyAssertion(max_ms=5000)\n\n# Create span with timing\nspan = Span(\n    name=\"test\",\n    span_type=SpanType.LLM,\n    started_at=datetime.now(timezone.utc),\n    ended_at=datetime.now(timezone.utc) + timedelta(milliseconds=1234)\n)\n\nresult = assertion.evaluate(output=\"\", trace=[span])\nprint(result.passed)  # True (1234ms &lt; 5000ms)\n</code></pre> <p>Configuration: <pre><code>{\n    \"type\": \"latency\",\n    \"max_ms\": 5000\n}\n</code></pre></p>"},{"location":"evals/assertions/#tool-assertions","title":"Tool Assertions","text":""},{"location":"evals/assertions/#toolcalledassertion","title":"ToolCalledAssertion","text":"<p>Validates that a specific tool was called.</p> <pre><code>from prela.evals.assertions import ToolCalledAssertion\nfrom prela.core.span import Span, SpanType\n\nassertion = ToolCalledAssertion(tool_name=\"search\")\n\n# Create span with tool call\nspan = Span(name=\"tool.search\", span_type=SpanType.TOOL)\nspan.set_attribute(\"tool.name\", \"search\")\n\nresult = assertion.evaluate(output=\"\", trace=[span])\nprint(result.passed)  # True\n</code></pre> <p>Configuration: <pre><code>{\n    \"type\": \"tool_called\",\n    \"tool_name\": \"search\"\n}\n</code></pre></p>"},{"location":"evals/assertions/#toolargsassertion","title":"ToolArgsAssertion","text":"<p>Validates tool was called with correct arguments.</p> <pre><code>from prela.evals.assertions import ToolArgsAssertion\n\nassertion = ToolArgsAssertion(\n    tool_name=\"calculator\",\n    args={\"x\": 5, \"y\": 3}\n)\n\n# Create span with tool args\nspan = Span(name=\"tool.calculator\", span_type=SpanType.TOOL)\nspan.set_attribute(\"tool.name\", \"calculator\")\nspan.set_attribute(\"tool.input\", {\"x\": 5, \"y\": 3})\n\nresult = assertion.evaluate(output=\"\", trace=[span])\nprint(result.passed)  # True\n</code></pre> <p>Configuration: <pre><code>{\n    \"type\": \"tool_args\",\n    \"tool_name\": \"calculator\",\n    \"args\": {\"x\": 5, \"y\": 3}\n}\n</code></pre></p>"},{"location":"evals/assertions/#toolsequenceassertion","title":"ToolSequenceAssertion","text":"<p>Validates tools were called in specific order.</p> <pre><code>from prela.evals.assertions import ToolSequenceAssertion\n\nassertion = ToolSequenceAssertion(sequence=[\"search\", \"summarize\", \"format\"])\n\n# Create spans for each tool\nspans = [\n    Span(name=\"tool.search\", span_type=SpanType.TOOL),\n    Span(name=\"tool.summarize\", span_type=SpanType.TOOL),\n    Span(name=\"tool.format\", span_type=SpanType.TOOL)\n]\n\nfor span, tool in zip(spans, [\"search\", \"summarize\", \"format\"]):\n    span.set_attribute(\"tool.name\", tool)\n\nresult = assertion.evaluate(output=\"\", trace=spans)\nprint(result.passed)  # True\n</code></pre> <p>Configuration: <pre><code>{\n    \"type\": \"tool_sequence\",\n    \"sequence\": [\"search\", \"summarize\", \"format\"]\n}\n</code></pre></p>"},{"location":"evals/assertions/#semantic-assertions","title":"Semantic Assertions","text":""},{"location":"evals/assertions/#semanticsimilarityassertion","title":"SemanticSimilarityAssertion","text":"<p>Validates semantic similarity to reference text.</p> <p>Requirements: <pre><code>pip install sentence-transformers\n</code></pre></p> <pre><code>from prela.evals.assertions import SemanticSimilarityAssertion\n\nassertion = SemanticSimilarityAssertion(\n    reference=\"The capital of France is Paris\",\n    threshold=0.8\n)\n\nresult = assertion.evaluate(output=\"Paris is the capital city of France\")\nprint(result.passed)  # True (similarity &gt; 0.8)\n</code></pre> <p>Configuration: <pre><code>{\n    \"type\": \"semantic_similarity\",\n    \"reference\": \"Expected meaning\",\n    \"threshold\": 0.8,  # 0.0 to 1.0\n    \"model\": \"all-MiniLM-L6-v2\"  # Optional\n}\n</code></pre></p>"},{"location":"evals/assertions/#using-assertions","title":"Using Assertions","text":""},{"location":"evals/assertions/#in-test-cases","title":"In Test Cases","text":"<pre><code>from prela.evals import EvalCase, EvalInput\n\ncase = EvalCase(\n    id=\"test_1\",\n    input=EvalInput(query=\"What is 2+2?\"),\n    assertions=[\n        {\"type\": \"contains\", \"value\": \"4\"},\n        {\"type\": \"latency\", \"max_ms\": 3000},\n        {\"type\": \"length\", \"min\": 5, \"max\": 100}\n    ]\n)\n</code></pre>"},{"location":"evals/assertions/#programmatically","title":"Programmatically","text":"<pre><code>from prela.evals.assertions import ContainsAssertion, LengthAssertion\n\nassertions = [\n    ContainsAssertion(text=\"success\"),\n    LengthAssertion(min_length=10, max_length=500)\n]\n\nfor assertion in assertions:\n    result = assertion.evaluate(output=agent_output)\n    if not result.passed:\n        print(f\"Failed: {result.message}\")\n</code></pre>"},{"location":"evals/assertions/#with-create_assertion-factory","title":"With create_assertion Factory","text":"<pre><code>from prela.evals.runner import create_assertion\n\n# Create from config\nconfig = {\"type\": \"contains\", \"value\": \"hello\"}\nassertion = create_assertion(config)\n\nresult = assertion.evaluate(output=\"Hello, world!\")\n</code></pre>"},{"location":"evals/assertions/#best-practices","title":"Best Practices","text":""},{"location":"evals/assertions/#1-combine-multiple-assertions","title":"1. Combine Multiple Assertions","text":"<pre><code>assertions = [\n    {\"type\": \"contains\", \"value\": \"result\"},\n    {\"type\": \"not_contains\", \"value\": \"error\"},\n    {\"type\": \"json_valid\"},\n    {\"type\": \"latency\", \"max_ms\": 5000}\n]\n</code></pre>"},{"location":"evals/assertions/#2-use-semantic-similarity-for-fuzzy-matching","title":"2. Use Semantic Similarity for Fuzzy Matching","text":"<pre><code># Instead of exact match\n{\"type\": \"contains\", \"value\": \"Paris is the capital of France\"}\n\n# Use semantic similarity\n{\n    \"type\": \"semantic_similarity\",\n    \"threshold\": 0.85,\n    \"reference\": \"Paris is the capital of France\"\n}\n</code></pre>"},{"location":"evals/assertions/#3-validate-tool-usage","title":"3. Validate Tool Usage","text":"<pre><code># Ensure tool was called\n{\"type\": \"tool_called\", \"tool_name\": \"search\"}\n\n# Ensure correct arguments\n{\"type\": \"tool_args\", \"tool_name\": \"search\", \"args\": {\"query\": \"expected\"}}\n\n# Ensure correct sequence\n{\"type\": \"tool_sequence\", \"sequence\": [\"retrieve\", \"process\", \"respond\"]}\n</code></pre>"},{"location":"evals/assertions/#4-set-realistic-latency-thresholds","title":"4. Set Realistic Latency Thresholds","text":"<pre><code># Fast operations\n{\"type\": \"latency\", \"max_ms\": 1000}\n\n# LLM calls\n{\"type\": \"latency\", \"max_ms\": 10000}\n\n# Complex workflows\n{\"type\": \"latency\", \"max_ms\": 30000}\n</code></pre>"},{"location":"evals/assertions/#multi-agent-assertions","title":"Multi-Agent Assertions","text":"<p>Specialized assertions for testing multi-agent systems (CrewAI, AutoGen, LangGraph, Swarm):</p>"},{"location":"evals/assertions/#agentusedassertion","title":"AgentUsedAssertion","text":"<p>Verify that a specific agent was invoked during execution:</p> <pre><code>from prela.evals.assertions import AgentUsedAssertion\n\n# Verify agent participated\nAgentUsedAssertion(agent_name=\"Researcher\", min_invocations=1)\n</code></pre> <p>Use Cases: - Verify agent participation in multi-agent workflows - Ensure critical agents are used - Test agent selection logic</p>"},{"location":"evals/assertions/#taskcompletedassertion","title":"TaskCompletedAssertion","text":"<p>Verify that a task completed successfully (CrewAI):</p> <pre><code>from prela.evals.assertions import TaskCompletedAssertion\n\n# Verify task completion\nTaskCompletedAssertion(task_description=\"Research AI trends\")\n</code></pre> <p>Use Cases: - Verify task completion in CrewAI crews - Ensure all workflow steps execute - Test task orchestration</p>"},{"location":"evals/assertions/#delegationoccurredassertion","title":"DelegationOccurredAssertion","text":"<p>Verify agent-to-agent delegation (CrewAI):</p> <pre><code>from prela.evals.assertions import DelegationOccurredAssertion\n\n# Verify specific delegation\nDelegationOccurredAssertion(from_agent=\"Manager\", to_agent=\"Worker\")\n\n# Verify any delegation to agent\nDelegationOccurredAssertion(to_agent=\"Worker\")\n</code></pre> <p>Use Cases: - Test hierarchical crew processes - Verify delegation logic - Ensure proper task routing</p>"},{"location":"evals/assertions/#handoffoccurredassertion","title":"HandoffOccurredAssertion","text":"<p>Verify agent handoffs (Swarm):</p> <pre><code>from prela.evals.assertions import HandoffOccurredAssertion\n\n# Verify specific handoff\nHandoffOccurredAssertion(from_agent=\"Triage\", to_agent=\"Billing\")\n\n# Verify any handoff from agent\nHandoffOccurredAssertion(from_agent=\"Triage\")\n</code></pre> <p>Use Cases: - Test Swarm routing logic - Verify specialist assignment - Ensure handoff triggers work</p>"},{"location":"evals/assertions/#agentcollaborationassertion","title":"AgentCollaborationAssertion","text":"<p>Verify minimum number of agents participated:</p> <pre><code>from prela.evals.assertions import AgentCollaborationAssertion\n\n# Require at least 3 agents\nAgentCollaborationAssertion(min_agents=3)\n</code></pre> <p>Use Cases: - Ensure multi-agent collaboration - Verify sufficient agent participation - Test collaborative workflows</p>"},{"location":"evals/assertions/#conversationturnsassertion","title":"ConversationTurnsAssertion","text":"<p>Verify conversation length (AutoGen):</p> <pre><code>from prela.evals.assertions import ConversationTurnsAssertion\n\n# Verify turn count range\nConversationTurnsAssertion(min_turns=2, max_turns=10)\n</code></pre> <p>Use Cases: - Test conversation flow - Verify termination conditions - Ensure efficient dialogues</p>"},{"location":"evals/assertions/#nocirculardelegationassertion","title":"NoCircularDelegationAssertion","text":"<p>Detect circular delegation loops:</p> <pre><code>from prela.evals.assertions import NoCircularDelegationAssertion\n\n# Verify no circular delegation\nNoCircularDelegationAssertion()\n</code></pre> <p>Use Cases: - Prevent infinite delegation loops - Verify workflow correctness - Ensure proper delegation graphs</p>"},{"location":"evals/assertions/#example-multi-agent-test","title":"Example: Multi-Agent Test","text":"<pre><code>from prela.evals import EvalCase, EvalSuite, EvalRunner\nfrom prela.evals.assertions import (\n    AgentUsedAssertion,\n    AgentCollaborationAssertion,\n    DelegationOccurredAssertion,\n    NoCircularDelegationAssertion\n)\n\n# Test multi-agent workflow\ntest_case = EvalCase(\n    id=\"test_research_crew\",\n    name=\"Research crew with delegation\",\n    input={\"topic\": \"AI agents\"},\n    assertions=[\n        # Verify all agents used\n        AgentUsedAssertion(agent_name=\"Manager\", min_invocations=1),\n        AgentUsedAssertion(agent_name=\"Researcher\", min_invocations=1),\n        AgentUsedAssertion(agent_name=\"Writer\", min_invocations=1),\n\n        # Verify collaboration\n        AgentCollaborationAssertion(min_agents=3),\n\n        # Verify delegation flow\n        DelegationOccurredAssertion(from_agent=\"Manager\", to_agent=\"Researcher\"),\n        DelegationOccurredAssertion(from_agent=\"Manager\", to_agent=\"Writer\"),\n\n        # Verify no circular delegation\n        NoCircularDelegationAssertion()\n    ]\n)\n\nsuite = EvalSuite(name=\"Multi-Agent Tests\", cases=[test_case])\nrunner = EvalRunner(suite, my_crew_function)\nresult = runner.run()\n</code></pre> <p>For framework-specific examples: - CrewAI Integration - AutoGen Integration - LangGraph Integration - Swarm Integration</p>"},{"location":"evals/assertions/#next-steps","title":"Next Steps","text":"<ul> <li>See Writing Tests for test case creation</li> <li>Learn Running Evaluations</li> <li>Explore CI Integration</li> </ul>"},{"location":"evals/ci-integration/","title":"CI/CD Integration","text":"<p>Integrate Prela evaluations into your continuous integration pipeline for automated testing.</p>"},{"location":"evals/ci-integration/#github-actions","title":"GitHub Actions","text":"<pre><code>name: Run Evaluations\n\non: [push, pull_request]\n\njobs:\n  evaluate:\n    runs-on: ubuntu-latest\n\n    steps:\n      - uses: actions/checkout@v3\n\n      - uses: actions/setup-python@v4\n        with:\n          python-version: '3.10'\n\n      - name: Install dependencies\n        run: |\n          pip install prela anthropic openai\n          pip install -r requirements.txt\n\n      - name: Run evaluations\n        env:\n          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}\n          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}\n        run: |\n          prela eval run tests/eval_suite.yaml \\\n            --agent src/agent.py \\\n            --format junit \\\n            --output results/junit.xml\n\n      - name: Publish test results\n        uses: EnricoMi/publish-unit-test-result-action@v2\n        if: always()\n        with:\n          files: results/junit.xml\n\n      - name: Upload results\n        uses: actions/upload-artifact@v3\n        if: always()\n        with:\n          name: eval-results\n          path: results/\n</code></pre>"},{"location":"evals/ci-integration/#gitlab-ci","title":"GitLab CI","text":"<pre><code>test:\n  image: python:3.10\n  stage: test\n\n  before_script:\n    - pip install prela anthropic openai\n\n  script:\n    - prela eval run tests/eval_suite.yaml --agent src/agent.py --format junit --output junit.xml\n\n  artifacts:\n    when: always\n    reports:\n      junit: junit.xml\n    paths:\n      - junit.xml\n\n  variables:\n    ANTHROPIC_API_KEY: $ANTHROPIC_API_KEY\n    OPENAI_API_KEY: $OPENAI_API_KEY\n</code></pre>"},{"location":"evals/ci-integration/#circleci","title":"CircleCI","text":"<pre><code>version: 2.1\n\njobs:\n  evaluate:\n    docker:\n      - image: cimg/python:3.10\n\n    steps:\n      - checkout\n\n      - run:\n          name: Install dependencies\n          command: pip install prela anthropic openai\n\n      - run:\n          name: Run evaluations\n          command: |\n            prela eval run tests/eval_suite.yaml \\\n              --agent src/agent.py \\\n              --format junit \\\n              --output results/junit.xml\n\n      - store_test_results:\n          path: results/\n\n      - store_artifacts:\n          path: results/\n\nworkflows:\n  test:\n    jobs:\n      - evaluate\n</code></pre>"},{"location":"evals/ci-integration/#jenkins","title":"Jenkins","text":"<pre><code>pipeline {\n    agent any\n\n    stages {\n        stage('Setup') {\n            steps {\n                sh 'pip install prela anthropic openai'\n            }\n        }\n\n        stage('Evaluate') {\n            steps {\n                sh '''\n                    prela eval run tests/eval_suite.yaml \\\n                      --agent src/agent.py \\\n                      --format junit \\\n                      --output results/junit.xml\n                '''\n            }\n        }\n    }\n\n    post {\n        always {\n            junit 'results/junit.xml'\n            archiveArtifacts artifacts: 'results/**', allowEmptyArchive: true\n        }\n    }\n}\n</code></pre>"},{"location":"evals/ci-integration/#azure-pipelines","title":"Azure Pipelines","text":"<pre><code>trigger:\n  - main\n\npool:\n  vmImage: 'ubuntu-latest'\n\nsteps:\n  - task: UsePythonVersion@0\n    inputs:\n      versionSpec: '3.10'\n\n  - script: |\n      pip install prela anthropic openai\n    displayName: 'Install dependencies'\n\n  - script: |\n      prela eval run tests/eval_suite.yaml \\\n        --agent src/agent.py \\\n        --format junit \\\n        --output $(Build.ArtifactStagingDirectory)/junit.xml\n    displayName: 'Run evaluations'\n    env:\n      ANTHROPIC_API_KEY: $(ANTHROPIC_API_KEY)\n      OPENAI_API_KEY: $(OPENAI_API_KEY)\n\n  - task: PublishTestResults@2\n    inputs:\n      testResultsFiles: '$(Build.ArtifactStagingDirectory)/junit.xml'\n      failTaskOnFailedTests: true\n\n  - task: PublishBuildArtifacts@1\n    inputs:\n      pathToPublish: '$(Build.ArtifactStagingDirectory)'\n      artifactName: 'eval-results'\n</code></pre>"},{"location":"evals/ci-integration/#best-practices","title":"Best Practices","text":""},{"location":"evals/ci-integration/#1-use-secrets-for-api-keys","title":"1. Use Secrets for API Keys","text":"<pre><code># GitHub Actions\nenv:\n  ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}\n\n# GitLab CI\nvariables:\n  ANTHROPIC_API_KEY: $ANTHROPIC_API_KEY\n\n# CircleCI (in Project Settings)\n</code></pre>"},{"location":"evals/ci-integration/#2-fail-ci-on-test-failures","title":"2. Fail CI on Test Failures","text":"<pre><code># In agent code or CI script\nresult = runner.run()\nif result.pass_rate &lt; 1.0:\n    sys.exit(1)  # Non-zero exit code fails CI\n</code></pre>"},{"location":"evals/ci-integration/#3-store-results-as-artifacts","title":"3. Store Results as Artifacts","text":"<pre><code># GitHub Actions\n- uses: actions/upload-artifact@v3\n  with:\n    name: eval-results\n    path: results/\n</code></pre>"},{"location":"evals/ci-integration/#4-run-on-pull-requests","title":"4. Run on Pull Requests","text":"<pre><code>on:\n  pull_request:\n    branches: [main, develop]\n</code></pre>"},{"location":"evals/ci-integration/#5-cache-dependencies","title":"5. Cache Dependencies","text":"<pre><code>- uses: actions/cache@v3\n  with:\n    path: ~/.cache/pip\n    key: ${{ runner.os }}-pip-${{ hashFiles('requirements.txt') }}\n</code></pre>"},{"location":"evals/ci-integration/#example-python-script","title":"Example Python Script","text":"<pre><code># run_evals.py\nimport sys\nfrom prela.evals import EvalSuite, EvalRunner\nfrom prela.evals.reporters import JUnitReporter, ConsoleReporter\n\ndef my_agent(input_data):\n    # Your agent implementation\n    pass\n\ndef main():\n    # Load suite\n    suite = EvalSuite.from_yaml(\"tests/eval_suite.yaml\")\n\n    # Run evaluations\n    runner = EvalRunner(suite, my_agent, parallel=True)\n    result = runner.run()\n\n    # Report results\n    ConsoleReporter(verbosity=\"verbose\").report(result)\n    JUnitReporter(\"results/junit.xml\").report(result)\n\n    # Exit with appropriate code\n    if result.pass_rate &lt; 1.0:\n        print(f\"\u274c {result.failed} test(s) failed\")\n        sys.exit(1)\n    else:\n        print(f\"\u2705 All {result.total} tests passed\")\n        sys.exit(0)\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Use in CI:</p> <pre><code>python run_evals.py\n</code></pre>"},{"location":"evals/ci-integration/#next-steps","title":"Next Steps","text":"<ul> <li>See Running Evaluations</li> <li>Learn Writing Tests</li> <li>Explore GitHub Actions Examples</li> </ul>"},{"location":"evals/multi-agent-assertions/","title":"Multi-Agent Assertions","text":"<p>Prela provides specialized assertion types for testing multi-agent system behaviors. These assertions work across all supported multi-agent frameworks: CrewAI, AutoGen, LangGraph, and Swarm.</p>"},{"location":"evals/multi-agent-assertions/#overview","title":"Overview","text":"<p>Multi-agent assertions validate behaviors like:</p> <ul> <li>Agent participation and invocation</li> <li>Task completion and delegation</li> <li>Agent-to-agent handoffs</li> <li>Conversation flow and turn management</li> <li>Circular delegation detection</li> </ul> <p>All assertions operate on trace data captured during multi-agent execution, providing systematic testing without framework-specific code.</p>"},{"location":"evals/multi-agent-assertions/#available-assertions","title":"Available Assertions","text":""},{"location":"evals/multi-agent-assertions/#agentusedassertion","title":"AgentUsedAssertion","text":"<p>Verifies that a specific agent was invoked during execution with a minimum number of invocations.</p> <p>Parameters: - <code>agent_name</code> (str): Name of the agent to check - <code>min_invocations</code> (int): Minimum expected invocations (default: 1)</p> <p>Works with: CrewAI, AutoGen, LangGraph, Swarm</p> <p>Example: <pre><code>from prela.evals.assertions import AgentUsedAssertion\n\nassertion = AgentUsedAssertion(agent_name=\"Researcher\", min_invocations=1)\n</code></pre></p> <p>Use cases: - Verify an agent participated in the workflow - Ensure critical agents were invoked - Validate agent usage in multi-agent scenarios</p>"},{"location":"evals/multi-agent-assertions/#taskcompletedassertion","title":"TaskCompletedAssertion","text":"<p>Verifies that a specific task completed successfully, optionally matching task description.</p> <p>Parameters: - <code>task_description</code> (str): Task description to match (partial match) - <code>exact_match</code> (bool): Whether to require exact description match (default: False)</p> <p>Works with: CrewAI, AutoGen</p> <p>Example: <pre><code>from prela.evals.assertions import TaskCompletedAssertion\n\nassertion = TaskCompletedAssertion(\n    task_description=\"Research AI trends\",\n    exact_match=False\n)\n</code></pre></p> <p>Use cases: - Verify task execution in CrewAI workflows - Ensure critical tasks completed - Validate task orchestration</p>"},{"location":"evals/multi-agent-assertions/#delegationoccurredassertion","title":"DelegationOccurredAssertion","text":"<p>Verifies agent-to-agent delegation occurred in task-based multi-agent systems.</p> <p>Parameters: - <code>from_agent</code> (str | None): Delegating agent name (None = any) - <code>to_agent</code> (str | None): Receiving agent name (None = any)</p> <p>Works with: CrewAI</p> <p>Example: <pre><code>from prela.evals.assertions import DelegationOccurredAssertion\n\n# Specific delegation\nassertion = DelegationOccurredAssertion(\n    from_agent=\"Manager\",\n    to_agent=\"Researcher\"\n)\n\n# Any delegation to Researcher\nassertion = DelegationOccurredAssertion(\n    from_agent=None,\n    to_agent=\"Researcher\"\n)\n</code></pre></p> <p>Use cases: - Verify delegation flow in CrewAI - Ensure managers delegate to workers - Validate task assignment patterns</p>"},{"location":"evals/multi-agent-assertions/#handoffoccurredassertion","title":"HandoffOccurredAssertion","text":"<p>Verifies agent handoffs occurred in handoff-based multi-agent systems (Swarm).</p> <p>Parameters: - <code>from_agent</code> (str | None): Initial agent name (None = any) - <code>to_agent</code> (str | None): Final agent name (None = any)</p> <p>Works with: Swarm</p> <p>Example: <pre><code>from prela.evals.assertions import HandoffOccurredAssertion\n\n# Specific handoff\nassertion = HandoffOccurredAssertion(\n    from_agent=\"Triage\",\n    to_agent=\"Billing\"\n)\n\n# Any handoff to Billing\nassertion = HandoffOccurredAssertion(\n    from_agent=None,\n    to_agent=\"Billing\"\n)\n</code></pre></p> <p>Use cases: - Verify handoff routing in Swarm - Ensure agents transfer control correctly - Validate agent switching logic</p>"},{"location":"evals/multi-agent-assertions/#agentcollaborationassertion","title":"AgentCollaborationAssertion","text":"<p>Verifies that a minimum number of unique agents participated in the execution.</p> <p>Parameters: - <code>min_agents</code> (int): Minimum expected unique agents</p> <p>Works with: CrewAI, AutoGen, LangGraph, Swarm</p> <p>Example: <pre><code>from prela.evals.assertions import AgentCollaborationAssertion\n\nassertion = AgentCollaborationAssertion(min_agents=3)\n</code></pre></p> <p>Use cases: - Verify multi-agent collaboration - Ensure minimum agent participation - Validate team-based workflows</p>"},{"location":"evals/multi-agent-assertions/#conversationturnsassertion","title":"ConversationTurnsAssertion","text":"<p>Verifies that a conversation had a specific number of turns (message exchanges).</p> <p>Parameters: - <code>min_turns</code> (int | None): Minimum expected turns (default: None) - <code>max_turns</code> (int | None): Maximum expected turns (default: None)</p> <p>Works with: AutoGen</p> <p>Example: <pre><code>from prela.evals.assertions import ConversationTurnsAssertion\n\n# Exact range\nassertion = ConversationTurnsAssertion(min_turns=3, max_turns=10)\n\n# At least N turns\nassertion = ConversationTurnsAssertion(min_turns=5, max_turns=None)\n</code></pre></p> <p>Use cases: - Verify conversation length in AutoGen - Ensure conversations don't exceed limits - Validate termination conditions</p>"},{"location":"evals/multi-agent-assertions/#nocirculardelegationassertion","title":"NoCircularDelegationAssertion","text":"<p>Detects circular delegation loops using depth-first search (DFS) on the delegation graph.</p> <p>Parameters: - None</p> <p>Works with: CrewAI, AutoGen, LangGraph, Swarm</p> <p>Example: <pre><code>from prela.evals.assertions import NoCircularDelegationAssertion\n\nassertion = NoCircularDelegationAssertion()\n</code></pre></p> <p>Use cases: - Prevent infinite delegation loops - Ensure proper termination - Validate delegation graph structure</p> <p>Algorithm: - Builds delegation graph from trace events - Performs DFS to detect cycles - Returns list of detected cycles (agent names)</p>"},{"location":"evals/multi-agent-assertions/#complete-example","title":"Complete Example","text":"<p>Here's a complete test case using multiple multi-agent assertions:</p> <pre><code>from prela.evals import EvalCase, EvalInput, EvalRunner, EvalSuite\nfrom prela.evals.assertions import (\n    AgentCollaborationAssertion,\n    AgentUsedAssertion,\n    DelegationOccurredAssertion,\n    NoCircularDelegationAssertion,\n    TaskCompletedAssertion,\n)\n\n# Define test case\ntest_case = EvalCase(\n    id=\"test_research_workflow\",\n    name=\"Research workflow with delegation\",\n    input=EvalInput(query=\"Research AI agent trends and write summary\"),\n    assertions=[\n        # Verify all agents participated\n        AgentUsedAssertion(agent_name=\"Manager\", min_invocations=1),\n        AgentUsedAssertion(agent_name=\"Researcher\", min_invocations=1),\n        AgentUsedAssertion(agent_name=\"Writer\", min_invocations=1),\n\n        # Verify task completion\n        TaskCompletedAssertion(task_description=\"Research AI trends\"),\n        TaskCompletedAssertion(task_description=\"Write summary\"),\n\n        # Verify delegation flow\n        DelegationOccurredAssertion(\n            from_agent=\"Manager\",\n            to_agent=\"Researcher\"\n        ),\n        DelegationOccurredAssertion(\n            from_agent=\"Manager\",\n            to_agent=\"Writer\"\n        ),\n\n        # Verify collaboration\n        AgentCollaborationAssertion(min_agents=3),\n\n        # Ensure no circular delegation\n        NoCircularDelegationAssertion(),\n    ]\n)\n\n# Create suite\nsuite = EvalSuite(name=\"Multi-Agent Tests\", cases=[test_case])\n\n# Define agent function\ndef my_crew_workflow(input_data):\n    # Your CrewAI implementation\n    # ...\n    return result\n\n# Run evaluation\nrunner = EvalRunner(suite, my_crew_workflow)\nresult = runner.run()\n\n# View results\nprint(result.summary())\n</code></pre> <p>Output: <pre><code>Evaluation Suite: Multi-Agent Tests\nTotal Cases: 1\nPassed: 1 (100.0%)\nCase Results:\n  \u2713 Research workflow with delegation (2.5s)\n    \u2713 PASS [agent_used] Agent 'Manager' invoked 1 times\n    \u2713 PASS [agent_used] Agent 'Researcher' invoked 1 times\n    \u2713 PASS [agent_used] Agent 'Writer' invoked 1 times\n    \u2713 PASS [task_completed] Task 'Research AI trends' completed\n    \u2713 PASS [task_completed] Task 'Write summary' completed\n    \u2713 PASS [delegation_occurred] Delegation from Manager to Researcher\n    \u2713 PASS [delegation_occurred] Delegation from Manager to Writer\n    \u2713 PASS [agent_collaboration] Found 3 agents\n    \u2713 PASS [no_circular_delegation] No circular delegation detected\n</code></pre></p>"},{"location":"evals/multi-agent-assertions/#framework-specific-examples","title":"Framework-Specific Examples","text":""},{"location":"evals/multi-agent-assertions/#crewai-example","title":"CrewAI Example","text":"<pre><code>from prela.evals import EvalCase, EvalInput\nfrom prela.evals.assertions import (\n    AgentUsedAssertion,\n    DelegationOccurredAssertion,\n    TaskCompletedAssertion,\n)\n\ncrewai_case = EvalCase(\n    id=\"test_crewai_delegation\",\n    name=\"CrewAI task delegation\",\n    input=EvalInput(query=\"Complete research task\"),\n    assertions=[\n        AgentUsedAssertion(agent_name=\"Researcher\", min_invocations=1),\n        TaskCompletedAssertion(task_description=\"Research\"),\n        DelegationOccurredAssertion(\n            from_agent=\"Manager\",\n            to_agent=\"Researcher\"\n        ),\n    ]\n)\n</code></pre>"},{"location":"evals/multi-agent-assertions/#autogen-example","title":"AutoGen Example","text":"<pre><code>from prela.evals import EvalCase, EvalInput\nfrom prela.evals.assertions import (\n    AgentUsedAssertion,\n    ConversationTurnsAssertion,\n    AgentCollaborationAssertion,\n)\n\nautogen_case = EvalCase(\n    id=\"test_autogen_conversation\",\n    name=\"AutoGen multi-turn conversation\",\n    input=EvalInput(query=\"Discuss project requirements\"),\n    assertions=[\n        AgentUsedAssertion(agent_name=\"UserProxy\", min_invocations=1),\n        AgentUsedAssertion(agent_name=\"Assistant\", min_invocations=1),\n        ConversationTurnsAssertion(min_turns=3, max_turns=10),\n        AgentCollaborationAssertion(min_agents=2),\n    ]\n)\n</code></pre>"},{"location":"evals/multi-agent-assertions/#langgraph-example","title":"LangGraph Example","text":"<pre><code>from prela.evals import EvalCase, EvalInput\nfrom prela.evals.assertions import (\n    AgentUsedAssertion,\n    AgentCollaborationAssertion,\n)\n\nlanggraph_case = EvalCase(\n    id=\"test_langgraph_workflow\",\n    name=\"LangGraph stateful workflow\",\n    input=EvalInput(query=\"Process data through graph\"),\n    assertions=[\n        AgentUsedAssertion(agent_name=\"analyze\", min_invocations=1),\n        AgentUsedAssertion(agent_name=\"respond\", min_invocations=1),\n        AgentCollaborationAssertion(min_agents=2),\n    ]\n)\n</code></pre>"},{"location":"evals/multi-agent-assertions/#swarm-example","title":"Swarm Example","text":"<pre><code>from prela.evals import EvalCase, EvalInput\nfrom prela.evals.assertions import (\n    AgentUsedAssertion,\n    HandoffOccurredAssertion,\n)\n\nswarm_case = EvalCase(\n    id=\"test_swarm_handoff\",\n    name=\"Swarm agent handoff\",\n    input=EvalInput(query=\"Handle customer inquiry\"),\n    assertions=[\n        AgentUsedAssertion(agent_name=\"Triage\", min_invocations=1),\n        AgentUsedAssertion(agent_name=\"Billing\", min_invocations=1),\n        HandoffOccurredAssertion(\n            from_agent=\"Triage\",\n            to_agent=\"Billing\"\n        ),\n    ]\n)\n</code></pre>"},{"location":"evals/multi-agent-assertions/#convenience-functions","title":"Convenience Functions","text":"<p>All assertions have convenience factory functions for cleaner syntax:</p> <pre><code>from prela.evals.assertions import (\n    agent_used,\n    task_completed,\n    delegation_occurred,\n    handoff_occurred,\n    agent_collaboration,\n    conversation_turns,\n    no_circular_delegation,\n)\n\n# Cleaner syntax\nassertions = [\n    agent_used(\"Manager\", min_invocations=1),\n    task_completed(\"Research AI trends\"),\n    delegation_occurred(from_agent=\"Manager\", to_agent=\"Researcher\"),\n    handoff_occurred(from_agent=\"Triage\", to_agent=\"Billing\"),\n    agent_collaboration(min_agents=3),\n    conversation_turns(min_turns=3, max_turns=10),\n    no_circular_delegation(),\n]\n</code></pre>"},{"location":"evals/multi-agent-assertions/#best-practices","title":"Best Practices","text":""},{"location":"evals/multi-agent-assertions/#1-test-critical-flows","title":"1. Test Critical Flows","text":"<p>Always verify that critical agents and tasks execute:</p> <pre><code>assertions = [\n    agent_used(\"CriticalAgent\", min_invocations=1),\n    task_completed(\"Critical task description\"),\n]\n</code></pre>"},{"location":"evals/multi-agent-assertions/#2-validate-collaboration","title":"2. Validate Collaboration","text":"<p>Ensure minimum agent participation in team workflows:</p> <pre><code>assertions = [\n    agent_collaboration(min_agents=3),  # At least 3 agents must participate\n]\n</code></pre>"},{"location":"evals/multi-agent-assertions/#3-prevent-infinite-loops","title":"3. Prevent Infinite Loops","text":"<p>Always include circular delegation detection:</p> <pre><code>assertions = [\n    no_circular_delegation(),  # Catch delegation cycles\n]\n</code></pre>"},{"location":"evals/multi-agent-assertions/#4-framework-specific-assertions","title":"4. Framework-Specific Assertions","text":"<p>Use framework-appropriate assertions:</p> <ul> <li>CrewAI: Use <code>DelegationOccurredAssertion</code> and <code>TaskCompletedAssertion</code></li> <li>AutoGen: Use <code>ConversationTurnsAssertion</code></li> <li>Swarm: Use <code>HandoffOccurredAssertion</code></li> <li>LangGraph: Use <code>AgentUsedAssertion</code> for node verification</li> </ul>"},{"location":"evals/multi-agent-assertions/#5-combine-assertions","title":"5. Combine Assertions","text":"<p>Test multiple aspects of multi-agent behavior:</p> <pre><code>assertions = [\n    agent_used(\"Manager\", min_invocations=1),          # Verify participation\n    delegation_occurred(\"Manager\", \"Worker\"),          # Verify delegation\n    agent_collaboration(min_agents=2),                  # Verify collaboration\n    no_circular_delegation(),                           # Verify no loops\n]\n</code></pre>"},{"location":"evals/multi-agent-assertions/#troubleshooting","title":"Troubleshooting","text":""},{"location":"evals/multi-agent-assertions/#assertion-failing-agent-not-found","title":"Assertion Failing: Agent Not Found","text":"<p>Problem: <code>AgentUsedAssertion</code> fails with \"Agent 'X' not found\"</p> <p>Solution: Verify agent name matches exactly: <pre><code># Check trace data for actual agent names\n# Agent names must match framework's naming convention\n</code></pre></p>"},{"location":"evals/multi-agent-assertions/#assertion-failing-no-delegation-detected","title":"Assertion Failing: No Delegation Detected","text":"<p>Problem: <code>DelegationOccurredAssertion</code> fails even though delegation occurred</p> <p>Solution: Verify framework emits delegation events: <pre><code># CrewAI: Delegation tracked via task assignments\n# Swarm: Handoffs tracked via agent switching\n# Check that framework instrumentation is enabled\n</code></pre></p>"},{"location":"evals/multi-agent-assertions/#assertion-failing-circular-delegation-false-positive","title":"Assertion Failing: Circular Delegation False Positive","text":"<p>Problem: <code>NoCircularDelegationAssertion</code> detects cycles when none exist</p> <p>Solution: Review delegation graph structure: <pre><code># Check trace events for delegation patterns\n# Ensure agents don't delegate back to themselves\n</code></pre></p>"},{"location":"evals/multi-agent-assertions/#integration-with-evaluation-framework","title":"Integration with Evaluation Framework","text":"<p>Multi-agent assertions integrate seamlessly with Prela's evaluation framework:</p> <pre><code>from prela.evals import EvalRunner, EvalSuite\nfrom prela.evals.reporters import ConsoleReporter, JUnitReporter\n\n# Create suite with multi-agent assertions\nsuite = EvalSuite(name=\"Multi-Agent Tests\", cases=[...])\n\n# Run with tracer integration\nrunner = EvalRunner(suite, my_agent_function, tracer=tracer)\nresult = runner.run()\n\n# Report results\nConsoleReporter(verbose=True).report(result)\nJUnitReporter(\"results/junit.xml\").report(result)\n</code></pre>"},{"location":"evals/multi-agent-assertions/#next-steps","title":"Next Steps","text":"<ul> <li>Writing Tests - Learn how to create comprehensive test cases</li> <li>Running Evaluations - Execute evaluation suites with parallel support</li> <li>CI Integration - Integrate with GitHub Actions, GitLab CI, etc.</li> <li>CrewAI Integration - CrewAI-specific instrumentation details</li> <li>AutoGen Integration - AutoGen-specific instrumentation details</li> <li>LangGraph Integration - LangGraph-specific instrumentation details</li> <li>Swarm Integration - Swarm-specific instrumentation details</li> </ul>"},{"location":"evals/n8n-workflows/","title":"N8N Workflow Evaluation","text":"<p>Prela provides a specialized evaluation framework for systematically testing n8n workflows. This enables automated testing, regression detection, and continuous integration of workflow changes.</p>"},{"location":"evals/n8n-workflows/#overview","title":"Overview","text":"<p>The n8n evaluation framework allows you to:</p> <ul> <li>Test workflows automatically - Trigger workflows and validate results</li> <li>Assert on node outputs - Verify specific node data using path notation</li> <li>Check execution metrics - Validate duration, status, token usage</li> <li>Test AI nodes - Assert on LLM token budgets and costs</li> <li>Integrate with CI/CD - Run tests in GitHub Actions, GitLab CI, etc.</li> </ul> <p>Key Features: - Async HTTP-based execution (n8n API) - Node output validation with dot-notation paths - Token budget assertions for AI nodes - Duration and status assertions - Convenience factory functions for clean test syntax</p>"},{"location":"evals/n8n-workflows/#installation","title":"Installation","text":"<pre><code># Install Prela with evaluation support\npip install prela\n\n# Requires: httpx for async HTTP calls\npip install httpx\n</code></pre>"},{"location":"evals/n8n-workflows/#quick-start","title":"Quick Start","text":""},{"location":"evals/n8n-workflows/#basic-workflow-test","title":"Basic Workflow Test","text":"<pre><code>import asyncio\nfrom prela.evals.n8n import eval_n8n_workflow, N8nEvalCase\nfrom prela.evals.n8n.assertions import node_completed, workflow_completed\n\nasync def test_simple_workflow():\n    case = N8nEvalCase(\n        id=\"test_basic\",\n        name=\"Basic workflow execution\",\n        trigger_data={\"message\": \"Hello World\"},\n        node_assertions={\n            \"Process Data\": [node_completed(\"Process Data\")],\n        },\n        workflow_assertions=[workflow_completed()],\n    )\n\n    results = await eval_n8n_workflow(\n        workflow_id=\"workflow-123\",\n        test_cases=[case],\n        n8n_url=\"http://localhost:5678\",\n    )\n\n    print(f\"Passed: {results['passed']}/{results['total']}\")\n\nasyncio.run(test_simple_workflow())\n</code></pre>"},{"location":"evals/n8n-workflows/#n8nevalcase","title":"N8nEvalCase","text":"<p>Data structure for defining test cases:</p> <pre><code>@dataclass\nclass N8nEvalCase:\n    id: str                                # Unique test case ID\n    name: str                              # Test case name\n    trigger_data: dict                     # Data to trigger workflow\n    node_assertions: dict[str, list]       # Per-node assertions {node_name: [assertions]}\n    workflow_assertions: list              # Workflow-level assertions\n    expected_output: Any | None            # Expected final output (optional)\n    tags: list[str]                        # Tags for filtering (optional)\n    timeout_seconds: float                 # Execution timeout (default: 30s)\n    metadata: dict[str, Any]               # Additional metadata (optional)\n</code></pre> <p>Example: <pre><code>case = N8nEvalCase(\n    id=\"test_ai_workflow\",\n    name=\"AI content generation workflow\",\n    trigger_data={\n        \"topic\": \"AI agents\",\n        \"length\": \"short\"\n    },\n    node_assertions={\n        \"OpenAI GPT-4\": [\n            node_completed(\"OpenAI GPT-4\"),\n            node_output(\"OpenAI GPT-4\", \"response.content\", expected_value=\"AI agents\"),\n            tokens_under(\"OpenAI GPT-4\", 1000)\n        ],\n        \"Format Output\": [\n            node_completed(\"Format Output\"),\n            node_output(\"Format Output\", \"formatted\", expected_value=True)\n        ]\n    },\n    workflow_assertions=[\n        workflow_completed(),\n        duration_under(15.0)\n    ],\n    expected_output={\"status\": \"success\"},\n    tags=[\"ai\", \"content\", \"production\"],\n    timeout_seconds=60,\n)\n</code></pre></p>"},{"location":"evals/n8n-workflows/#assertions","title":"Assertions","text":""},{"location":"evals/n8n-workflows/#n8nnodecompleted","title":"N8nNodeCompleted","text":"<p>Verify that a node completed successfully.</p> <p>Usage: <pre><code>from prela.evals.n8n.assertions import N8nNodeCompleted, node_completed\n\n# Class instantiation\nassertion = N8nNodeCompleted(node_name=\"Data Processor\")\n\n# Convenience function\nassertion = node_completed(\"Data Processor\")\n</code></pre></p> <p>Checks: - Node exists in execution - Node status is \"success\"</p>"},{"location":"evals/n8n-workflows/#n8nnodeoutput","title":"N8nNodeOutput","text":"<p>Assert on node outputs using dot-notation paths.</p> <p>Usage: <pre><code>from prela.evals.n8n.assertions import N8nNodeOutput, node_output\n\n# Simple path\nassertion = node_output(\"API Call\", \"response.status\", 200)\n\n# Nested path\nassertion = node_output(\"API Call\", \"response.data.user.id\", \"user-123\")\n\n# Array index\nassertion = node_output(\"Process Items\", \"items.0.name\", \"first-item\")\n</code></pre></p> <p>Path Notation: - <code>response.status</code> - Navigate nested objects with dots - <code>items.0.name</code> - Access array elements with numeric indices - <code>data.user.id</code> - Multiple levels of nesting supported</p> <p>Example: <pre><code>node_output(\"HTTP Request\", \"response.status\", 200)\nnode_output(\"Transform\", \"output.count\", 42)\nnode_output(\"Parse JSON\", \"data.items.0.id\", \"item-1\")\n</code></pre></p>"},{"location":"evals/n8n-workflows/#n8nworkflowduration","title":"N8nWorkflowDuration","text":"<p>Assert that workflow completed within a time limit.</p> <p>Usage: <pre><code>from prela.evals.n8n.assertions import N8nWorkflowDuration, duration_under\n\n# Class instantiation\nassertion = N8nWorkflowDuration(max_seconds=10.0)\n\n# Convenience function\nassertion = duration_under(10.0)\n</code></pre></p> <p>Checks: - Workflow duration_ms &lt;= max_seconds * 1000</p>"},{"location":"evals/n8n-workflows/#n8nainodetokens","title":"N8nAINodeTokens","text":"<p>Assert that AI node token usage is within budget.</p> <p>Usage: <pre><code>from prela.evals.n8n.assertions import N8nAINodeTokens, tokens_under\n\n# Class instantiation\nassertion = N8nAINodeTokens(node_name=\"GPT-4\", max_tokens=1000)\n\n# Convenience function\nassertion = tokens_under(\"GPT-4\", 1000)\n</code></pre></p> <p>Checks: - Node total_tokens &lt;= max_tokens</p> <p>Note: Works with AI nodes that report token usage (OpenAI, Anthropic, etc.)</p>"},{"location":"evals/n8n-workflows/#n8nworkflowstatus","title":"N8nWorkflowStatus","text":"<p>Assert that workflow completed with expected status.</p> <p>Usage: <pre><code>from prela.evals.n8n.assertions import N8nWorkflowStatus, workflow_completed, workflow_status\n\n# Success status (convenience)\nassertion = workflow_completed()  # Expects \"success\"\n\n# Custom status\nassertion = workflow_status(\"error\")  # Expects \"error\"\n\n# Class instantiation\nassertion = N8nWorkflowStatus(expected_status=\"success\")\n</code></pre></p> <p>Statuses: - <code>success</code> - Workflow completed successfully - <code>error</code> - Workflow failed with error - <code>crashed</code> - Workflow crashed unexpectedly - <code>running</code> - Workflow still executing (should not occur after polling)</p>"},{"location":"evals/n8n-workflows/#complete-example","title":"Complete Example","text":""},{"location":"evals/n8n-workflows/#lead-scoring-workflow-test","title":"Lead Scoring Workflow Test","text":"<pre><code>import asyncio\nfrom prela.evals.n8n import eval_n8n_workflow, N8nEvalCase\nfrom prela.evals.n8n.assertions import (\n    node_completed,\n    node_output,\n    duration_under,\n    tokens_under,\n    workflow_completed,\n)\n\nasync def test_lead_scoring():\n    \"\"\"Test lead scoring workflow with AI classification.\"\"\"\n\n    # Test Case 1: High-intent lead\n    high_intent_case = N8nEvalCase(\n        id=\"test_high_intent\",\n        name=\"High-intent lead scoring\",\n        trigger_data={\n            \"email\": \"I want to buy your premium plan immediately\",\n            \"company\": \"ACME Corp\",\n            \"employees\": 500\n        },\n        node_assertions={\n            \"AI Classifier\": [\n                node_completed(\"AI Classifier\"),\n                node_output(\"AI Classifier\", \"intent\", \"high_intent\"),\n                tokens_under(\"AI Classifier\", 500)\n            ],\n            \"Lead Scorer\": [\n                node_completed(\"Lead Scorer\"),\n                node_output(\"Lead Scorer\", \"score\", 95)\n            ],\n            \"Route Lead\": [\n                node_completed(\"Route Lead\"),\n                node_output(\"Route Lead\", \"route\", \"sales_team\")\n            ]\n        },\n        workflow_assertions=[\n            workflow_completed(),\n            duration_under(15.0)\n        ],\n        expected_output={\n            \"intent\": \"high_intent\",\n            \"score\": 95,\n            \"route\": \"sales_team\"\n        },\n        tags=[\"high-priority\", \"sales\"]\n    )\n\n    # Test Case 2: Low-intent lead\n    low_intent_case = N8nEvalCase(\n        id=\"test_low_intent\",\n        name=\"Low-intent lead scoring\",\n        trigger_data={\n            \"email\": \"Just browsing your website\",\n            \"company\": \"Unknown\",\n            \"employees\": None\n        },\n        node_assertions={\n            \"AI Classifier\": [\n                node_completed(\"AI Classifier\"),\n                node_output(\"AI Classifier\", \"intent\", \"low_intent\"),\n            ],\n            \"Lead Scorer\": [\n                node_completed(\"Lead Scorer\"),\n                node_output(\"Lead Scorer\", \"score\", 30)\n            ],\n        },\n        workflow_assertions=[\n            workflow_completed(),\n            duration_under(15.0)\n        ],\n        tags=[\"low-priority\"]\n    )\n\n    # Run all test cases\n    results = await eval_n8n_workflow(\n        workflow_id=\"lead-scoring-workflow-123\",\n        test_cases=[high_intent_case, low_intent_case],\n        n8n_url=\"http://localhost:5678\",\n        timeout_seconds=120,\n    )\n\n    # Print results\n    print(f\"\\nTest Results: {results['passed']}/{results['total']} passed\")\n    for case_result in results['cases']:\n        status = \"\u2713\" if case_result['passed'] else \"\u2717\"\n        print(f\"  {status} {case_result['case_name']}\")\n\n        # Print failed assertions\n        if not case_result['passed']:\n            for assertion_result in case_result['assertion_results']:\n                if not assertion_result['passed']:\n                    print(f\"    \u2717 {assertion_result['message']}\")\n\nasyncio.run(test_lead_scoring())\n</code></pre> <p>Output: <pre><code>Test Results: 2/2 passed\n  \u2713 High-intent lead scoring\n  \u2713 Low-intent lead scoring\n</code></pre></p>"},{"location":"evals/n8n-workflows/#n8nworkflowevalrunner","title":"N8nWorkflowEvalRunner","text":"<p>Low-level async runner for advanced usage:</p> <pre><code>from prela.evals.n8n import N8nWorkflowEvalRunner, N8nWorkflowEvalConfig, N8nEvalCase\n\n# Configure runner\nconfig = N8nWorkflowEvalConfig(\n    workflow_id=\"workflow-123\",\n    n8n_url=\"http://localhost:5678\",\n    n8n_api_key=\"n8n-api-key\",  # Optional\n    timeout_seconds=120,\n)\n\n# Create runner\nrunner = N8nWorkflowEvalRunner(config)\n\n# Run test cases\ncases = [case1, case2, case3]\nresults = await runner.run_suite(cases)\n\n# Access detailed results\nfor case_result in results['cases']:\n    print(f\"Case: {case_result['case_name']}\")\n    print(f\"Passed: {case_result['passed']}\")\n    print(f\"Duration: {case_result['duration_ms']}ms\")\n    print(f\"Assertions: {len(case_result['assertion_results'])}\")\n</code></pre> <p>Configuration: <pre><code>@dataclass\nclass N8nWorkflowEvalConfig:\n    workflow_id: str           # n8n workflow ID\n    n8n_url: str              # n8n instance URL (e.g., http://localhost:5678)\n    n8n_api_key: str | None   # Optional API key for authentication\n    timeout_seconds: int      # Maximum execution time (default: 120)\n</code></pre></p>"},{"location":"evals/n8n-workflows/#convenience-function","title":"Convenience Function","text":""},{"location":"evals/n8n-workflows/#eval_n8n_workflow","title":"eval_n8n_workflow()","text":"<p>High-level function for simple usage:</p> <pre><code>async def eval_n8n_workflow(\n    workflow_id: str,\n    test_cases: list[N8nEvalCase],\n    n8n_url: str = \"http://localhost:5678\",\n    n8n_api_key: str | None = None,\n    timeout_seconds: int = 120,\n    tracer: Tracer | None = None,\n) -&gt; dict:\n    \"\"\"\n    Evaluate n8n workflow with test cases.\n\n    Args:\n        workflow_id: n8n workflow ID\n        test_cases: List of test cases to run\n        n8n_url: n8n instance URL\n        n8n_api_key: Optional API key\n        timeout_seconds: Maximum execution time\n        tracer: Optional Prela tracer for observability\n\n    Returns:\n        dict with keys:\n        - total: Total test cases\n        - passed: Number passed\n        - failed: Number failed\n        - cases: List of case results\n    \"\"\"\n</code></pre> <p>Example: <pre><code>results = await eval_n8n_workflow(\n    workflow_id=\"my-workflow-123\",\n    test_cases=[case1, case2],\n    n8n_url=os.environ[\"N8N_URL\"],\n    n8n_api_key=os.environ[\"N8N_API_KEY\"],\n)\n</code></pre></p>"},{"location":"evals/n8n-workflows/#cicd-integration","title":"CI/CD Integration","text":""},{"location":"evals/n8n-workflows/#github-actions","title":"GitHub Actions","text":"<pre><code>name: N8N Workflow Tests\n\non: [push, pull_request]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n\n      - name: Set up Python\n        uses: actions/setup-python@v4\n        with:\n          python-version: '3.11'\n\n      - name: Install dependencies\n        run: |\n          pip install prela httpx\n\n      - name: Run workflow tests\n        env:\n          N8N_URL: ${{ secrets.N8N_URL }}\n          N8N_API_KEY: ${{ secrets.N8N_API_KEY }}\n        run: |\n          python test_workflows.py\n</code></pre>"},{"location":"evals/n8n-workflows/#gitlab-ci","title":"GitLab CI","text":"<pre><code>test-workflows:\n  image: python:3.11\n  script:\n    - pip install prela httpx\n    - python test_workflows.py\n  variables:\n    N8N_URL: $N8N_URL\n    N8N_API_KEY: $N8N_API_KEY\n</code></pre>"},{"location":"evals/n8n-workflows/#pytest-integration","title":"pytest Integration","text":"<pre><code>import pytest\nfrom prela.evals.n8n import eval_n8n_workflow, N8nEvalCase\nfrom prela.evals.n8n.assertions import node_completed, workflow_completed\n\n@pytest.mark.asyncio\nasync def test_data_pipeline():\n    \"\"\"Test data processing pipeline workflow.\"\"\"\n    case = N8nEvalCase(\n        id=\"test_pipeline\",\n        name=\"Data pipeline\",\n        trigger_data={\"data\": [1, 2, 3, 4, 5]},\n        node_assertions={\n            \"Process\": [node_completed(\"Process\")],\n            \"Transform\": [node_completed(\"Transform\")],\n        },\n        workflow_assertions=[workflow_completed()],\n    )\n\n    results = await eval_n8n_workflow(\n        workflow_id=os.environ[\"WORKFLOW_ID\"],\n        test_cases=[case],\n        n8n_url=os.environ[\"N8N_URL\"],\n    )\n\n    assert results['passed'] == results['total'], \\\n        f\"Test failed: {results['passed']}/{results['total']}\"\n</code></pre>"},{"location":"evals/n8n-workflows/#best-practices","title":"Best Practices","text":""},{"location":"evals/n8n-workflows/#1-use-descriptive-test-names","title":"1. Use Descriptive Test Names","text":"<pre><code># \u2713 Good - clear intent\ncase = N8nEvalCase(\n    id=\"test_high_priority_leads\",\n    name=\"High-priority lead routing with AI classification\",\n    ...\n)\n\n# \u2717 Bad - vague\ncase = N8nEvalCase(\n    id=\"test1\",\n    name=\"Test\",\n    ...\n)\n</code></pre>"},{"location":"evals/n8n-workflows/#2-assert-on-critical-nodes","title":"2. Assert on Critical Nodes","text":"<p>Focus assertions on business-critical nodes:</p> <pre><code>node_assertions={\n    \"AI Classifier\": [\n        node_completed(\"AI Classifier\"),\n        node_output(\"AI Classifier\", \"decision\", \"approve\"),\n        tokens_under(\"AI Classifier\", 1000),  # Cost control\n    ],\n    # Skip non-critical nodes\n}\n</code></pre>"},{"location":"evals/n8n-workflows/#3-set-realistic-timeouts","title":"3. Set Realistic Timeouts","text":"<p>Configure timeouts based on workflow complexity:</p> <pre><code># Simple workflow\ncase = N8nEvalCase(..., timeout_seconds=30)\n\n# Complex AI workflow\ncase = N8nEvalCase(..., timeout_seconds=120)\n</code></pre>"},{"location":"evals/n8n-workflows/#4-use-tags-for-organization","title":"4. Use Tags for Organization","text":"<pre><code>case = N8nEvalCase(\n    ...,\n    tags=[\"production\", \"ai\", \"high-priority\"],\n)\n\n# Filter tests by tags\nproduction_cases = [c for c in cases if \"production\" in c.tags]\n</code></pre>"},{"location":"evals/n8n-workflows/#5-validate-expected-outputs","title":"5. Validate Expected Outputs","text":"<p>Assert on final workflow outputs:</p> <pre><code>case = N8nEvalCase(\n    ...,\n    expected_output={\n        \"status\": \"success\",\n        \"score\": 95,\n        \"next_step\": \"contact_sales\"\n    },\n)\n</code></pre>"},{"location":"evals/n8n-workflows/#troubleshooting","title":"Troubleshooting","text":""},{"location":"evals/n8n-workflows/#issue-workflow-not-triggering","title":"Issue: Workflow Not Triggering","text":"<p>Symptoms: eval_n8n_workflow() times out, workflow never starts.</p> <p>Solutions: 1. Verify workflow ID is correct:    <pre><code># Check n8n UI for workflow ID\nworkflow_id=\"abc-123-def-456\"\n</code></pre></p> <ol> <li> <p>Verify n8n URL and API key:    <pre><code>n8n_url=\"http://localhost:5678\"  # Include http://\nn8n_api_key=\"your-api-key\"\n</code></pre></p> </li> <li> <p>Check n8n API is accessible:    <pre><code>curl http://localhost:5678/api/v1/workflows/abc-123\n</code></pre></p> </li> </ol>"},{"location":"evals/n8n-workflows/#issue-assertions-failing","title":"Issue: Assertions Failing","text":"<p>Symptoms: node_output() assertions fail unexpectedly.</p> <p>Solutions: 1. Verify path notation is correct:    <pre><code># \u2713 Correct\nnode_output(\"API\", \"response.data.id\", \"user-123\")\n\n# \u2717 Wrong - typo in path\nnode_output(\"API\", \"response.datta.id\", \"user-123\")\n</code></pre></p> <ol> <li> <p>Check node name matches exactly:    <pre><code># Node names are case-sensitive\nnode_output(\"OpenAI GPT-4\", ...)  # Must match n8n node name exactly\n</code></pre></p> </li> <li> <p>Inspect actual node output:    <pre><code># Print node data for debugging\nprint(execution_result[\"nodes\"][\"API Call\"])\n</code></pre></p> </li> </ol>"},{"location":"evals/n8n-workflows/#issue-timeout-errors","title":"Issue: Timeout Errors","text":"<p>Symptoms: Tests timing out even though workflow completes in n8n UI.</p> <p>Solutions: 1. Increase timeout:    <pre><code>case = N8nEvalCase(..., timeout_seconds=180)  # 3 minutes\n</code></pre></p> <ol> <li>Check for polling issues:    <pre><code># Runner polls every 1 second\n# Timeout must be &gt; execution time\n</code></pre></li> </ol>"},{"location":"evals/n8n-workflows/#issue-token-assertions-failing","title":"Issue: Token Assertions Failing","text":"<p>Symptoms: tokens_under() fails even though token count seems reasonable.</p> <p>Solutions: 1. Verify AI node reports tokens:    <pre><code># Check if node has token data\nprint(node_data.get(\"total_tokens\"))\n</code></pre></p> <ol> <li>Check token field name:    <pre><code># Different nodes may use different field names\n# Prela looks for: total_tokens, token_count, tokens\n</code></pre></li> </ol>"},{"location":"evals/n8n-workflows/#next-steps","title":"Next Steps","text":"<ul> <li>N8N Webhook Integration - External workflow tracing</li> <li>N8N Code Nodes - Internal instrumentation</li> <li>Assertions - Full assertion reference</li> <li>CI Integration - GitHub Actions, GitLab CI setup</li> <li>Writing Tests - Comprehensive test case patterns</li> </ul>"},{"location":"evals/overview/","title":"Evaluation Framework","text":"<p>Prela's evaluation framework provides a comprehensive system for testing AI agents with assertions, test suites, and CI/CD integration.</p>"},{"location":"evals/overview/#overview","title":"Overview","text":"<p>The evaluation framework helps you:</p> <ul> <li>Define test cases with inputs and expected outputs</li> <li>Run assertions to validate agent behavior</li> <li>Execute test suites sequentially or in parallel</li> <li>Generate reports in multiple formats (console, JSON, JUnit)</li> <li>Integrate with CI/CD for automated testing</li> </ul> <pre><code>graph LR\n    A[Test Cases] --&gt; B[Test Suite]\n    B --&gt; C[Runner]\n    C --&gt; D[Agent]\n    D --&gt; E[Output]\n    E --&gt; F[Assertions]\n    F --&gt; G[Results]\n    G --&gt; H[Reporter]\n    H --&gt; I[Console/JSON/JUnit]</code></pre>"},{"location":"evals/overview/#quick-start","title":"Quick Start","text":"<pre><code>from prela.evals import EvalCase, EvalInput, EvalExpected, EvalSuite, EvalRunner\n\n# Define test case\ncase = EvalCase(\n    id=\"test_qa\",\n    name=\"Basic QA Test\",\n    input=EvalInput(query=\"What is 2+2?\"),\n    expected=EvalExpected(contains=[\"4\"])\n)\n\n# Create suite\nsuite = EvalSuite(name=\"Math Tests\", cases=[case])\n\n# Define your agent\ndef my_agent(input_data):\n    query = input_data.get(\"query\", \"\")\n    if \"2+2\" in query:\n        return \"The answer is 4\"\n    return \"I don't know\"\n\n# Run evaluation\nrunner = EvalRunner(suite, my_agent)\nresult = runner.run()\n\n# View results\nprint(result.summary())\n# Evaluation Suite: Math Tests\n# Total Cases: 1\n# Passed: 1 (100.0%)\n</code></pre>"},{"location":"evals/overview/#core-components","title":"Core Components","text":""},{"location":"evals/overview/#1-evalcase","title":"1. EvalCase","text":"<p>A test case defines: - Input: Data to pass to your agent - Expected: What the output should contain/match - Assertions: Validation rules - Metadata: Tags, timeout, description</p> <pre><code>from prela.evals import EvalCase, EvalInput, EvalExpected\n\ncase = EvalCase(\n    id=\"test_greeting\",\n    name=\"Greeting Test\",\n    input=EvalInput(\n        query=\"Say hello\",\n        context={\"language\": \"english\"}\n    ),\n    expected=EvalExpected(\n        contains=[\"hello\", \"hi\"],\n        not_contains=[\"error\"]\n    ),\n    tags=[\"greeting\", \"basic\"],\n    timeout_seconds=5.0\n)\n</code></pre>"},{"location":"evals/overview/#2-evalsuite","title":"2. EvalSuite","text":"<p>A collection of test cases with optional setup/teardown:</p> <pre><code>from prela.evals import EvalSuite\n\nsuite = EvalSuite(\n    name=\"Agent Test Suite\",\n    cases=[case1, case2, case3],\n    setup=lambda: print(\"Setting up...\"),\n    teardown=lambda: print(\"Cleaning up...\"),\n    default_assertions=[\n        {\"type\": \"latency\", \"max_ms\": 2000}\n    ]\n)\n</code></pre>"},{"location":"evals/overview/#3-assertions","title":"3. Assertions","text":"<p>Validation rules that check agent outputs:</p> <pre><code># Simple expectations\nexpected = EvalExpected(\n    contains=[\"success\"],          # Must contain\n    not_contains=[\"error\"],        # Must not contain\n)\n\n# Advanced assertions\nassertions = [\n    {\"type\": \"contains\", \"value\": \"result\"},\n    {\"type\": \"regex\", \"pattern\": r\"\\d+\"},\n    {\"type\": \"json_valid\"},\n    {\"type\": \"semantic_similarity\", \"threshold\": 0.8, \"reference\": \"...\"},\n    {\"type\": \"tool_called\", \"tool_name\": \"search\"},\n]\n</code></pre>"},{"location":"evals/overview/#4-evalrunner","title":"4. EvalRunner","text":"<p>Executes test suites and collects results:</p> <pre><code>from prela.evals import EvalRunner\n\nrunner = EvalRunner(\n    suite=suite,\n    agent_fn=my_agent,\n    parallel=True,          # Run tests in parallel\n    max_workers=4,          # Parallel workers\n    tracer=tracer           # Optional: trace test execution\n)\n\nresult = runner.run()\n</code></pre>"},{"location":"evals/overview/#5-reporters","title":"5. Reporters","text":"<p>Generate results in different formats:</p> <pre><code>from prela.evals.reporters import ConsoleReporter, JSONReporter, JUnitReporter\n\n# Console output\nConsoleReporter(verbosity=\"verbose\").report(result)\n\n# JSON file\nJSONReporter(\"results.json\").report(result)\n\n# JUnit XML (for CI/CD)\nJUnitReporter(\"junit.xml\").report(result)\n</code></pre>"},{"location":"evals/overview/#assertion-types","title":"Assertion Types","text":""},{"location":"evals/overview/#structural-assertions","title":"Structural Assertions","text":"<ul> <li>contains: Text must be present</li> <li>not_contains: Text must not be present</li> <li>regex: Match regex pattern</li> <li>length: Output length constraints</li> <li>json_valid: Valid JSON output</li> <li>latency: Response time limits</li> </ul>"},{"location":"evals/overview/#tool-assertions","title":"Tool Assertions","text":"<ul> <li>tool_called: Specific tool was invoked</li> <li>tool_args: Tool called with correct arguments</li> <li>tool_sequence: Tools called in order</li> </ul>"},{"location":"evals/overview/#semantic-assertions","title":"Semantic Assertions","text":"<ul> <li>semantic_similarity: Semantic similarity to reference</li> </ul>"},{"location":"evals/overview/#yamljson-support","title":"YAML/JSON Support","text":"<p>Define test suites in YAML or JSON:</p> <pre><code># tests.yaml\nname: Agent Test Suite\ncases:\n  - id: test_greeting\n    name: Greeting Test\n    input:\n      query: \"Say hello\"\n    expected:\n      contains: [\"hello\"]\n    tags: [\"greeting\"]\n\n  - id: test_math\n    name: Math Test\n    input:\n      query: \"What is 5+3?\"\n    expected:\n      contains: [\"8\"]\n    tags: [\"math\"]\n</code></pre> <p>Load and run:</p> <pre><code>from prela.evals import EvalSuite\n\nsuite = EvalSuite.from_yaml(\"tests.yaml\")\nrunner = EvalRunner(suite, my_agent)\nresult = runner.run()\n</code></pre>"},{"location":"evals/overview/#cli-integration","title":"CLI Integration","text":"<p>Run evaluations from the command line:</p> <pre><code># Run test suite\nprela eval run tests.yaml --agent my_agent.py\n\n# With custom reporter\nprela eval run tests.yaml --agent my_agent.py --format junit --output results.xml\n\n# Parallel execution\nprela eval run tests.yaml --agent my_agent.py --parallel --workers 8\n</code></pre>"},{"location":"evals/overview/#cicd-integration","title":"CI/CD Integration","text":""},{"location":"evals/overview/#github-actions","title":"GitHub Actions","text":"<pre><code>name: Run Evaluations\n\non: [push, pull_request]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2\n      - uses: actions/setup-python@v2\n      - run: pip install prela\n\n      - name: Run evaluations\n        run: |\n          prela eval run tests.yaml --agent agent.py --format junit --output results.xml\n\n      - name: Publish test results\n        uses: EnricoMi/publish-unit-test-result-action@v2\n        if: always()\n        with:\n          files: results.xml\n</code></pre>"},{"location":"evals/overview/#gitlab-ci","title":"GitLab CI","text":"<pre><code>test:\n  script:\n    - pip install prela\n    - prela eval run tests.yaml --agent agent.py --format junit --output results.xml\n  artifacts:\n    reports:\n      junit: results.xml\n</code></pre>"},{"location":"evals/overview/#observability-integration","title":"Observability Integration","text":"<p>Trace test execution with Prela's tracer:</p> <pre><code>import prela\nfrom prela.evals import EvalRunner\n\n# Initialize tracer\ntracer = prela.init(service_name=\"eval-runner\", exporter=\"console\")\n\n# Run with tracing\nrunner = EvalRunner(suite, my_agent, tracer=tracer)\nresult = runner.run()\n\n# Each test case execution is captured as a span\n# View complete trace of agent behavior during tests\n</code></pre>"},{"location":"evals/overview/#best-practices","title":"Best Practices","text":""},{"location":"evals/overview/#1-organize-tests-by-feature","title":"1. Organize Tests by Feature","text":"<pre><code>qa_suite = EvalSuite(name=\"QA Tests\", cases=qa_cases, tags=[\"qa\"])\nrag_suite = EvalSuite(name=\"RAG Tests\", cases=rag_cases, tags=[\"rag\"])\ntool_suite = EvalSuite(name=\"Tool Tests\", cases=tool_cases, tags=[\"tools\"])\n</code></pre>"},{"location":"evals/overview/#2-use-tags-for-filtering","title":"2. Use Tags for Filtering","text":"<pre><code>case = EvalCase(\n    id=\"test_1\",\n    tags=[\"smoke\", \"critical\", \"qa\"]\n)\n\n# Filter by tags (future feature)\n# runner.run(tags=[\"smoke\"])\n</code></pre>"},{"location":"evals/overview/#3-set-appropriate-timeouts","title":"3. Set Appropriate Timeouts","text":"<pre><code># Fast operations\ncase = EvalCase(..., timeout_seconds=2.0)\n\n# LLM calls\ncase = EvalCase(..., timeout_seconds=30.0)\n\n# Complex workflows\ncase = EvalCase(..., timeout_seconds=120.0)\n</code></pre>"},{"location":"evals/overview/#4-combine-multiple-assertions","title":"4. Combine Multiple Assertions","text":"<pre><code>case = EvalCase(\n    id=\"test_comprehensive\",\n    expected=EvalExpected(\n        contains=[\"result\"],\n        not_contains=[\"error\"]\n    ),\n    assertions=[\n        {\"type\": \"latency\", \"max_ms\": 5000},\n        {\"type\": \"json_valid\"},\n        {\"type\": \"length\", \"min\": 10, \"max\": 500},\n    ]\n)\n</code></pre>"},{"location":"evals/overview/#5-use-setupteardown","title":"5. Use Setup/Teardown","text":"<pre><code>def setup():\n    \"\"\"Initialize test environment.\"\"\"\n    db.connect()\n    cache.clear()\n\ndef teardown():\n    \"\"\"Clean up after tests.\"\"\"\n    db.disconnect()\n\nsuite = EvalSuite(\n    name=\"Integration Tests\",\n    cases=cases,\n    setup=setup,\n    teardown=teardown\n)\n</code></pre>"},{"location":"evals/overview/#example-workflows","title":"Example Workflows","text":""},{"location":"evals/overview/#regression-testing","title":"Regression Testing","text":"<pre><code># Define expected behaviors\nregression_cases = [\n    EvalCase(\n        id=f\"regression_{i}\",\n        input=EvalInput(query=query),\n        expected=EvalExpected(output=known_good_output)\n    )\n    for i, (query, known_good_output) in enumerate(test_data)\n]\n\nsuite = EvalSuite(name=\"Regression Tests\", cases=regression_cases)\nresult = EvalRunner(suite, agent).run()\n\n# Fail CI if regressions detected\nassert result.pass_rate == 1.0, f\"Regressions detected: {result.failed} failures\"\n</code></pre>"},{"location":"evals/overview/#ab-testing","title":"A/B Testing","text":"<pre><code># Test two agent versions\nresult_v1 = EvalRunner(suite, agent_v1).run()\nresult_v2 = EvalRunner(suite, agent_v2).run()\n\nprint(f\"V1 pass rate: {result_v1.pass_rate:.1%}\")\nprint(f\"V2 pass rate: {result_v2.pass_rate:.1%}\")\n\n# Deploy version with better performance\n</code></pre>"},{"location":"evals/overview/#performance-benchmarking","title":"Performance Benchmarking","text":"<pre><code># Add latency assertions to all cases\nsuite.default_assertions = [\n    {\"type\": \"latency\", \"max_ms\": 2000}\n]\n\nresult = EvalRunner(suite, agent, parallel=True, max_workers=10).run()\n\n# Analyze performance\nfor case_result in result.case_results:\n    print(f\"{case_result.case_name}: {case_result.duration_ms:.0f}ms\")\n</code></pre>"},{"location":"evals/overview/#next-steps","title":"Next Steps","text":"<ul> <li>Learn Writing Tests</li> <li>Explore Assertions</li> <li>See Running Evaluations</li> <li>Set up CI Integration</li> </ul>"},{"location":"evals/running/","title":"Running Evaluations","text":"<p>This guide covers how to execute evaluation suites, configure runners, and analyze results.</p>"},{"location":"evals/running/#basic-execution","title":"Basic Execution","text":"<pre><code>from prela.evals import EvalSuite, EvalRunner\n\n# Load suite\nsuite = EvalSuite.from_yaml(\"tests.yaml\")\n\n# Define agent\ndef my_agent(input_data):\n    query = input_data.get(\"query\", \"\")\n    return f\"Response to: {query}\"\n\n# Run\nrunner = EvalRunner(suite, my_agent)\nresult = runner.run()\n\n# View results\nprint(result.summary())\n</code></pre>"},{"location":"evals/running/#sequential-execution","title":"Sequential Execution","text":"<p>Default mode runs tests one at a time:</p> <pre><code>runner = EvalRunner(suite, my_agent, parallel=False)\nresult = runner.run()\n</code></pre> <p>Advantages: - Predictable order - Easier debugging - Lower resource usage</p>"},{"location":"evals/running/#parallel-execution","title":"Parallel Execution","text":"<p>Run tests concurrently for speed:</p> <pre><code>runner = EvalRunner(\n    suite=suite,\n    agent_fn=my_agent,\n    parallel=True,\n    max_workers=4  # Number of parallel workers\n)\nresult = runner.run()\n</code></pre> <p>Advantages: - Faster execution - Better resource utilization - Scales with CPU cores</p> <p>Considerations: - Tests must be independent - Higher memory usage - Non-deterministic order</p>"},{"location":"evals/running/#with-tracer-integration","title":"With Tracer Integration","text":"<p>Capture execution traces:</p> <pre><code>import prela\n\n# Initialize tracer\ntracer = prela.init(service_name=\"eval\", exporter=\"console\")\n\n# Run with tracing\nrunner = EvalRunner(suite, my_agent, tracer=tracer)\nresult = runner.run()\n\n# Each test execution captured as span\n</code></pre>"},{"location":"evals/running/#progress-callbacks","title":"Progress Callbacks","text":"<p>Monitor progress in real-time:</p> <pre><code>def on_case_complete(case_result):\n    status = \"\u2713\" if case_result.passed else \"\u2717\"\n    print(f\"{status} {case_result.case_name} ({case_result.duration_ms:.0f}ms)\")\n\nrunner = EvalRunner(\n    suite, my_agent,\n    on_case_complete=on_case_complete\n)\nresult = runner.run()\n</code></pre>"},{"location":"evals/running/#results","title":"Results","text":""},{"location":"evals/running/#evalrunresult","title":"EvalRunResult","text":"<pre><code>result = runner.run()\n\nprint(f\"Total: {result.total}\")\nprint(f\"Passed: {result.passed}\")\nprint(f\"Failed: {result.failed}\")\nprint(f\"Pass Rate: {result.pass_rate:.1%}\")\nprint(f\"Duration: {result.duration_ms:.0f}ms\")\n\n# Individual cases\nfor case_result in result.case_results:\n    print(f\"{case_result.case_name}: {case_result.passed}\")\n</code></pre>"},{"location":"evals/running/#summary-output","title":"Summary Output","text":"<pre><code>print(result.summary())\n# Evaluation Suite: My Tests\n# Total Cases: 10\n# Passed: 8 (80.0%)\n# Failed: 2 (20.0%)\n#\n# Case Results:\n#   \u2713 test_1 (123ms)\n#   \u2713 test_2 (456ms)\n#   \u2717 test_3 (789ms)\n#     - Assertion failed: Expected to contain 'result'\n</code></pre>"},{"location":"evals/running/#reporting","title":"Reporting","text":""},{"location":"evals/running/#console-reporter","title":"Console Reporter","text":"<pre><code>from prela.evals.reporters import ConsoleReporter\n\n# Minimal output\nConsoleReporter(verbosity=\"minimal\").report(result)\n\n# Normal output (default)\nConsoleReporter(verbosity=\"normal\").report(result)\n\n# Verbose output\nConsoleReporter(verbosity=\"verbose\").report(result)\n</code></pre>"},{"location":"evals/running/#json-reporter","title":"JSON Reporter","text":"<pre><code>from prela.evals.reporters import JSONReporter\n\nJSONReporter(\"results.json\").report(result)\n\n# Pretty print\nJSONReporter(\"results.json\", indent=2).report(result)\n</code></pre>"},{"location":"evals/running/#junit-reporter","title":"JUnit Reporter","text":"<pre><code>from prela.evals.reporters import JUnitReporter\n\nJUnitReporter(\"junit.xml\").report(result)\n</code></pre>"},{"location":"evals/running/#multiple-reporters","title":"Multiple Reporters","text":"<pre><code>reporters = [\n    ConsoleReporter(verbosity=\"normal\"),\n    JSONReporter(\"results.json\"),\n    JUnitReporter(\"junit.xml\")\n]\n\nfor reporter in reporters:\n    reporter.report(result)\n</code></pre>"},{"location":"evals/running/#cli-usage","title":"CLI Usage","text":"<pre><code># Basic run\nprela eval run tests.yaml --agent agent.py\n\n# With specific format\nprela eval run tests.yaml --agent agent.py --format junit --output results.xml\n\n# Parallel execution\nprela eval run tests.yaml --agent agent.py --parallel --workers 8\n\n# With tracer\nprela eval run tests.yaml --agent agent.py --trace --exporter console\n</code></pre>"},{"location":"evals/running/#best-practices","title":"Best Practices","text":""},{"location":"evals/running/#1-use-parallel-for-large-suites","title":"1. Use Parallel for Large Suites","text":"<pre><code># Small suite (&lt;10 tests): sequential\nrunner = EvalRunner(suite, agent, parallel=False)\n\n# Large suite (&gt;50 tests): parallel\nrunner = EvalRunner(suite, agent, parallel=True, max_workers=8)\n</code></pre>"},{"location":"evals/running/#2-monitor-progress","title":"2. Monitor Progress","text":"<pre><code>def progress(case_result):\n    print(f\"[{datetime.now()}] Completed: {case_result.case_name}\")\n\nrunner = EvalRunner(suite, agent, on_case_complete=progress)\n</code></pre>"},{"location":"evals/running/#3-save-results","title":"3. Save Results","text":"<pre><code>result = runner.run()\n\n# Save for analysis\nJSONReporter(\"results/eval_{datetime.now().isoformat()}.json\").report(result)\n</code></pre>"},{"location":"evals/running/#4-handle-failures","title":"4. Handle Failures","text":"<pre><code>result = runner.run()\n\nif result.failed &gt; 0:\n    print(f\"\u26a0\ufe0f {result.failed} test(s) failed\")\n\n    for case_result in result.case_results:\n        if not case_result.passed:\n            print(f\"\\nFailed: {case_result.case_name}\")\n            for assertion_result in case_result.assertion_results:\n                if not assertion_result.passed:\n                    print(f\"  - {assertion_result.message}\")\n</code></pre>"},{"location":"evals/running/#next-steps","title":"Next Steps","text":"<ul> <li>See CI Integration for automation</li> <li>Learn Writing Tests</li> <li>Explore Assertions</li> </ul>"},{"location":"evals/writing-tests/","title":"Writing Tests","text":"<p>This guide shows how to write effective test cases for your AI agents using Prela's evaluation framework.</p>"},{"location":"evals/writing-tests/#basic-test-case","title":"Basic Test Case","text":"<pre><code>from prela.evals import EvalCase, EvalInput, EvalExpected\n\ncase = EvalCase(\n    id=\"test_simple_qa\",\n    name=\"Simple QA Test\",\n    input=EvalInput(query=\"What is the capital of France?\"),\n    expected=EvalExpected(contains=[\"Paris\"])\n)\n</code></pre>"},{"location":"evals/writing-tests/#test-case-structure","title":"Test Case Structure","text":""},{"location":"evals/writing-tests/#required-fields","title":"Required Fields","text":"<ul> <li>id: Unique identifier</li> <li>name: Human-readable name</li> <li>input: Input data for the agent</li> </ul>"},{"location":"evals/writing-tests/#optional-fields","title":"Optional Fields","text":"<ul> <li>expected: Expected output patterns</li> <li>assertions: Validation rules</li> <li>tags: Categorization labels</li> <li>timeout_seconds: Maximum execution time</li> <li>description: Detailed test description</li> </ul>"},{"location":"evals/writing-tests/#input-formats","title":"Input Formats","text":""},{"location":"evals/writing-tests/#simple-query","title":"Simple Query","text":"<pre><code>input = EvalInput(query=\"What is Python?\")\n</code></pre>"},{"location":"evals/writing-tests/#with-context","title":"With Context","text":"<pre><code>input = EvalInput(\n    query=\"Summarize this document\",\n    context={\n        \"document\": \"Long text here...\",\n        \"source\": \"report.pdf\"\n    }\n)\n</code></pre>"},{"location":"evals/writing-tests/#with-messages-chat","title":"With Messages (Chat)","text":"<pre><code>input = EvalInput(\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n        {\"role\": \"user\", \"content\": \"Hello!\"}\n    ]\n)\n</code></pre>"},{"location":"evals/writing-tests/#custom-fields","title":"Custom Fields","text":"<pre><code>input = EvalInput(\n    query=\"Process this\",\n    custom_field=\"value\",\n    another_field=123\n)\n</code></pre>"},{"location":"evals/writing-tests/#expected-outputs","title":"Expected Outputs","text":""},{"location":"evals/writing-tests/#contains-text","title":"Contains Text","text":"<pre><code>expected = EvalExpected(contains=[\"success\", \"completed\"])\n</code></pre>"},{"location":"evals/writing-tests/#does-not-contain","title":"Does Not Contain","text":"<pre><code>expected = EvalExpected(not_contains=[\"error\", \"failed\"])\n</code></pre>"},{"location":"evals/writing-tests/#exact-output","title":"Exact Output","text":"<pre><code>expected = EvalExpected(output=\"The answer is 42\")\n</code></pre>"},{"location":"evals/writing-tests/#combined","title":"Combined","text":"<pre><code>expected = EvalExpected(\n    contains=[\"result\"],\n    not_contains=[\"error\"],\n    output=\"Exact expected output\"\n)\n</code></pre>"},{"location":"evals/writing-tests/#assertions","title":"Assertions","text":""},{"location":"evals/writing-tests/#basic-assertions","title":"Basic Assertions","text":"<pre><code>assertions = [\n    {\"type\": \"contains\", \"value\": \"success\"},\n    {\"type\": \"not_contains\", \"value\": \"error\"},\n    {\"type\": \"regex\", \"pattern\": r\"\\\\d+\"},\n]\n</code></pre>"},{"location":"evals/writing-tests/#latency-assertions","title":"Latency Assertions","text":"<pre><code>assertions = [\n    {\"type\": \"latency\", \"max_ms\": 5000}\n]\n</code></pre>"},{"location":"evals/writing-tests/#json-validation","title":"JSON Validation","text":"<pre><code>assertions = [\n    {\"type\": \"json_valid\"},\n    {\"type\": \"contains\", \"value\": \"result\"}\n]\n</code></pre>"},{"location":"evals/writing-tests/#tool-usage","title":"Tool Usage","text":"<pre><code>assertions = [\n    {\"type\": \"tool_called\", \"tool_name\": \"search\"},\n    {\"type\": \"tool_args\", \"tool_name\": \"calculator\", \"args\": {\"x\": 5}}\n]\n</code></pre>"},{"location":"evals/writing-tests/#semantic-similarity","title":"Semantic Similarity","text":"<pre><code>assertions = [\n    {\n        \"type\": \"semantic_similarity\",\n        \"threshold\": 0.8,\n        \"reference\": \"The capital of France is Paris\"\n    }\n]\n</code></pre>"},{"location":"evals/writing-tests/#complete-examples","title":"Complete Examples","text":""},{"location":"evals/writing-tests/#qa-test","title":"QA Test","text":"<pre><code>from prela.evals import EvalCase, EvalInput, EvalExpected\n\nqa_test = EvalCase(\n    id=\"qa_geography\",\n    name=\"Geography Question\",\n    input=EvalInput(query=\"What is the capital of Japan?\"),\n    expected=EvalExpected(\n        contains=[\"Tokyo\"],\n        not_contains=[\"error\", \"don't know\"]\n    ),\n    assertions=[\n        {\"type\": \"latency\", \"max_ms\": 3000},\n        {\"type\": \"length\", \"min\": 5, \"max\": 200}\n    ],\n    tags=[\"qa\", \"geography\"],\n    timeout_seconds=10.0\n)\n</code></pre>"},{"location":"evals/writing-tests/#rag-test","title":"RAG Test","text":"<pre><code>rag_test = EvalCase(\n    id=\"rag_summarization\",\n    name=\"Document Summarization\",\n    input=EvalInput(\n        query=\"Summarize the key findings\",\n        context={\n            \"documents\": [\n                \"Doc 1: Revenue increased 15%...\",\n                \"Doc 2: Customer satisfaction at 92%...\"\n            ]\n        }\n    ),\n    expected=EvalExpected(\n        contains=[\"revenue\", \"15%\", \"customer satisfaction\"],\n        not_contains=[\"error\"]\n    ),\n    assertions=[\n        {\"type\": \"latency\", \"max_ms\": 10000},\n        {\"type\": \"length\", \"min\": 50, \"max\": 500},\n        {\n            \"type\": \"semantic_similarity\",\n            \"threshold\": 0.7,\n            \"reference\": \"Revenue grew 15% and customer satisfaction is high\"\n        }\n    ],\n    tags=[\"rag\", \"summarization\"]\n)\n</code></pre>"},{"location":"evals/writing-tests/#tool-use-test","title":"Tool Use Test","text":"<pre><code>tool_test = EvalCase(\n    id=\"tool_calculator\",\n    name=\"Calculator Tool Usage\",\n    input=EvalInput(query=\"What is 25 * 4 + 10?\"),\n    expected=EvalExpected(contains=[\"110\"]),\n    assertions=[\n        {\"type\": \"tool_called\", \"tool_name\": \"calculator\"},\n        {\"type\": \"tool_args\", \"tool_name\": \"calculator\", \"args\": {\"expression\": \"25 * 4 + 10\"}},\n        {\"type\": \"contains\", \"value\": \"110\"}\n    ],\n    tags=[\"tools\", \"math\"]\n)\n</code></pre>"},{"location":"evals/writing-tests/#yaml-format","title":"YAML Format","text":""},{"location":"evals/writing-tests/#single-test","title":"Single Test","text":"<pre><code># test.yaml\nid: test_greeting\nname: Greeting Test\ninput:\n  query: \"Say hello\"\nexpected:\n  contains: [\"hello\", \"hi\"]\n  not_contains: [\"error\"]\ntags: [\"greeting\", \"basic\"]\ntimeout_seconds: 5.0\n</code></pre>"},{"location":"evals/writing-tests/#multiple-tests","title":"Multiple Tests","text":"<pre><code># suite.yaml\nname: My Test Suite\ncases:\n  - id: test_1\n    name: First Test\n    input:\n      query: \"Question 1\"\n    expected:\n      contains: [\"answer\"]\n\n  - id: test_2\n    name: Second Test\n    input:\n      query: \"Question 2\"\n    expected:\n      contains: [\"response\"]\n    assertions:\n      - type: latency\n        max_ms: 2000\n</code></pre>"},{"location":"evals/writing-tests/#with-setupteardown","title":"With Setup/Teardown","text":"<pre><code>name: Integration Tests\nsetup: \"setup_function\"\nteardown: \"cleanup_function\"\ndefault_assertions:\n  - type: latency\n    max_ms: 5000\ncases:\n  - id: test_1\n    name: Test Case 1\n    input:\n      query: \"Test input\"\n</code></pre>"},{"location":"evals/writing-tests/#best-practices","title":"Best Practices","text":""},{"location":"evals/writing-tests/#1-use-descriptive-names","title":"1. Use Descriptive Names","text":"<pre><code># Good\ncase = EvalCase(\n    id=\"qa_geography_capitals_europe\",\n    name=\"European Capitals Geography Quiz\"\n)\n\n# Bad\ncase = EvalCase(id=\"test1\", name=\"Test\")\n</code></pre>"},{"location":"evals/writing-tests/#2-add-multiple-assertions","title":"2. Add Multiple Assertions","text":"<pre><code>case = EvalCase(\n    id=\"comprehensive_test\",\n    expected=EvalExpected(contains=[\"result\"]),\n    assertions=[\n        {\"type\": \"latency\", \"max_ms\": 5000},\n        {\"type\": \"json_valid\"},\n        {\"type\": \"not_contains\", \"value\": \"error\"}\n    ]\n)\n</code></pre>"},{"location":"evals/writing-tests/#3-use-tags-for-organization","title":"3. Use Tags for Organization","text":"<pre><code>case = EvalCase(\n    id=\"test_auth\",\n    tags=[\"auth\", \"critical\", \"smoke\"]\n)\n</code></pre>"},{"location":"evals/writing-tests/#4-set-realistic-timeouts","title":"4. Set Realistic Timeouts","text":"<pre><code># Fast operations\nEvalCase(..., timeout_seconds=2.0)\n\n# LLM calls\nEvalCase(..., timeout_seconds=30.0)\n\n# Complex workflows\nEvalCase(..., timeout_seconds=120.0)\n</code></pre>"},{"location":"evals/writing-tests/#5-test-edge-cases","title":"5. Test Edge Cases","text":"<pre><code>edge_cases = [\n    EvalCase(id=\"empty_input\", input=EvalInput(query=\"\")),\n    EvalCase(id=\"very_long_input\", input=EvalInput(query=\"...\" * 1000)),\n    EvalCase(id=\"special_chars\", input=EvalInput(query=\"!@#$%^&amp;*()\")),\n    EvalCase(id=\"unicode\", input=EvalInput(query=\"\u4f60\u597d\u4e16\u754c \ud83c\udf0d\"))\n]\n</code></pre>"},{"location":"evals/writing-tests/#next-steps","title":"Next Steps","text":"<ul> <li>See Assertions for all assertion types</li> <li>Learn Running Evaluations</li> <li>Explore CI Integration</li> </ul>"},{"location":"examples/basic/","title":"Basic Examples","text":"<p>Simple examples to get started with Prela tracing.</p>"},{"location":"examples/basic/#minimal-setup","title":"Minimal Setup","text":"<pre><code>import prela\n\n# One-line initialization\nprela.init(service_name=\"my-app\")\n\n# Your agent code here - auto-instrumented!\nfrom anthropic import Anthropic\n\nclient = Anthropic()\nresponse = client.messages.create(\n    model=\"claude-sonnet-4-20250514\",\n    max_tokens=1024,\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n\n# Trace automatically captured and exported to console\n</code></pre>"},{"location":"examples/basic/#basic-custom-span","title":"Basic Custom Span","text":"<pre><code>import prela\n\nprela.init(service_name=\"my-app\")\ntracer = prela.get_tracer()\n\n# Create a custom span\nwith tracer.span(\"process_request\", prela.SpanType.AGENT) as span:\n    span.set_attribute(\"user_id\", \"user123\")\n    span.set_attribute(\"request_type\", \"query\")\n\n    # Do work\n    result = process_query(\"What is AI?\")\n\n    span.set_attribute(\"result_length\", len(result))\n</code></pre>"},{"location":"examples/basic/#file-export","title":"File Export","text":"<pre><code>import prela\n\n# Export to file instead of console\nprela.init(\n    service_name=\"my-app\",\n    exporter=\"file\",\n    directory=\"./traces\"\n)\n\n# Traces saved to ./traces/my-app/YYYY-MM-DD/trace_*.jsonl\n</code></pre>"},{"location":"examples/basic/#with-sampling","title":"With Sampling","text":"<pre><code>import prela\n\n# Sample 10% of requests (production)\nprela.init(\n    service_name=\"my-app\",\n    sample_rate=0.1\n)\n\n# Only 10% of traces will be captured\n</code></pre>"},{"location":"examples/basic/#multiple-llm-calls","title":"Multiple LLM Calls","text":"<pre><code>import prela\nfrom openai import OpenAI\n\nprela.init(service_name=\"chatbot\")\n\nclient = OpenAI()\n\n# Parent span for the conversation\nwith prela.get_tracer().span(\"conversation\", prela.SpanType.AGENT) as conv:\n    # First message (auto-traced)\n    response1 = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": \"Tell me a joke\"}]\n    )\n\n    # Second message (auto-traced)\n    response2 = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[\n            {\"role\": \"user\", \"content\": \"Tell me a joke\"},\n            {\"role\": \"assistant\", \"content\": response1.choices[0].message.content},\n            {\"role\": \"user\", \"content\": \"Explain why it's funny\"}\n        ]\n    )\n\n    conv.set_attribute(\"messages_exchanged\", 2)\n</code></pre>"},{"location":"examples/basic/#error-handling","title":"Error Handling","text":"<pre><code>import prela\nfrom openai import OpenAI, APIError\n\nprela.init(service_name=\"my-app\")\ntracer = prela.get_tracer()\n\nclient = OpenAI()\n\nwith tracer.span(\"llm_call\", prela.SpanType.LLM) as span:\n    try:\n        response = client.chat.completions.create(\n            model=\"gpt-4\",\n            messages=[{\"role\": \"user\", \"content\": \"Hello\"}]\n        )\n        span.set_attribute(\"success\", True)\n    except APIError as e:\n        span.set_status(prela.SpanStatus.ERROR, str(e))\n        span.set_attribute(\"error_type\", type(e).__name__)\n        raise\n</code></pre>"},{"location":"examples/basic/#simple-rag","title":"Simple RAG","text":"<pre><code>import prela\nfrom openai import OpenAI\n\nprela.init(service_name=\"rag-app\")\ntracer = prela.get_tracer()\n\nclient = OpenAI()\n\ndef answer_question(question: str, documents: list[str]) -&gt; str:\n    with tracer.span(\"rag_query\", prela.SpanType.AGENT) as span:\n        # Retrieval\n        with tracer.span(\"retrieve\", prela.SpanType.RETRIEVAL) as r_span:\n            r_span.set_attribute(\"query\", question)\n            r_span.set_attribute(\"num_docs\", len(documents))\n            # Assume documents already retrieved\n            context = \"\\n\\n\".join(documents)\n\n        # LLM call (auto-traced)\n        response = client.chat.completions.create(\n            model=\"gpt-4\",\n            messages=[\n                {\"role\": \"system\", \"content\": f\"Context:\\n{context}\"},\n                {\"role\": \"user\", \"content\": question}\n            ]\n        )\n\n        answer = response.choices[0].message.content\n        span.set_attribute(\"answer_length\", len(answer))\n\n        return answer\n\n# Use it\ndocs = [\"Document 1 text\", \"Document 2 text\"]\nanswer = answer_question(\"What is in the documents?\", docs)\n</code></pre>"},{"location":"examples/basic/#environment-variables","title":"Environment Variables","text":"<pre><code># .env\nPRELA_SERVICE_NAME=my-app\nPRELA_EXPORTER=file\nPRELA_TRACE_DIR=./traces\nPRELA_SAMPLE_RATE=1.0\n</code></pre> <pre><code>import prela\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\n# Reads configuration from environment\nprela.init()\n</code></pre>"},{"location":"examples/basic/#async-example","title":"Async Example","text":"<pre><code>import asyncio\nimport prela\nfrom openai import AsyncOpenAI\n\nprela.init(service_name=\"async-app\")\n\nasync def main():\n    client = AsyncOpenAI()\n\n    # Async LLM call (auto-traced)\n    response = await client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": \"Hello async!\"}]\n    )\n\n    print(response.choices[0].message.content)\n\nasyncio.run(main())\n</code></pre>"},{"location":"examples/basic/#next-steps","title":"Next Steps","text":"<ul> <li>Learn Custom Spans</li> <li>Explore Parallel Execution</li> <li>See Production Setup</li> </ul>"},{"location":"examples/custom-spans/","title":"Custom Spans","text":"<p>Advanced patterns for creating custom spans and organizing traces.</p>"},{"location":"examples/custom-spans/#nested-spans","title":"Nested Spans","text":"<pre><code>import prela\n\nprela.init(service_name=\"my-app\")\ntracer = prela.get_tracer()\n\nwith tracer.span(\"parent_operation\", prela.SpanType.AGENT) as parent:\n    parent.set_attribute(\"stage\", \"initialization\")\n\n    # Child span 1\n    with tracer.span(\"load_data\", prela.SpanType.CUSTOM) as child1:\n        child1.set_attribute(\"source\", \"database\")\n        data = load_data()\n\n    # Child span 2\n    with tracer.span(\"process_data\", prela.SpanType.CUSTOM) as child2:\n        child2.set_attribute(\"num_records\", len(data))\n        results = process(data)\n\n    parent.set_attribute(\"total_results\", len(results))\n</code></pre>"},{"location":"examples/custom-spans/#span-attributes","title":"Span Attributes","text":"<pre><code>with tracer.span(\"custom_op\") as span:\n    # Basic attributes\n    span.set_attribute(\"user_id\", \"user123\")\n    span.set_attribute(\"request_id\", \"req456\")\n\n    # Numeric attributes\n    span.set_attribute(\"count\", 42)\n    span.set_attribute(\"latency_ms\", 123.45)\n\n    # Boolean attributes\n    span.set_attribute(\"cached\", True)\n    span.set_attribute(\"success\", False)\n\n    # Complex attributes (auto-serialized)\n    span.set_attribute(\"metadata\", {\n        \"version\": \"1.0\",\n        \"features\": [\"a\", \"b\", \"c\"]\n    })\n</code></pre>"},{"location":"examples/custom-spans/#span-events","title":"Span Events","text":"<pre><code>with tracer.span(\"long_operation\") as span:\n    # Add events for milestones\n    span.add_event(\"started_phase_1\")\n\n    phase1_result = do_phase_1()\n\n    span.add_event(\"completed_phase_1\", {\n        \"duration_ms\": 1000,\n        \"records_processed\": 100\n    })\n\n    span.add_event(\"started_phase_2\")\n\n    phase2_result = do_phase_2()\n\n    span.add_event(\"completed_phase_2\", {\n        \"duration_ms\": 2000,\n        \"records_processed\": 200\n    })\n</code></pre>"},{"location":"examples/custom-spans/#span-types","title":"Span Types","text":"<pre><code># Agent operations\nwith tracer.span(\"agent_reasoning\", prela.SpanType.AGENT):\n    plan = create_plan()\n\n# LLM calls\nwith tracer.span(\"llm_generation\", prela.SpanType.LLM):\n    response = call_llm()\n\n# Tool invocations\nwith tracer.span(\"tool_execution\", prela.SpanType.TOOL):\n    result = call_tool()\n\n# Retrieval operations\nwith tracer.span(\"vector_search\", prela.SpanType.RETRIEVAL):\n    docs = search_vectors()\n\n# Embedding generation\nwith tracer.span(\"embed_text\", prela.SpanType.EMBEDDING):\n    embedding = embed(text)\n\n# Custom operations\nwith tracer.span(\"custom_logic\", prela.SpanType.CUSTOM):\n    output = custom_function()\n</code></pre>"},{"location":"examples/custom-spans/#error-handling","title":"Error Handling","text":"<pre><code>with tracer.span(\"risky_operation\") as span:\n    try:\n        result = risky_function()\n        span.set_attribute(\"result\", result)\n    except ValueError as e:\n        span.set_status(prela.SpanStatus.ERROR, f\"Value error: {e}\")\n        span.set_attribute(\"error_type\", \"ValueError\")\n        span.set_attribute(\"error_details\", str(e))\n        raise\n    except Exception as e:\n        span.set_status(prela.SpanStatus.ERROR, f\"Unexpected error: {e}\")\n        raise\n</code></pre>"},{"location":"examples/custom-spans/#conditional-tracing","title":"Conditional Tracing","text":"<pre><code>def process_request(request, trace=True):\n    \"\"\"Process request with optional tracing.\"\"\"\n    if trace:\n        with tracer.span(\"process_request\") as span:\n            span.set_attribute(\"request_id\", request.id)\n            return _do_process(request)\n    else:\n        return _do_process(request)\n</code></pre>"},{"location":"examples/custom-spans/#trace-correlation","title":"Trace Correlation","text":"<pre><code>from prela.core.context import get_current_trace_id\n\nwith tracer.span(\"external_call\") as span:\n    # Get trace ID for correlation\n    trace_id = get_current_trace_id()\n\n    # Pass to external service\n    response = requests.post(\n        \"https://api.example.com/process\",\n        headers={\"X-Trace-ID\": trace_id},\n        json=data\n    )\n\n    span.set_attribute(\"external_trace_id\", trace_id)\n</code></pre>"},{"location":"examples/custom-spans/#multi-step-pipeline","title":"Multi-Step Pipeline","text":"<pre><code>def run_pipeline(input_data):\n    with tracer.span(\"pipeline\", prela.SpanType.AGENT) as pipeline_span:\n        pipeline_span.set_attribute(\"pipeline_name\", \"data_processing\")\n\n        # Step 1: Validation\n        with tracer.span(\"validate\", prela.SpanType.CUSTOM) as validate_span:\n            validate_span.set_attribute(\"input_size\", len(input_data))\n            validated = validate(input_data)\n            validate_span.set_attribute(\"valid\", validated)\n\n        if not validated:\n            pipeline_span.set_status(prela.SpanStatus.ERROR, \"Validation failed\")\n            return None\n\n        # Step 2: Transform\n        with tracer.span(\"transform\", prela.SpanType.CUSTOM) as transform_span:\n            transformed = transform(input_data)\n            transform_span.set_attribute(\"output_size\", len(transformed))\n\n        # Step 3: Enrich with LLM\n        with tracer.span(\"enrich\", prela.SpanType.LLM) as enrich_span:\n            # LLM call (auto-traced as child)\n            enriched = call_llm(transformed)\n            enrich_span.set_attribute(\"enrichment_applied\", True)\n\n        # Step 4: Finalize\n        with tracer.span(\"finalize\", prela.SpanType.CUSTOM) as final_span:\n            result = finalize(enriched)\n            final_span.set_attribute(\"result_size\", len(result))\n\n        pipeline_span.set_attribute(\"success\", True)\n        return result\n</code></pre>"},{"location":"examples/custom-spans/#tool-usage-tracking","title":"Tool Usage Tracking","text":"<pre><code>def agent_with_tools(query: str):\n    with tracer.span(\"agent_workflow\", prela.SpanType.AGENT) as agent_span:\n        tools_used = []\n\n        # Tool 1: Search\n        with tracer.span(\"tool.search\", prela.SpanType.TOOL) as search_span:\n            search_span.set_attribute(\"tool.name\", \"search\")\n            search_span.set_attribute(\"tool.input\", query)\n            results = search(query)\n            search_span.set_attribute(\"tool.output\", results)\n            tools_used.append(\"search\")\n\n        # Tool 2: Summarize\n        with tracer.span(\"tool.summarize\", prela.SpanType.TOOL) as summ_span:\n            summ_span.set_attribute(\"tool.name\", \"summarize\")\n            summ_span.set_attribute(\"tool.input\", results)\n            summary = summarize(results)\n            summ_span.set_attribute(\"tool.output\", summary)\n            tools_used.append(\"summarize\")\n\n        agent_span.set_attribute(\"tools_used\", tools_used)\n        agent_span.set_attribute(\"num_tools\", len(tools_used))\n\n        return summary\n</code></pre>"},{"location":"examples/custom-spans/#batch-processing","title":"Batch Processing","text":"<pre><code>def process_batch(items: list):\n    with tracer.span(\"batch_processing\", prela.SpanType.AGENT) as batch_span:\n        batch_span.set_attribute(\"batch_size\", len(items))\n\n        results = []\n        for i, item in enumerate(items):\n            with tracer.span(f\"process_item_{i}\", prela.SpanType.CUSTOM) as item_span:\n                item_span.set_attribute(\"item_id\", item.id)\n                item_span.set_attribute(\"batch_index\", i)\n\n                try:\n                    result = process_item(item)\n                    item_span.set_attribute(\"success\", True)\n                    results.append(result)\n                except Exception as e:\n                    item_span.set_status(prela.SpanStatus.ERROR, str(e))\n                    item_span.set_attribute(\"success\", False)\n\n        batch_span.set_attribute(\"successful\", len(results))\n        batch_span.set_attribute(\"failed\", len(items) - len(results))\n\n        return results\n</code></pre>"},{"location":"examples/custom-spans/#retry-logic","title":"Retry Logic","text":"<pre><code>import time\n\ndef call_with_retry(func, max_retries=3):\n    with tracer.span(\"retry_wrapper\", prela.SpanType.CUSTOM) as wrapper_span:\n        wrapper_span.set_attribute(\"max_retries\", max_retries)\n\n        for attempt in range(max_retries):\n            with tracer.span(f\"attempt_{attempt}\", prela.SpanType.CUSTOM) as attempt_span:\n                attempt_span.set_attribute(\"attempt_number\", attempt)\n\n                try:\n                    result = func()\n                    attempt_span.set_attribute(\"success\", True)\n                    wrapper_span.set_attribute(\"attempts_needed\", attempt + 1)\n                    return result\n                except Exception as e:\n                    attempt_span.set_status(prela.SpanStatus.ERROR, str(e))\n                    attempt_span.set_attribute(\"success\", False)\n\n                    if attempt &lt; max_retries - 1:\n                        delay = 2 ** attempt\n                        attempt_span.add_event(\"retrying\", {\"delay_seconds\": delay})\n                        time.sleep(delay)\n                    else:\n                        wrapper_span.set_status(prela.SpanStatus.ERROR, \"All retries failed\")\n                        raise\n</code></pre>"},{"location":"examples/custom-spans/#next-steps","title":"Next Steps","text":"<ul> <li>See Parallel Execution</li> <li>Explore Production Setup</li> <li>Learn about Context Propagation</li> </ul>"},{"location":"examples/parallel/","title":"Parallel Execution","text":"<p>Examples of tracing parallel and concurrent operations.</p>"},{"location":"examples/parallel/#thread-pool-execution","title":"Thread Pool Execution","text":"<pre><code>from concurrent.futures import ThreadPoolExecutor, as_completed\nimport prela\nfrom prela.core.context import copy_context_to_thread\n\nprela.init(service_name=\"parallel-app\")\ntracer = prela.get_tracer()\n\ndef process_item(item_id):\n    \"\"\"Process a single item (runs in worker thread).\"\"\"\n    with tracer.span(f\"process_{item_id}\", prela.SpanType.CUSTOM) as span:\n        span.set_attribute(\"item_id\", item_id)\n        # Processing logic\n        return f\"Result for {item_id}\"\n\n# Create parent span\nwith tracer.span(\"batch_processing\", prela.SpanType.AGENT) as parent:\n    parent.set_attribute(\"batch_size\", 10)\n\n    # Wrap function INSIDE parent span context\n    wrapped_process = copy_context_to_thread(process_item)\n\n    # Submit to thread pool\n    with ThreadPoolExecutor(max_workers=4) as executor:\n        futures = [\n            executor.submit(wrapped_process, i)\n            for i in range(10)\n        ]\n\n        # Collect results\n        results = []\n        for future in as_completed(futures):\n            result = future.result()\n            results.append(result)\n\n    parent.set_attribute(\"results_count\", len(results))\n\n# All child spans properly linked to parent\n</code></pre>"},{"location":"examples/parallel/#async-concurrent-operations","title":"Async Concurrent Operations","text":"<pre><code>import asyncio\nimport prela\nfrom openai import AsyncOpenAI\n\nprela.init(service_name=\"async-app\")\ntracer = prela.get_tracer()\n\nasync def fetch_completion(prompt: str, index: int):\n    \"\"\"Fetch completion asynchronously.\"\"\"\n    with tracer.span(f\"completion_{index}\", prela.SpanType.LLM) as span:\n        span.set_attribute(\"prompt_index\", index)\n        client = AsyncOpenAI()\n\n        response = await client.chat.completions.create(\n            model=\"gpt-4\",\n            messages=[{\"role\": \"user\", \"content\": prompt}]\n        )\n\n        return response.choices[0].message.content\n\nasync def process_batch(prompts: list[str]):\n    \"\"\"Process multiple prompts concurrently.\"\"\"\n    with tracer.span(\"async_batch\", prela.SpanType.AGENT) as batch_span:\n        batch_span.set_attribute(\"num_prompts\", len(prompts))\n\n        # Create tasks\n        tasks = [\n            fetch_completion(prompt, i)\n            for i, prompt in enumerate(prompts)\n        ]\n\n        # Run concurrently\n        results = await asyncio.gather(*tasks)\n\n        batch_span.set_attribute(\"results_count\", len(results))\n        return results\n\n# Run\nprompts = [\"Prompt 1\", \"Prompt 2\", \"Prompt 3\"]\nresults = asyncio.run(process_batch(prompts))\n</code></pre>"},{"location":"examples/parallel/#parallel-with-different-span-types","title":"Parallel with Different Span Types","text":"<pre><code>from concurrent.futures import ThreadPoolExecutor\nfrom prela.core.context import copy_context_to_thread\n\ndef search_database(query: str):\n    with tracer.span(\"database_search\", prela.SpanType.RETRIEVAL) as span:\n        span.set_attribute(\"query\", query)\n        # Search logic\n        return [\"result1\", \"result2\"]\n\ndef call_llm(prompt: str):\n    with tracer.span(\"llm_call\", prela.SpanType.LLM) as span:\n        span.set_attribute(\"prompt\", prompt)\n        # LLM call\n        return \"LLM response\"\n\ndef execute_tool(tool_name: str):\n    with tracer.span(f\"tool_{tool_name}\", prela.SpanType.TOOL) as span:\n        span.set_attribute(\"tool.name\", tool_name)\n        # Tool execution\n        return f\"{tool_name} result\"\n\nwith tracer.span(\"parallel_operations\", prela.SpanType.AGENT) as parent:\n    # Wrap functions\n    wrapped_search = copy_context_to_thread(search_database)\n    wrapped_llm = copy_context_to_thread(call_llm)\n    wrapped_tool = copy_context_to_thread(execute_tool)\n\n    # Execute in parallel\n    with ThreadPoolExecutor() as executor:\n        search_future = executor.submit(wrapped_search, \"user query\")\n        llm_future = executor.submit(wrapped_llm, \"generate text\")\n        tool_future = executor.submit(wrapped_tool, \"calculator\")\n\n        # Collect results\n        search_results = search_future.result()\n        llm_response = llm_future.result()\n        tool_result = tool_future.result()\n\n    parent.set_attribute(\"operations_completed\", 3)\n</code></pre>"},{"location":"examples/parallel/#map-reduce-pattern","title":"Map-Reduce Pattern","text":"<pre><code>from concurrent.futures import ThreadPoolExecutor\nfrom prela.core.context import copy_context_to_thread\n\ndef map_function(item):\n    \"\"\"Map step (parallel).\"\"\"\n    with tracer.span(f\"map_{item}\", prela.SpanType.CUSTOM) as span:\n        span.set_attribute(\"input\", item)\n        result = item * 2  # Transform\n        span.set_attribute(\"output\", result)\n        return result\n\ndef reduce_function(results):\n    \"\"\"Reduce step (sequential).\"\"\"\n    with tracer.span(\"reduce\", prela.SpanType.CUSTOM) as span:\n        span.set_attribute(\"input_count\", len(results))\n        total = sum(results)\n        span.set_attribute(\"total\", total)\n        return total\n\nwith tracer.span(\"map_reduce\", prela.SpanType.AGENT) as parent:\n    items = [1, 2, 3, 4, 5]\n\n    # Map phase (parallel)\n    with tracer.span(\"map_phase\", prela.SpanType.CUSTOM) as map_span:\n        wrapped_map = copy_context_to_thread(map_function)\n\n        with ThreadPoolExecutor(max_workers=3) as executor:\n            map_results = list(executor.map(wrapped_map, items))\n\n        map_span.set_attribute(\"results_count\", len(map_results))\n\n    # Reduce phase (sequential)\n    with tracer.span(\"reduce_phase\", prela.SpanType.CUSTOM):\n        final_result = reduce_function(map_results)\n\n    parent.set_attribute(\"final_result\", final_result)\n</code></pre>"},{"location":"examples/parallel/#async-streaming","title":"Async Streaming","text":"<pre><code>import asyncio\nfrom openai import AsyncOpenAI\n\nasync def stream_completion(prompt: str):\n    \"\"\"Stream completion asynchronously.\"\"\"\n    with tracer.span(\"streaming_completion\", prela.SpanType.LLM) as span:\n        client = AsyncOpenAI()\n\n        stream = await client.chat.completions.create(\n            model=\"gpt-4\",\n            messages=[{\"role\": \"user\", \"content\": prompt}],\n            stream=True\n        )\n\n        chunks = []\n        async for chunk in stream:\n            if chunk.choices[0].delta.content:\n                chunks.append(chunk.choices[0].delta.content)\n\n        full_response = \"\".join(chunks)\n        span.set_attribute(\"response_length\", len(full_response))\n        return full_response\n\nasync def process_multiple_streams(prompts: list[str]):\n    \"\"\"Process multiple streaming completions concurrently.\"\"\"\n    with tracer.span(\"multi_stream\", prela.SpanType.AGENT) as span:\n        span.set_attribute(\"num_streams\", len(prompts))\n\n        # Create streaming tasks\n        tasks = [stream_completion(prompt) for prompt in prompts]\n\n        # Run all streams concurrently\n        results = await asyncio.gather(*tasks)\n\n        span.set_attribute(\"total_responses\", len(results))\n        return results\n\n# Run\nprompts = [\"Question 1\", \"Question 2\", \"Question 3\"]\nresults = asyncio.run(process_multiple_streams(prompts))\n</code></pre>"},{"location":"examples/parallel/#fan-out-fan-in","title":"Fan-Out Fan-In","text":"<pre><code>from concurrent.futures import ThreadPoolExecutor, as_completed\nfrom prela.core.context import copy_context_to_thread\n\ndef query_source(source_name: str, query: str):\n    \"\"\"Query a single data source.\"\"\"\n    with tracer.span(f\"query_{source_name}\", prela.SpanType.RETRIEVAL) as span:\n        span.set_attribute(\"source\", source_name)\n        span.set_attribute(\"query\", query)\n        # Simulate data retrieval\n        return [f\"{source_name}_result_{i}\" for i in range(3)]\n\ndef aggregate_results(all_results: list):\n    \"\"\"Aggregate results from all sources.\"\"\"\n    with tracer.span(\"aggregate\", prela.SpanType.CUSTOM) as span:\n        flat_results = [item for sublist in all_results for item in sublist]\n        span.set_attribute(\"total_items\", len(flat_results))\n        return flat_results\n\nwith tracer.span(\"fan_out_fan_in\", prela.SpanType.AGENT) as parent:\n    query = \"search term\"\n    sources = [\"database\", \"api\", \"cache\"]\n\n    # Fan-out: Query all sources in parallel\n    with tracer.span(\"fan_out\", prela.SpanType.CUSTOM) as fan_out_span:\n        wrapped_query = copy_context_to_thread(query_source)\n\n        with ThreadPoolExecutor(max_workers=len(sources)) as executor:\n            futures = {\n                executor.submit(wrapped_query, source, query): source\n                for source in sources\n            }\n\n            all_results = []\n            for future in as_completed(futures):\n                source = futures[future]\n                try:\n                    result = future.result()\n                    all_results.append(result)\n                except Exception as e:\n                    fan_out_span.add_event(f\"{source}_failed\", {\"error\": str(e)})\n\n        fan_out_span.set_attribute(\"sources_queried\", len(sources))\n        fan_out_span.set_attribute(\"successful_queries\", len(all_results))\n\n    # Fan-in: Aggregate all results\n    with tracer.span(\"fan_in\", prela.SpanType.CUSTOM):\n        final_results = aggregate_results(all_results)\n\n    parent.set_attribute(\"final_count\", len(final_results))\n</code></pre>"},{"location":"examples/parallel/#rate-limited-parallel-execution","title":"Rate-Limited Parallel Execution","text":"<pre><code>import time\nfrom concurrent.futures import ThreadPoolExecutor\nfrom prela.core.context import copy_context_to_thread\n\nclass RateLimiter:\n    def __init__(self, calls_per_second):\n        self.calls_per_second = calls_per_second\n        self.last_call = 0\n\n    def wait(self):\n        now = time.time()\n        time_since_last = now - self.last_call\n        min_interval = 1.0 / self.calls_per_second\n\n        if time_since_last &lt; min_interval:\n            time.sleep(min_interval - time_since_last)\n\n        self.last_call = time.time()\n\nlimiter = RateLimiter(calls_per_second=5)\n\ndef rate_limited_operation(item_id):\n    \"\"\"Operation with rate limiting.\"\"\"\n    limiter.wait()\n\n    with tracer.span(f\"operation_{item_id}\", prela.SpanType.CUSTOM) as span:\n        span.set_attribute(\"item_id\", item_id)\n        # Operation logic\n        return f\"Result {item_id}\"\n\nwith tracer.span(\"rate_limited_batch\", prela.SpanType.AGENT) as parent:\n    wrapped_op = copy_context_to_thread(rate_limited_operation)\n\n    with ThreadPoolExecutor(max_workers=10) as executor:\n        futures = [executor.submit(wrapped_op, i) for i in range(20)]\n        results = [f.result() for f in futures]\n\n    parent.set_attribute(\"completed\", len(results))\n</code></pre>"},{"location":"examples/parallel/#next-steps","title":"Next Steps","text":"<ul> <li>See Production Setup</li> <li>Learn about Context Propagation</li> <li>Explore Custom Spans</li> </ul>"},{"location":"examples/production/","title":"Production Setup","text":"<p>Best practices and examples for running Prela in production.</p>"},{"location":"examples/production/#production-configuration","title":"Production Configuration","text":"<pre><code>import prela\nimport os\n\n# Production initialization\ntracer = prela.init(\n    service_name=os.getenv(\"SERVICE_NAME\", \"prod-agent\"),\n    exporter=\"file\",\n    directory=\"/var/log/traces\",\n    sample_rate=0.05,  # 5% sampling for cost control\n    max_file_size_mb=500,\n    rotate=True\n)\n</code></pre>"},{"location":"examples/production/#environment-based-configuration","title":"Environment-Based Configuration","text":"<pre><code># config.py\nimport os\nfrom prela import init\n\ndef get_tracer():\n    \"\"\"Get tracer based on environment.\"\"\"\n    env = os.getenv(\"ENVIRONMENT\", \"development\")\n\n    if env == \"production\":\n        return init(\n            service_name=\"prod-agent\",\n            exporter=\"file\",\n            directory=\"/var/log/traces\",\n            sample_rate=0.01,  # 1% sampling\n            max_file_size_mb=1000,\n            rotate=True\n        )\n    elif env == \"staging\":\n        return init(\n            service_name=\"staging-agent\",\n            exporter=\"file\",\n            directory=\"./traces\",\n            sample_rate=0.25,  # 25% sampling\n            max_file_size_mb=100,\n            rotate=True\n        )\n    else:  # development\n        return init(\n            service_name=\"dev-agent\",\n            exporter=\"console\",\n            sample_rate=1.0,  # 100% sampling\n            verbosity=\"verbose\"\n        )\n\n# Use in your app\ntracer = get_tracer()\n</code></pre>"},{"location":"examples/production/#sampling-strategy","title":"Sampling Strategy","text":"<pre><code>from prela.core.sampler import RateLimitingSampler\n\n# Rate limiting for high-traffic production\ntracer = prela.init(\n    service_name=\"high-traffic-agent\",\n    sampler=RateLimitingSampler(traces_per_second=10.0)\n)\n\n# Probability-based for moderate traffic\ntracer = prela.init(\n    service_name=\"moderate-agent\",\n    sample_rate=0.1  # 10% sampling\n)\n</code></pre>"},{"location":"examples/production/#error-monitoring","title":"Error Monitoring","text":"<pre><code>import logging\nimport prela\n\n# Setup logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n)\n\nlogger = logging.getLogger(__name__)\n\ntracer = prela.init(service_name=\"prod-agent\")\n\ndef process_request(request):\n    \"\"\"Process request with comprehensive error handling.\"\"\"\n    with tracer.span(\"process_request\", prela.SpanType.AGENT) as span:\n        span.set_attribute(\"request_id\", request.id)\n        span.set_attribute(\"user_id\", request.user_id)\n\n        try:\n            # Processing logic\n            result = process(request)\n            span.set_attribute(\"success\", True)\n            return result\n\n        except ValueError as e:\n            # Business logic error\n            logger.warning(f\"Validation error: {e}\", extra={\n                \"request_id\": request.id,\n                \"trace_id\": span.trace_id\n            })\n            span.set_status(prela.SpanStatus.ERROR, f\"Validation: {e}\")\n            span.set_attribute(\"error_type\", \"validation\")\n            raise\n\n        except Exception as e:\n            # Unexpected error\n            logger.error(f\"Unexpected error: {e}\", extra={\n                \"request_id\": request.id,\n                \"trace_id\": span.trace_id\n            }, exc_info=True)\n            span.set_status(prela.SpanStatus.ERROR, f\"Internal: {e}\")\n            span.set_attribute(\"error_type\", \"internal\")\n            raise\n</code></pre>"},{"location":"examples/production/#performance-monitoring","title":"Performance Monitoring","text":"<pre><code>import time\nimport prela\n\ntracer = prela.init(service_name=\"prod-agent\")\n\ndef monitor_performance(operation_name):\n    \"\"\"Decorator to monitor operation performance.\"\"\"\n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            with tracer.span(operation_name, prela.SpanType.CUSTOM) as span:\n                start = time.time()\n\n                try:\n                    result = func(*args, **kwargs)\n                    duration = (time.time() - start) * 1000\n\n                    span.set_attribute(\"duration_ms\", duration)\n                    span.set_attribute(\"success\", True)\n\n                    # Alert on slow operations\n                    if duration &gt; 5000:  # 5 seconds\n                        logger.warning(f\"Slow operation: {operation_name} took {duration:.0f}ms\")\n\n                    return result\n\n                except Exception as e:\n                    duration = (time.time() - start) * 1000\n                    span.set_attribute(\"duration_ms\", duration)\n                    span.set_attribute(\"success\", False)\n                    raise\n\n        return wrapper\n    return decorator\n\n@monitor_performance(\"critical_operation\")\ndef critical_operation(data):\n    # Critical business logic\n    return process_data(data)\n</code></pre>"},{"location":"examples/production/#graceful-degradation","title":"Graceful Degradation","text":"<pre><code>import prela\n\ndef init_with_fallback():\n    \"\"\"Initialize tracer with graceful fallback.\"\"\"\n    try:\n        return prela.init(\n            service_name=\"prod-agent\",\n            exporter=\"file\",\n            directory=\"/var/log/traces\"\n        )\n    except Exception as e:\n        logger.error(f\"Failed to initialize tracer: {e}\")\n        # Fallback: disable tracing\n        return prela.init(\n            service_name=\"prod-agent\",\n            sample_rate=0.0  # Disable tracing\n        )\n\ntracer = init_with_fallback()\n</code></pre>"},{"location":"examples/production/#resource-management","title":"Resource Management","text":"<pre><code>import prela\nfrom contextlib import contextmanager\n\n@contextmanager\ndef traced_operation(name, **attributes):\n    \"\"\"Context manager for traced operations with cleanup.\"\"\"\n    span = None\n    try:\n        with tracer.span(name) as span:\n            for key, value in attributes.items():\n                span.set_attribute(key, value)\n            yield span\n    except Exception as e:\n        if span:\n            span.set_status(prela.SpanStatus.ERROR, str(e))\n        raise\n    finally:\n        # Cleanup logic\n        pass\n\n# Usage\nwith traced_operation(\"database_query\", table=\"users\", operation=\"select\"):\n    results = db.query(\"SELECT * FROM users\")\n</code></pre>"},{"location":"examples/production/#multi-tenant-support","title":"Multi-Tenant Support","text":"<pre><code>import prela\nfrom prela.core.context import get_trace_context\n\ndef process_tenant_request(tenant_id, request):\n    \"\"\"Process request with tenant isolation.\"\"\"\n    with tracer.span(\"tenant_request\", prela.SpanType.AGENT) as span:\n        # Add tenant context\n        span.set_attribute(\"tenant_id\", tenant_id)\n        span.set_attribute(\"request_id\", request.id)\n\n        # Store in baggage for child spans\n        ctx = get_trace_context()\n        ctx.baggage[\"tenant_id\"] = tenant_id\n        ctx.baggage[\"environment\"] = \"production\"\n\n        # Process request\n        result = process(request)\n\n        span.set_attribute(\"result_size\", len(result))\n        return result\n</code></pre>"},{"location":"examples/production/#health-checks","title":"Health Checks","text":"<pre><code>import prela\n\ntracer = prela.init(service_name=\"prod-agent\")\n\ndef health_check():\n    \"\"\"Health check endpoint.\"\"\"\n    with tracer.span(\"health_check\", prela.SpanType.CUSTOM) as span:\n        checks = {}\n\n        # Check database\n        try:\n            db.ping()\n            checks[\"database\"] = \"healthy\"\n        except Exception as e:\n            checks[\"database\"] = f\"unhealthy: {e}\"\n            span.add_event(\"database_check_failed\", {\"error\": str(e)})\n\n        # Check external API\n        try:\n            api.ping()\n            checks[\"api\"] = \"healthy\"\n        except Exception as e:\n            checks[\"api\"] = f\"unhealthy: {e}\"\n            span.add_event(\"api_check_failed\", {\"error\": str(e)})\n\n        # Check tracing\n        checks[\"tracing\"] = \"healthy\"  # If we got here, tracing works\n\n        all_healthy = all(v == \"healthy\" for v in checks.values())\n        span.set_attribute(\"overall_health\", \"healthy\" if all_healthy else \"degraded\")\n\n        return checks, 200 if all_healthy else 503\n</code></pre>"},{"location":"examples/production/#docker-deployment","title":"Docker Deployment","text":"<pre><code># Dockerfile\nFROM python:3.10-slim\n\nWORKDIR /app\n\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\nCOPY . .\n\n# Create traces directory\nRUN mkdir -p /var/log/traces\n\n# Environment variables\nENV SERVICE_NAME=prod-agent\nENV ENVIRONMENT=production\nENV PRELA_EXPORTER=file\nENV PRELA_TRACE_DIR=/var/log/traces\nENV PRELA_SAMPLE_RATE=0.05\n\nCMD [\"python\", \"app.py\"]\n</code></pre> <pre><code># docker-compose.yml\nversion: '3.8'\n\nservices:\n  agent:\n    build: .\n    environment:\n      - SERVICE_NAME=prod-agent\n      - ENVIRONMENT=production\n      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}\n      - PRELA_TRACE_DIR=/var/log/traces\n      - PRELA_SAMPLE_RATE=0.05\n    volumes:\n      - ./traces:/var/log/traces\n    restart: unless-stopped\n</code></pre>"},{"location":"examples/production/#kubernetes-deployment","title":"Kubernetes Deployment","text":"<pre><code># deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: prela-agent\nspec:\n  replicas: 3\n  template:\n    spec:\n      containers:\n      - name: agent\n        image: prela-agent:latest\n        env:\n        - name: SERVICE_NAME\n          value: \"prod-agent\"\n        - name: ENVIRONMENT\n          value: \"production\"\n        - name: PRELA_EXPORTER\n          value: \"file\"\n        - name: PRELA_TRACE_DIR\n          value: \"/var/log/traces\"\n        - name: PRELA_SAMPLE_RATE\n          value: \"0.05\"\n        - name: ANTHROPIC_API_KEY\n          valueFrom:\n            secretKeyRef:\n              name: api-keys\n              key: anthropic-key\n        volumeMounts:\n        - name: traces\n          mountPath: /var/log/traces\n      volumes:\n      - name: traces\n        persistentVolumeClaim:\n          claimName: traces-pvc\n</code></pre>"},{"location":"examples/production/#monitoring-and-alerting","title":"Monitoring and Alerting","text":"<pre><code>import prela\nfrom prometheus_client import Counter, Histogram\n\n# Prometheus metrics\ntrace_counter = Counter('prela_traces_total', 'Total traces created')\ntrace_duration = Histogram('prela_trace_duration_seconds', 'Trace duration')\n\nclass MonitoredTracer:\n    \"\"\"Tracer wrapper with monitoring.\"\"\"\n\n    def __init__(self, tracer):\n        self.tracer = tracer\n\n    def span(self, name, span_type=None):\n        trace_counter.inc()\n        return self.tracer.span(name, span_type)\n\ntracer = MonitoredTracer(prela.init(service_name=\"prod-agent\"))\n</code></pre>"},{"location":"examples/production/#cost-management","title":"Cost Management","text":"<pre><code>import prela\n\n# Cost-aware sampling\nclass CostAwareSampler:\n    \"\"\"Sample based on estimated cost.\"\"\"\n\n    def __init__(self, budget_per_hour=100):\n        self.budget_per_hour = budget_per_hour\n        self.cost_per_trace = 0.01  # Estimate\n        self.max_traces_per_hour = budget_per_hour / self.cost_per_trace\n\n    def should_sample(self, trace_id):\n        # Implement rate limiting based on budget\n        # For simplicity, using probability\n        rate = min(1.0, self.max_traces_per_hour / (3600 / 1))  # Per second\n        return prela.ProbabilitySampler(rate).should_sample(trace_id)\n\ntracer = prela.init(\n    service_name=\"prod-agent\",\n    sampler=CostAwareSampler(budget_per_hour=100)\n)\n</code></pre>"},{"location":"examples/production/#next-steps","title":"Next Steps","text":"<ul> <li>See Context Propagation</li> <li>Learn about Sampling</li> <li>Explore Exporters</li> </ul>"},{"location":"examples/replay-multi-agent/","title":"Replay with Multi-Agent Frameworks","text":"<p>This guide demonstrates how to capture and replay traces from multi-agent frameworks: CrewAI, AutoGen, LangGraph, and Swarm.</p>"},{"location":"examples/replay-multi-agent/#overview","title":"Overview","text":"<p>All multi-agent frameworks support replay capture when <code>capture_for_replay=True</code>:</p> <ul> <li>CrewAI - Captures system prompt, tools, agent memory, tasks</li> <li>AutoGen - Captures system messages, function maps, chat history</li> <li>LangGraph - Captures graph config, node list, input state</li> <li>Swarm - Captures instructions, functions, context variables</li> </ul> <p>Replay enables: - Test multi-agent workflows with different models - Compare agent performance across LLM providers - Detect regressions in agent behavior - Optimize agent prompts and configurations</p>"},{"location":"examples/replay-multi-agent/#crewai-replay","title":"CrewAI Replay","text":""},{"location":"examples/replay-multi-agent/#capture","title":"Capture","text":"<pre><code>import prela\nfrom crewai import Agent, Task, Crew\n\n# Enable replay capture\ntracer = prela.init(\n    service_name=\"crewai-demo\",\n    exporter=\"file\",\n    file_path=\"crew_trace.jsonl\",\n    capture_for_replay=True,  # Enable capture\n)\n\n# Define agents\nresearcher = Agent(\n    name=\"Researcher\",\n    role=\"AI Research Specialist\",\n    goal=\"Research latest AI trends\",\n    backstory=\"PhD in AI with 10 years experience\",\n    tools=[web_search_tool, scraper_tool],\n    allow_delegation=False,\n)\n\nwriter = Agent(\n    name=\"Writer\",\n    role=\"Technical Writer\",\n    goal=\"Write clear technical content\",\n    backstory=\"10 years writing about AI\",\n    tools=[grammar_checker_tool],\n    allow_delegation=False,\n)\n\n# Define tasks\nresearch_task = Task(\n    description=\"Research the latest AI agent trends and frameworks\",\n    agent=researcher,\n)\n\nwrite_task = Task(\n    description=\"Write a comprehensive article about AI agents\",\n    agent=writer,\n)\n\n# Create and execute crew\ncrew = Crew(\n    agents=[researcher, writer],\n    tasks=[research_task, write_task],\n    verbose=True,\n)\n\nresult = crew.kickoff()\nprint(result)\n</code></pre> <p>Captured Data: - System Prompt: <code>\"Crew: {crew_name}\"</code> - Available Tools: All tools across all agents with agent attribution - Agent Memory: Complete agent definitions (name, role, goal, backstory, model) - Tasks: Task descriptions and assignments - Config: Framework, execution_id, num_agents, num_tasks, process type</p>"},{"location":"examples/replay-multi-agent/#replay","title":"Replay","text":"<pre><code>from prela.replay import ReplayEngine, TraceLoader\n\n# Load trace\ntrace = TraceLoader.from_file(\"crew_trace.jsonl\")\n\n# Replay with different model\nengine = ReplayEngine(trace)\nresult = engine.replay_with_modifications(\n    model=\"gpt-4o\",  # Try different model\n    temperature=0.5,\n)\n\nprint(f\"Original model: {trace.spans[0].attributes.get('llm.model')}\")\nprint(f\"Replayed with: gpt-4o\")\nprint(f\"Output similarity: {result.output_similarity:.2%}\")\n</code></pre>"},{"location":"examples/replay-multi-agent/#compare-crewai-configurations","title":"Compare CrewAI Configurations","text":"<pre><code>from prela.replay import ReplayEngine, TraceLoader\n\ntrace = TraceLoader.from_file(\"crew_trace.jsonl\")\nengine = ReplayEngine(trace)\n\n# Test 1: Original model\nresult1 = engine.replay_with_modifications(model=\"gpt-4\")\n\n# Test 2: Faster model\nresult2 = engine.replay_with_modifications(model=\"gpt-3.5-turbo\")\n\n# Test 3: Different provider\nresult3 = engine.replay_with_modifications(model=\"claude-sonnet-4\")\n\n# Compare\ncomparison12 = engine.compare_replay(result1, result2)\ncomparison13 = engine.compare_replay(result1, result3)\n\nprint(f\"GPT-4 vs GPT-3.5: {comparison12.output_similarity:.2%}\")\nprint(f\"GPT-4 vs Claude: {comparison13.output_similarity:.2%}\")\n</code></pre>"},{"location":"examples/replay-multi-agent/#autogen-replay","title":"AutoGen Replay","text":""},{"location":"examples/replay-multi-agent/#capture_1","title":"Capture","text":"<pre><code>import prela\nfrom autogen import ConversableAgent\n\n# Enable replay capture\ntracer = prela.init(\n    service_name=\"autogen-demo\",\n    exporter=\"file\",\n    file_path=\"autogen_trace.jsonl\",\n    capture_for_replay=True,\n)\n\n# Define agents\nuser_proxy = ConversableAgent(\n    name=\"UserProxy\",\n    system_message=\"A human user providing requirements\",\n    code_execution_config={\"work_dir\": \"workspace\", \"use_docker\": False},\n    llm_config=False,\n    human_input_mode=\"NEVER\",\n)\n\ncoder = ConversableAgent(\n    name=\"Coder\",\n    system_message=\"Expert Python programmer writing clean code\",\n    llm_config={\"model\": \"gpt-4\", \"temperature\": 0.1},\n)\n\ncritic = ConversableAgent(\n    name=\"Critic\",\n    system_message=\"Code reviewer providing improvement suggestions\",\n    llm_config={\"model\": \"gpt-4\", \"temperature\": 0.3},\n)\n\n# Initiate conversation\nuser_proxy.initiate_chat(\n    coder,\n    message=\"Write a Python function to calculate Fibonacci numbers with memoization\"\n)\n</code></pre> <p>Captured Data: - System Messages: Each agent's system prompt - Function Map: Available functions with names and descriptions - Chat History: Complete message sequence - Config: Framework, conversation_id, initiator, recipient, max_turns</p>"},{"location":"examples/replay-multi-agent/#replay_1","title":"Replay","text":"<pre><code>from prela.replay import ReplayEngine, TraceLoader\n\ntrace = TraceLoader.from_file(\"autogen_trace.jsonl\")\nengine = ReplayEngine(trace)\n\n# Replay conversation with different temperature\nresult = engine.replay_with_modifications(\n    model=\"gpt-4\",\n    temperature=0.0,  # More deterministic\n)\n\nprint(\"Replayed conversation with temperature=0.0\")\nprint(f\"Turn count: {len([s for s in result.spans if 'message' in s.name])}\")\n</code></pre>"},{"location":"examples/replay-multi-agent/#test-conversation-consistency","title":"Test Conversation Consistency","text":"<pre><code>from prela.replay import ReplayEngine, TraceLoader\n\ntrace = TraceLoader.from_file(\"autogen_trace.jsonl\")\nengine = ReplayEngine(trace)\n\n# Replay 5 times with same settings\nresults = []\nfor i in range(5):\n    result = engine.replay_with_modifications(\n        model=\"gpt-4\",\n        temperature=0.0,  # Should be deterministic\n    )\n    results.append(result)\n\n# Check consistency\nsimilarities = []\nfor i in range(1, len(results)):\n    comparison = engine.compare_replay(results[0], results[i])\n    similarities.append(comparison.output_similarity)\n\navg_similarity = sum(similarities) / len(similarities)\nprint(f\"Average consistency: {avg_similarity:.2%}\")\nprint(f\"Expected: &gt;95% for temperature=0.0\")\n</code></pre>"},{"location":"examples/replay-multi-agent/#langgraph-replay","title":"LangGraph Replay","text":""},{"location":"examples/replay-multi-agent/#capture_2","title":"Capture","text":"<pre><code>import prela\nfrom langgraph.graph import StateGraph\n\n# Enable replay capture\ntracer = prela.init(\n    service_name=\"langgraph-demo\",\n    exporter=\"file\",\n    file_path=\"langgraph_trace.jsonl\",\n    capture_for_replay=True,\n)\n\n# Define state schema\ngraph = StateGraph(state_schema={\n    \"messages\": list,\n    \"analyzed\": bool,\n    \"sentiment\": str,\n    \"response\": str,\n})\n\n# Define nodes\ndef analyze_node(state):\n    messages = state[\"messages\"]\n    text = \" \".join(messages)\n    sentiment = \"positive\" if \"good\" in text.lower() else \"neutral\"\n    return {**state, \"analyzed\": True, \"sentiment\": sentiment}\n\ndef respond_node(state):\n    sentiment = state.get(\"sentiment\", \"neutral\")\n    response = \"Glad to hear!\" if sentiment == \"positive\" else \"Thank you\"\n    return {**state, \"response\": response}\n\n# Build graph\ngraph.add_node(\"analyze\", analyze_node)\ngraph.add_node(\"respond\", respond_node)\ngraph.set_entry_point(\"analyze\")\ngraph.add_edge(\"analyze\", \"respond\")\ngraph.set_finish_point(\"respond\")\n\n# Compile and execute\ncompiled = graph.compile()\nresult = compiled.invoke({\"messages\": [\"This is good!\"], \"analyzed\": False})\nprint(result)\n</code></pre> <p>Captured Data: - Graph Config: Framework, graph_id, nodes list - Input State: Complete input state (dict or truncated string) - Node Changes: Keys modified by each node</p>"},{"location":"examples/replay-multi-agent/#replay_2","title":"Replay","text":"<pre><code>from prela.replay import ReplayEngine, TraceLoader\n\ntrace = TraceLoader.from_file(\"langgraph_trace.jsonl\")\nengine = ReplayEngine(trace)\n\n# Replay with different input state\nresult = engine.replay_with_modifications(\n    # Note: LangGraph replay uses captured state\n    # Modifications apply to LLM calls within nodes (if any)\n    model=\"gpt-4o\",\n)\n\nprint(\"Replayed LangGraph execution\")\nprint(f\"Nodes executed: {len([s for s in result.spans if 'node' in s.name])}\")\nprint(f\"State changes captured: {sum(1 for s in result.spans if 'changed_keys' in s.attributes)}\")\n</code></pre>"},{"location":"examples/replay-multi-agent/#compare-graph-execution","title":"Compare Graph Execution","text":"<pre><code>from prela.replay import ReplayEngine, TraceLoader\n\ntrace = TraceLoader.from_file(\"langgraph_trace.jsonl\")\nengine = ReplayEngine(trace)\n\n# Replay 3 times (should be identical for deterministic graphs)\nresults = []\nfor i in range(3):\n    result = engine.replay_with_modifications()\n    results.append(result)\n\n# Verify determinism\nfor i in range(1, len(results)):\n    comparison = engine.compare_replay(results[0], results[i])\n    print(f\"Run {i+1} similarity: {comparison.output_similarity:.2%}\")\n</code></pre>"},{"location":"examples/replay-multi-agent/#swarm-replay","title":"Swarm Replay","text":""},{"location":"examples/replay-multi-agent/#capture_3","title":"Capture","text":"<pre><code>import prela\nfrom swarm import Swarm, Agent\n\n# Enable replay capture\ntracer = prela.init(\n    service_name=\"swarm-demo\",\n    exporter=\"file\",\n    file_path=\"swarm_trace.jsonl\",\n    capture_for_replay=True,\n)\n\n# Define agents with handoff functions\ndef transfer_to_billing():\n    \"\"\"Transfer to billing specialist.\"\"\"\n    return billing_agent\n\ndef transfer_to_technical():\n    \"\"\"Transfer to technical support.\"\"\"\n    return technical_agent\n\ntriage_agent = Agent(\n    name=\"Triage\",\n    instructions=\"Route customers to right specialist\",\n    functions=[transfer_to_billing, transfer_to_technical],\n)\n\nbilling_agent = Agent(\n    name=\"Billing\",\n    instructions=\"Help with payment and subscription questions\",\n)\n\ntechnical_agent = Agent(\n    name=\"Technical\",\n    instructions=\"Help with technical issues and bugs\",\n)\n\n# Execute with Swarm\nclient = Swarm()\nresponse = client.run(\n    agent=triage_agent,\n    messages=[{\"role\": \"user\", \"content\": \"I want to cancel my subscription\"}],\n    context_variables={\"user_id\": \"user-123\", \"tier\": \"premium\"},\n)\n\nprint(f\"Final agent: {response.agent.name}\")\nprint(f\"Handoffs: {response.context_variables.get('handoff_count', 0)}\")\n</code></pre> <p>Captured Data: - Instructions: Agent's system instructions - Functions: Agent's available functions with names and docstrings - Context Variables: Keys only (not values for privacy) - Config: Framework, execution_id, initial_agent, final_agent</p>"},{"location":"examples/replay-multi-agent/#replay_3","title":"Replay","text":"<pre><code>from prela.replay import ReplayEngine, TraceLoader\n\ntrace = TraceLoader.from_file(\"swarm_trace.jsonl\")\nengine = ReplayEngine(trace)\n\n# Replay with different model\nresult = engine.replay_with_modifications(\n    model=\"gpt-4o\",\n    temperature=0.3,\n)\n\nprint(\"Replayed Swarm execution\")\nprint(f\"Agents used: {len([s for s in result.spans if 'agent' in s.attributes])}\")\nprint(f\"Handoffs: {sum(1 for s in result.spans if 'handoff' in s.name)}\")\n</code></pre>"},{"location":"examples/replay-multi-agent/#test-agent-handoff-consistency","title":"Test Agent Handoff Consistency","text":"<pre><code>from prela.replay import ReplayEngine, TraceLoader\n\ntrace = TraceLoader.from_file(\"swarm_trace.jsonl\")\nengine = ReplayEngine(trace)\n\n# Replay multiple times\nhandoff_counts = []\nfor i in range(10):\n    result = engine.replay_with_modifications(\n        model=\"gpt-4\",\n        temperature=0.0,  # Deterministic\n    )\n\n    handoffs = sum(1 for s in result.spans if 'handoff' in s.name)\n    handoff_counts.append(handoffs)\n\n# Check consistency\nprint(f\"Handoff counts: {handoff_counts}\")\nprint(f\"Consistent: {len(set(handoff_counts)) == 1}\")\n</code></pre>"},{"location":"examples/replay-multi-agent/#cross-framework-comparison","title":"Cross-Framework Comparison","text":""},{"location":"examples/replay-multi-agent/#compare-approaches-to-same-task","title":"Compare Approaches to Same Task","text":"<pre><code>from prela.replay import ReplayEngine, TraceLoader\n\n# Load traces from different frameworks solving same task\ncrewai_trace = TraceLoader.from_file(\"crew_research_trace.jsonl\")\nautogen_trace = TraceLoader.from_file(\"autogen_research_trace.jsonl\")\nlanggraph_trace = TraceLoader.from_file(\"langgraph_research_trace.jsonl\")\nswarm_trace = TraceLoader.from_file(\"swarm_research_trace.jsonl\")\n\n# Replay all with same model\nmodel = \"gpt-4o\"\ntemperature = 0.7\n\ncrewai_result = ReplayEngine(crewai_trace).replay_with_modifications(\n    model=model, temperature=temperature\n)\nautogen_result = ReplayEngine(autogen_trace).replay_with_modifications(\n    model=model, temperature=temperature\n)\nlanggraph_result = ReplayEngine(langgraph_trace).replay_with_modifications(\n    model=model, temperature=temperature\n)\nswarm_result = ReplayEngine(swarm_trace).replay_with_modifications(\n    model=model, temperature=temperature\n)\n\n# Compare metrics\nresults = {\n    \"CrewAI\": crewai_result,\n    \"AutoGen\": autogen_result,\n    \"LangGraph\": langgraph_result,\n    \"Swarm\": swarm_result,\n}\n\nfor framework, result in results.items():\n    duration = sum(s.duration_ms for s in result.spans)\n    tokens = sum(s.attributes.get(\"llm.total_tokens\", 0) for s in result.spans)\n    print(f\"{framework:12} - Duration: {duration:6.0f}ms, Tokens: {tokens:5}\")\n</code></pre> <p>Output: <pre><code>CrewAI       - Duration: 8250ms, Tokens: 2150\nAutoGen      - Duration: 6800ms, Tokens: 1950\nLangGraph    - Duration: 5400ms, Tokens: 1800\nSwarm        - Duration: 7200ms, Tokens: 2050\n</code></pre></p>"},{"location":"examples/replay-multi-agent/#multi-agent-regression-testing","title":"Multi-Agent Regression Testing","text":""},{"location":"examples/replay-multi-agent/#detect-agent-behavior-changes","title":"Detect Agent Behavior Changes","text":"<pre><code>from prela.replay import ReplayEngine, TraceLoader\n\ndef test_agent_regression(baseline_trace_path: str, current_trace_path: str):\n    \"\"\"Compare baseline vs current agent behavior.\"\"\"\n\n    baseline = TraceLoader.from_file(baseline_trace_path)\n    current = TraceLoader.from_file(current_trace_path)\n\n    # Replay both with same settings\n    engine = ReplayEngine(baseline)\n    baseline_result = engine.replay_with_modifications(\n        model=\"gpt-4\",\n        temperature=0.0,\n    )\n\n    engine = ReplayEngine(current)\n    current_result = engine.replay_with_modifications(\n        model=\"gpt-4\",\n        temperature=0.0,\n    )\n\n    # Compare\n    comparison = engine.compare_replay(baseline_result, current_result)\n\n    print(f\"Output similarity: {comparison.output_similarity:.2%}\")\n    print(f\"Span count: {comparison.span_count_match}\")\n    print(f\"Status match: {comparison.all_status_match}\")\n\n    # Alert if significant change\n    if comparison.output_similarity &lt; 0.9:\n        print(\"\u26a0\ufe0f  Warning: Significant behavior change detected\")\n        return False\n\n    print(\"\u2713 No regression detected\")\n    return True\n\n# Run regression test\ntest_agent_regression(\n    baseline_trace_path=\"baseline_v1.0_crew.jsonl\",\n    current_trace_path=\"current_v1.1_crew.jsonl\",\n)\n</code></pre>"},{"location":"examples/replay-multi-agent/#best-practices","title":"Best Practices","text":""},{"location":"examples/replay-multi-agent/#1-capture-with-production-settings","title":"1. Capture with Production Settings","text":"<pre><code># Capture traces with realistic configurations\ntracer = prela.init(\n    capture_for_replay=True,\n    sample_rate=0.1,  # Sample 10% to avoid overhead\n)\n</code></pre>"},{"location":"examples/replay-multi-agent/#2-use-deterministic-settings-for-testing","title":"2. Use Deterministic Settings for Testing","text":"<pre><code># Replay with temperature=0 for consistency tests\nresult = engine.replay_with_modifications(\n    model=\"gpt-4\",\n    temperature=0.0,  # Deterministic\n)\n</code></pre>"},{"location":"examples/replay-multi-agent/#3-test-multiple-models","title":"3. Test Multiple Models","text":"<pre><code>models = [\"gpt-4\", \"gpt-3.5-turbo\", \"claude-sonnet-4\"]\nresults = {}\n\nfor model in models:\n    result = engine.replay_with_modifications(model=model)\n    results[model] = result\n\n# Compare performance\nfor model, result in results.items():\n    duration = sum(s.duration_ms for s in result.spans)\n    print(f\"{model}: {duration:.0f}ms\")\n</code></pre>"},{"location":"examples/replay-multi-agent/#4-version-control-traces","title":"4. Version Control Traces","text":"<pre><code># Store baseline traces for regression testing\ngit add traces/baseline_v1.0_*.jsonl\ngit commit -m \"Add baseline traces for v1.0\"\n</code></pre>"},{"location":"examples/replay-multi-agent/#5-automate-regression-tests","title":"5. Automate Regression Tests","text":"<pre><code>import pytest\nfrom prela.replay import ReplayEngine, TraceLoader\n\n@pytest.mark.parametrize(\"framework\", [\"crewai\", \"autogen\", \"langgraph\", \"swarm\"])\ndef test_multi_agent_regression(framework):\n    \"\"\"Test for regressions in multi-agent behavior.\"\"\"\n    baseline = TraceLoader.from_file(f\"baselines/{framework}_baseline.jsonl\")\n\n    engine = ReplayEngine(baseline)\n    result = engine.replay_with_modifications(\n        model=\"gpt-4\",\n        temperature=0.0,\n    )\n\n    # Compare with baseline\n    comparison = engine.compare_replay(baseline, result)\n\n    assert comparison.output_similarity &gt;= 0.95, \\\n        f\"{framework} regression: similarity {comparison.output_similarity:.2%}\"\n</code></pre>"},{"location":"examples/replay-multi-agent/#next-steps","title":"Next Steps","text":"<ul> <li>Replay Advanced Features - API retry, semantic fallback, tool re-execution</li> <li>Replay with Tools - Tool re-execution examples</li> <li>CrewAI Integration - CrewAI instrumentation details</li> <li>AutoGen Integration - AutoGen instrumentation details</li> <li>LangGraph Integration - LangGraph instrumentation details</li> <li>Swarm Integration - Swarm instrumentation details</li> </ul>"},{"location":"examples/replay-with-tools/","title":"Replay with Tool Re-execution","text":"<p>This guide demonstrates how to re-execute tools during replay instead of using cached outputs. This enables testing with fresh data, validating tool implementations, and ensuring agent behavior with current external state.</p>"},{"location":"examples/replay-with-tools/#overview","title":"Overview","text":"<p>Tool re-execution allows you to:</p> <ul> <li>Test with fresh data - Query live APIs instead of cached responses</li> <li>Validate tool implementations - Ensure tools still work correctly</li> <li>Detect regressions - Compare cached vs fresh tool outputs</li> <li>Control execution - Use allowlists/blocklists for safety</li> </ul> <p>Priority System: 1. Mocks (highest) - Override with test data 2. Execution (medium) - Re-execute tool functions 3. Cached (lowest) - Use original trace data</p>"},{"location":"examples/replay-with-tools/#basic-tool-re-execution","title":"Basic Tool Re-execution","text":""},{"location":"examples/replay-with-tools/#simple-example","title":"Simple Example","text":"<pre><code>from prela.replay import ReplayEngine, TraceLoader\n\n# Define tool function\ndef calculator(expression: str) -&gt; str:\n    \"\"\"Safe calculator tool.\"\"\"\n    try:\n        result = eval(expression, {\"__builtins__\": {}}, {})\n        return str(result)\n    except Exception as e:\n        return f\"Error: {str(e)}\"\n\n# Create tool registry\ntool_registry = {\n    \"calculator\": calculator,\n}\n\n# Load trace\ntrace = TraceLoader.from_file(\"trace_with_tools.jsonl\")\n\n# Enable tool re-execution\nengine = ReplayEngine(\n    trace,\n    tool_registry=tool_registry,\n    enable_tool_execution=True,\n)\n\n# Replay - tools will be re-executed\nresult = engine.replay_with_modifications(model=\"gpt-4o\")\n\nprint(f\"Tool calls: {sum(1 for s in result.spans if s.span_type == 'tool')}\")\nprint(f\"Tool errors: {sum(1 for s in result.spans if s.span_type == 'tool' and s.status == 'error')}\")\n</code></pre>"},{"location":"examples/replay-with-tools/#compare-cached-vs-fresh-results","title":"Compare Cached vs Fresh Results","text":"<pre><code>from prela.replay import ReplayEngine, TraceLoader\n\ntrace = TraceLoader.from_file(\"trace_with_tools.jsonl\")\ntool_registry = {\"calculator\": calculator}\n\n# Test 1: Use cached tool outputs\nengine_cached = ReplayEngine(trace, enable_tool_execution=False)\nresult_cached = engine_cached.replay_with_modifications(model=\"gpt-4o\")\n\n# Test 2: Re-execute tools\nengine_fresh = ReplayEngine(\n    trace,\n    tool_registry=tool_registry,\n    enable_tool_execution=True,\n)\nresult_fresh = engine_fresh.replay_with_modifications(model=\"gpt-4o\")\n\n# Compare\ncomparison = engine_fresh.compare_replay(result_cached, result_fresh)\nprint(f\"Output similarity: {comparison.output_similarity:.2%}\")\nprint(f\"Tool outputs changed: {comparison.output_similarity &lt; 0.99}\")\n</code></pre>"},{"location":"examples/replay-with-tools/#safety-controls","title":"Safety Controls","text":""},{"location":"examples/replay-with-tools/#allowlist-recommended","title":"Allowlist (Recommended)","text":"<p>Only execute approved tools:</p> <pre><code># Define multiple tools\ndef calculator(expr: str) -&gt; str:\n    return str(eval(expr))\n\ndef search_api(query: str) -&gt; str:\n    import requests\n    response = requests.get(f\"https://api.example.com/search?q={query}\")\n    return response.json()\n\ndef delete_file(path: str) -&gt; str:\n    import os\n    os.remove(path)\n    return \"Deleted\"\n\ntool_registry = {\n    \"calculator\": calculator,\n    \"search_api\": search_api,\n    \"delete_file\": delete_file,  # Dangerous!\n}\n\n# Only execute safe tools\nengine = ReplayEngine(\n    trace,\n    tool_registry=tool_registry,\n    enable_tool_execution={\n        \"allowlist\": [\"calculator\", \"search_api\"],  # Exclude delete_file\n    }\n)\n\nresult = engine.replay_with_modifications(model=\"gpt-4o\")\n\n# delete_file will use cached output (not executed)\n</code></pre>"},{"location":"examples/replay-with-tools/#blocklist","title":"Blocklist","text":"<p>Explicitly block dangerous tools:</p> <pre><code>engine = ReplayEngine(\n    trace,\n    tool_registry=tool_registry,\n    enable_tool_execution={\n        \"blocklist\": [\"delete_file\", \"send_email\", \"make_payment\"],\n    }\n)\n\n# All tools except blocklist will be executed\n</code></pre>"},{"location":"examples/replay-with-tools/#combined-controls","title":"Combined Controls","text":"<p>Use both for extra safety:</p> <pre><code>engine = ReplayEngine(\n    trace,\n    tool_registry=tool_registry,\n    enable_tool_execution={\n        \"allowlist\": [\"calculator\", \"search_api\", \"weather_api\"],\n        \"blocklist\": [\"delete_file\"],  # Extra safety (redundant but explicit)\n    }\n)\n</code></pre>"},{"location":"examples/replay-with-tools/#external-api-tools","title":"External API Tools","text":""},{"location":"examples/replay-with-tools/#http-request-tool","title":"HTTP Request Tool","text":"<pre><code>import requests\nfrom prela.replay import ReplayEngine, TraceLoader\n\ndef github_search(query: str, max_results: int = 5) -&gt; str:\n    \"\"\"Search GitHub repositories.\"\"\"\n    response = requests.get(\n        \"https://api.github.com/search/repositories\",\n        params={\"q\": query, \"per_page\": max_results},\n        headers={\"Accept\": \"application/vnd.github.v3+json\"},\n    )\n    response.raise_for_status()\n\n    data = response.json()\n    repos = data.get(\"items\", [])\n\n    results = []\n    for repo in repos:\n        results.append({\n            \"name\": repo[\"full_name\"],\n            \"stars\": repo[\"stargazers_count\"],\n            \"url\": repo[\"html_url\"],\n        })\n\n    return str(results)\n\n# Test with live API\ntool_registry = {\"github_search\": github_search}\n\nengine = ReplayEngine(\n    trace,\n    tool_registry=tool_registry,\n    enable_tool_execution={\"allowlist\": [\"github_search\"]},\n)\n\nresult = engine.replay_with_modifications(model=\"gpt-4o\")\nprint(\"Replayed with fresh GitHub data\")\n</code></pre>"},{"location":"examples/replay-with-tools/#database-query-tool","title":"Database Query Tool","text":"<pre><code>import psycopg2\nfrom prela.replay import ReplayEngine, TraceLoader\n\ndef query_users(filter: str) -&gt; str:\n    \"\"\"Query user database.\"\"\"\n    conn = psycopg2.connect(\"dbname=mydb user=postgres\")\n    cur = conn.cursor()\n\n    # Safe parameterized query\n    cur.execute(\"SELECT id, name, email FROM users WHERE name LIKE %s\", (f\"%{filter}%\",))\n    results = cur.fetchall()\n\n    cur.close()\n    conn.close()\n\n    return str([{\"id\": r[0], \"name\": r[1], \"email\": r[2]} for r in results])\n\n# Test with current database state\ntool_registry = {\"query_users\": query_users}\n\nengine = ReplayEngine(\n    trace,\n    tool_registry=tool_registry,\n    enable_tool_execution={\"allowlist\": [\"query_users\"]},\n)\n\nresult = engine.replay_with_modifications(model=\"gpt-4o\")\nprint(\"Replayed with current database data\")\n</code></pre>"},{"location":"examples/replay-with-tools/#mocking-tools","title":"Mocking Tools","text":""},{"location":"examples/replay-with-tools/#override-with-test-data","title":"Override with Test Data","text":"<pre><code>from prela.replay import ReplayEngine, TraceLoader\n\ntrace = TraceLoader.from_file(\"trace_with_api_tools.jsonl\")\n\n# Mock API responses for testing\ntool_mocks = {\n    \"github_search\": '[{\"name\": \"test/repo\", \"stars\": 100}]',\n    \"weather_api\": '{\"temp\": 72, \"condition\": \"sunny\"}',\n}\n\nengine = ReplayEngine(\n    trace,\n    tool_mocks=tool_mocks,  # Mocks override execution\n)\n\nresult = engine.replay_with_modifications(model=\"gpt-4o\")\nprint(\"Replayed with mocked tool outputs\")\n</code></pre>"},{"location":"examples/replay-with-tools/#selective-mocking","title":"Selective Mocking","text":"<pre><code># Define real tool functions\ndef calculator(expr: str) -&gt; str:\n    return str(eval(expr))\n\ndef expensive_api(query: str) -&gt; str:\n    import requests\n    response = requests.get(f\"https://expensive-api.com/search?q={query}\")\n    return response.json()\n\ntool_registry = {\n    \"calculator\": calculator,\n    \"expensive_api\": expensive_api,\n}\n\n# Mock only the expensive API\ntool_mocks = {\n    \"expensive_api\": '{\"results\": [\"mocked result 1\", \"mocked result 2\"]}',\n}\n\nengine = ReplayEngine(\n    trace,\n    tool_registry=tool_registry,\n    enable_tool_execution={\"allowlist\": [\"calculator\"]},\n    tool_mocks=tool_mocks,  # Mock expensive_api\n)\n\n# calculator: Re-executed\n# expensive_api: Mocked (not executed)\nresult = engine.replay_with_modifications(model=\"gpt-4o\")\n</code></pre>"},{"location":"examples/replay-with-tools/#error-handling","title":"Error Handling","text":""},{"location":"examples/replay-with-tools/#capture-tool-failures","title":"Capture Tool Failures","text":"<pre><code>def risky_tool(input: str) -&gt; str:\n    \"\"\"Tool that might fail.\"\"\"\n    if input == \"error\":\n        raise ValueError(\"Invalid input\")\n    if input == \"timeout\":\n        import time\n        time.sleep(100)  # Timeout\n    return f\"Processed: {input}\"\n\ntool_registry = {\"risky_tool\": risky_tool}\n\nengine = ReplayEngine(\n    trace,\n    tool_registry=tool_registry,\n    enable_tool_execution=True,\n)\n\nresult = engine.replay_with_modifications(model=\"gpt-4o\")\n\n# Check for tool errors\nfor span in result.spans:\n    if span.span_type == \"tool\" and span.status == \"error\":\n        print(f\"Tool {span.name} failed:\")\n        print(f\"  Error type: {span.attributes.get('error.type')}\")\n        print(f\"  Error message: {span.attributes.get('error.message')}\")\n</code></pre>"},{"location":"examples/replay-with-tools/#fallback-to-cached-on-error","title":"Fallback to Cached on Error","text":"<pre><code>def flaky_api(query: str) -&gt; str:\n    \"\"\"API that sometimes fails.\"\"\"\n    import random\n    if random.random() &lt; 0.3:  # 30% failure rate\n        raise Exception(\"API unavailable\")\n    return fetch_real_data(query)\n\ntool_registry = {\"flaky_api\": flaky_api}\n\nengine = ReplayEngine(\n    trace,\n    tool_registry=tool_registry,\n    enable_tool_execution=True,\n)\n\nresult = engine.replay_with_modifications(model=\"gpt-4o\")\n\n# If tool fails, cached output is used automatically\n# Span status will show ERROR, but replay continues\n</code></pre>"},{"location":"examples/replay-with-tools/#integration-testing","title":"Integration Testing","text":""},{"location":"examples/replay-with-tools/#test-agent-with-live-external-systems","title":"Test Agent with Live External Systems","text":"<pre><code>from prela.replay import ReplayEngine, TraceLoader\nimport os\n\n# Define tools for real services\ndef slack_send_message(channel: str, message: str) -&gt; str:\n    \"\"\"Send Slack message.\"\"\"\n    import requests\n    response = requests.post(\n        \"https://slack.com/api/chat.postMessage\",\n        headers={\"Authorization\": f\"Bearer {os.environ['SLACK_TOKEN']}\"},\n        json={\"channel\": channel, \"text\": message},\n    )\n    return response.json()\n\ndef github_create_issue(repo: str, title: str, body: str) -&gt; str:\n    \"\"\"Create GitHub issue.\"\"\"\n    import requests\n    response = requests.post(\n        f\"https://api.github.com/repos/{repo}/issues\",\n        headers={\"Authorization\": f\"token {os.environ['GITHUB_TOKEN']}\"},\n        json={\"title\": title, \"body\": body},\n    )\n    return response.json()\n\ntool_registry = {\n    \"slack_send_message\": slack_send_message,\n    \"github_create_issue\": github_create_issue,\n}\n\n# Run integration test\nengine = ReplayEngine(\n    trace,\n    tool_registry=tool_registry,\n    enable_tool_execution={\n        \"allowlist\": [\"slack_send_message\"],  # Only test Slack\n    }\n)\n\nresult = engine.replay_with_modifications(model=\"gpt-4o\")\nprint(\"Integration test with live Slack API completed\")\n</code></pre>"},{"location":"examples/replay-with-tools/#tool-consistency-testing","title":"Tool Consistency Testing","text":""},{"location":"examples/replay-with-tools/#detect-tool-behavior-changes","title":"Detect Tool Behavior Changes","text":"<pre><code>from prela.replay import ReplayEngine, TraceLoader\n\ntrace = TraceLoader.from_file(\"baseline_trace.jsonl\")\n\n# Test current tool implementation\ndef calculator_v2(expr: str) -&gt; str:\n    \"\"\"Updated calculator with better error handling.\"\"\"\n    try:\n        # New safety checks\n        if any(x in expr for x in [\"__\", \"import\", \"exec\", \"eval\"]):\n            return \"Error: Forbidden operation\"\n        result = eval(expr, {\"__builtins__\": {}}, {})\n        return str(result)\n    except Exception as e:\n        return f\"Error: {str(e)}\"\n\ntool_registry = {\"calculator\": calculator_v2}\n\n# Test 1: Cached outputs (baseline)\nresult_baseline = ReplayEngine(trace).replay_with_modifications(model=\"gpt-4\")\n\n# Test 2: Re-execute with new implementation\nresult_current = ReplayEngine(\n    trace,\n    tool_registry=tool_registry,\n    enable_tool_execution=True,\n).replay_with_modifications(model=\"gpt-4\")\n\n# Compare\ncomparison = ReplayEngine(trace).compare_replay(result_baseline, result_current)\n\nif comparison.output_similarity &lt; 0.95:\n    print(\"\u26a0\ufe0f  Warning: Tool behavior changed significantly\")\n    print(f\"Similarity: {comparison.output_similarity:.2%}\")\nelse:\n    print(\"\u2713 Tool behavior consistent\")\n</code></pre>"},{"location":"examples/replay-with-tools/#regression-testing","title":"Regression Testing","text":""},{"location":"examples/replay-with-tools/#automated-tool-tests","title":"Automated Tool Tests","text":"<pre><code>import pytest\nfrom prela.replay import ReplayEngine, TraceLoader\n\n@pytest.mark.parametrize(\"tool_name,tool_fn\", [\n    (\"calculator\", calculator),\n    (\"weather_api\", weather_api),\n    (\"search_api\", search_api),\n])\ndef test_tool_consistency(tool_name, tool_fn):\n    \"\"\"Test that tool implementations produce consistent results.\"\"\"\n    trace = TraceLoader.from_file(f\"baselines/{tool_name}_trace.jsonl\")\n\n    # Baseline: Use cached outputs\n    engine_baseline = ReplayEngine(trace)\n    result_baseline = engine_baseline.replay_with_modifications(\n        model=\"gpt-4\",\n        temperature=0.0,\n    )\n\n    # Current: Re-execute tool\n    engine_current = ReplayEngine(\n        trace,\n        tool_registry={tool_name: tool_fn},\n        enable_tool_execution={\"allowlist\": [tool_name]},\n    )\n    result_current = engine_current.replay_with_modifications(\n        model=\"gpt-4\",\n        temperature=0.0,\n    )\n\n    # Compare\n    comparison = engine_baseline.compare_replay(result_baseline, result_current)\n\n    assert comparison.output_similarity &gt;= 0.90, \\\n        f\"Tool {tool_name} behavior changed: {comparison.output_similarity:.2%}\"\n</code></pre>"},{"location":"examples/replay-with-tools/#cicd-integration","title":"CI/CD Integration","text":"<pre><code># .github/workflows/tool-tests.yml\nname: Tool Consistency Tests\n\non: [push, pull_request]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n\n      - name: Set up Python\n        uses: actions/setup-python@v4\n        with:\n          python-version: '3.11'\n\n      - name: Install dependencies\n        run: |\n          pip install prela pytest\n\n      - name: Run tool tests\n        run: |\n          pytest tests/test_tool_consistency.py -v\n\n      - name: Check for regressions\n        run: |\n          python scripts/check_tool_regression.py\n</code></pre>"},{"location":"examples/replay-with-tools/#best-practices","title":"Best Practices","text":""},{"location":"examples/replay-with-tools/#1-use-allowlists-for-production","title":"1. Use Allowlists for Production","text":"<p>Always specify which tools are safe:</p> <pre><code># \u2713 Good - explicit allowlist\nengine = ReplayEngine(\n    trace,\n    tool_registry=all_tools,\n    enable_tool_execution={\"allowlist\": [\"safe_tool_1\", \"safe_tool_2\"]},\n)\n\n# \u2717 Bad - enables all tools\nengine = ReplayEngine(\n    trace,\n    tool_registry=all_tools,\n    enable_tool_execution=True,\n)\n</code></pre>"},{"location":"examples/replay-with-tools/#2-mock-expensive-operations","title":"2. Mock Expensive Operations","text":"<p>Use mocks for costly API calls:</p> <pre><code># Development/testing: Mock expensive APIs\ntool_mocks = {\n    \"gpt4_call\": '{\"response\": \"mocked\"}',\n    \"image_generation\": '{\"url\": \"https://example.com/mock.png\"}',\n}\n\nengine = ReplayEngine(trace, tool_mocks=tool_mocks)\n</code></pre>"},{"location":"examples/replay-with-tools/#3-test-tool-isolation","title":"3. Test Tool Isolation","text":"<p>Test tools independently:</p> <pre><code># Test one tool at a time\nfor tool_name in [\"tool1\", \"tool2\", \"tool3\"]:\n    engine = ReplayEngine(\n        trace,\n        tool_registry={tool_name: tools[tool_name]},\n        enable_tool_execution={\"allowlist\": [tool_name]},\n    )\n    result = engine.replay_with_modifications(model=\"gpt-4\")\n    print(f\"{tool_name}: {result.status}\")\n</code></pre>"},{"location":"examples/replay-with-tools/#4-version-control-tool-implementations","title":"4. Version Control Tool Implementations","text":"<pre><code># Store tool versions in trace metadata\nimport inspect\n\ntool_metadata = {\n    \"calculator_version\": \"2.0\",\n    \"calculator_hash\": hashlib.sha256(\n        inspect.getsource(calculator).encode()\n    ).hexdigest()[:8],\n}\n\n# Include in trace for future reference\n</code></pre>"},{"location":"examples/replay-with-tools/#5-monitor-tool-execution","title":"5. Monitor Tool Execution","text":"<pre><code>result = engine.replay_with_modifications(model=\"gpt-4o\")\n\n# Analyze tool usage\ntool_spans = [s for s in result.spans if s.span_type == \"tool\"]\nfor span in tool_spans:\n    print(f\"Tool: {span.name}\")\n    print(f\"  Duration: {span.duration_ms:.0f}ms\")\n    print(f\"  Status: {span.status}\")\n    print(f\"  Executed: {span.attributes.get('executed', False)}\")\n</code></pre>"},{"location":"examples/replay-with-tools/#troubleshooting","title":"Troubleshooting","text":""},{"location":"examples/replay-with-tools/#issue-tools-not-executing","title":"Issue: Tools Not Executing","text":"<p>Symptoms: Tools use cached outputs despite enable_tool_execution=True.</p> <p>Solutions: 1. Verify tool is in registry:    <pre><code>print(tool_registry.keys())  # Check tool name\n</code></pre></p> <ol> <li> <p>Check allowlist/blocklist:    <pre><code>enable_tool_execution={\"allowlist\": [\"exact_tool_name\"]}  # Must match\n</code></pre></p> </li> <li> <p>Ensure tool signature is correct:    <pre><code>def my_tool(input: str) -&gt; str:  # Must accept str, return str\n    return result\n</code></pre></p> </li> </ol>"},{"location":"examples/replay-with-tools/#issue-tool-errors-not-captured","title":"Issue: Tool Errors Not Captured","text":"<p>Symptoms: Tool failures crash replay instead of being captured.</p> <p>Solutions: 1. Tool errors are automatically captured - verify span status:    <pre><code>for span in result.spans:\n    if span.span_type == \"tool\":\n        print(f\"{span.name}: {span.status}\")  # Should show ERROR\n</code></pre></p> <ol> <li>Check error attributes:    <pre><code>print(span.attributes.get(\"error.type\"))\nprint(span.attributes.get(\"error.message\"))\n</code></pre></li> </ol>"},{"location":"examples/replay-with-tools/#issue-mocks-not-working","title":"Issue: Mocks Not Working","text":"<p>Symptoms: Tool executes instead of using mock.</p> <p>Solutions: 1. Verify mock key matches tool name:    <pre><code>tool_mocks = {\"exact_tool_name\": \"mocked output\"}  # Must match\n</code></pre></p> <ol> <li>Mocks have highest priority - they override execution:    <pre><code># This should use mock, not execute\nengine = ReplayEngine(\n    trace,\n    tool_registry={\"tool\": tool_fn},\n    enable_tool_execution=True,\n    tool_mocks={\"tool\": \"mock\"},  # Mock wins\n)\n</code></pre></li> </ol>"},{"location":"examples/replay-with-tools/#next-steps","title":"Next Steps","text":"<ul> <li>Replay Advanced Features - API retry, semantic fallback</li> <li>Replay Multi-Agent Examples - Replay with CrewAI, AutoGen, LangGraph, Swarm</li> <li>Basic Replay - Core replay concepts</li> <li>CLI Replay Commands - Command-line replay interface</li> </ul>"},{"location":"examples/replay/","title":"Replay Examples","text":"<p>Practical examples for deterministic replay and A/B testing.</p>"},{"location":"examples/replay/#basic-exact-replay","title":"Basic Exact Replay","text":"<p>Re-execute a captured trace without making API calls:</p> <pre><code>from prela.replay import ReplayEngine\nfrom prela.replay.loader import TraceLoader\n\n# Load trace from file\ntrace = TraceLoader.from_file(\"traces.jsonl\")\n\n# Create replay engine\nengine = ReplayEngine(trace)\n\n# Exact replay (deterministic, no API calls)\nresult = engine.replay_exact()\n\n# Inspect results\nprint(f\"Trace ID: {result.trace_id}\")\nprint(f\"Total Spans: {len(result.spans)}\")\nprint(f\"Duration: {result.total_duration_ms:.2f}ms\")\nprint(f\"Total Tokens: {result.total_tokens}\")\nprint(f\"Estimated Cost: ${result.total_cost_usd:.4f}\")\n\n# Examine individual spans\nfor span in result.spans:\n    print(f\"\\nSpan: {span.name}\")\n    print(f\"  Type: {span.span_type}\")\n    print(f\"  Duration: {span.duration_ms:.2f}ms\")\n    print(f\"  Output: {span.output[:100]}...\")  # First 100 chars\n</code></pre>"},{"location":"examples/replay/#ab-testing-compare-models","title":"A/B Testing: Compare Models","text":"<p>Test GPT-4 vs Claude Sonnet:</p> <pre><code>from prela.replay import ReplayEngine, compare_replays\nfrom prela.replay.loader import TraceLoader\n\n# Load trace (originally GPT-4)\ntrace = TraceLoader.from_file(\"gpt4_trace.jsonl\")\nengine = ReplayEngine(trace)\n\n# Baseline: Original execution\noriginal = engine.replay_exact()\n\n# Experiment: Claude Sonnet\nclaude_result = engine.replay_with_modifications(\n    model=\"claude-sonnet-4-20250514\"\n)\n\n# Compare\ncomparison = compare_replays(original, claude_result)\n\n# Print summary\nprint(comparison.generate_summary())\n\n# Detailed analysis\nprint(\"\\n=== Cost Analysis ===\")\nprint(f\"GPT-4 Cost: ${original.total_cost_usd:.4f}\")\nprint(f\"Claude Cost: ${claude_result.total_cost_usd:.4f}\")\nprint(f\"Savings: ${original.total_cost_usd - claude_result.total_cost_usd:.4f}\")\n\nprint(\"\\n=== Quality Analysis ===\")\nfor diff in comparison.differences:\n    if diff.field == \"output\" and diff.semantic_similarity:\n        print(f\"{diff.span_name}:\")\n        print(f\"  Semantic Similarity: {diff.semantic_similarity:.1%}\")\n        if diff.semantic_similarity &gt; 0.85:\n            print(\"  \u2713 High quality match\")\n        else:\n            print(\"  \u26a0\ufe0f Significant divergence\")\n</code></pre>"},{"location":"examples/replay/#parameter-tuning-temperature","title":"Parameter Tuning: Temperature","text":"<p>Find optimal temperature setting:</p> <pre><code>from prela.replay import ReplayEngine\nfrom prela.replay.loader import TraceLoader\n\ntrace = TraceLoader.from_file(\"trace.jsonl\")\nengine = ReplayEngine(trace)\n\n# Test different temperatures\ntemperatures = [0.0, 0.3, 0.5, 0.7, 1.0]\nresults = {}\n\nfor temp in temperatures:\n    result = engine.replay_with_modifications(temperature=temp)\n    results[temp] = result\n\n# Compare outputs\nprint(\"Temperature Comparison\")\nprint(\"=\" * 50)\n\nfor temp, result in results.items():\n    print(f\"\\nTemperature: {temp}\")\n    print(f\"  Tokens: {result.total_tokens}\")\n    print(f\"  Cost: ${result.total_cost_usd:.4f}\")\n    print(f\"  Duration: {result.total_duration_ms:.0f}ms\")\n\n    # Show first span output\n    if result.spans:\n        output = result.spans[0].output\n        print(f\"  Output preview: {output[:80]}...\")\n</code></pre>"},{"location":"examples/replay/#batch-model-comparison","title":"Batch Model Comparison","text":"<p>Compare multiple models and configurations:</p> <pre><code>from prela.replay import ReplayEngine, compare_replays\nfrom prela.replay.loader import TraceLoader\nimport json\n\ntrace = TraceLoader.from_file(\"trace.jsonl\")\nengine = ReplayEngine(trace)\n\n# Baseline\nbaseline = engine.replay_exact()\n\n# Experiments\nexperiments = {\n    \"gpt-4o\": {\"model\": \"gpt-4o\"},\n    \"gpt-4o-temp0.7\": {\"model\": \"gpt-4o\", \"temperature\": 0.7},\n    \"claude-sonnet\": {\"model\": \"claude-sonnet-4-20250514\"},\n    \"claude-haiku\": {\"model\": \"claude-3-haiku-20240307\"},\n}\n\nresults = {}\ncomparisons = {}\n\n# Run experiments\nfor name, modifications in experiments.items():\n    print(f\"Running: {name}...\")\n    result = engine.replay_with_modifications(**modifications)\n    results[name] = result\n    comparisons[name] = compare_replays(baseline, result)\n\n# Generate report\nreport = {\n    \"baseline\": {\n        \"cost\": baseline.total_cost_usd,\n        \"tokens\": baseline.total_tokens,\n        \"duration_ms\": baseline.total_duration_ms,\n    },\n    \"experiments\": {}\n}\n\nfor name, result in results.items():\n    comparison = comparisons[name]\n\n    report[\"experiments\"][name] = {\n        \"cost\": result.total_cost_usd,\n        \"cost_delta\": result.total_cost_usd - baseline.total_cost_usd,\n        \"tokens\": result.total_tokens,\n        \"tokens_delta\": result.total_tokens - baseline.total_tokens,\n        \"duration_ms\": result.total_duration_ms,\n        \"avg_similarity\": sum(\n            d.semantic_similarity or 0\n            for d in comparison.differences\n            if d.semantic_similarity\n        ) / len([d for d in comparison.differences if d.semantic_similarity])\n        if [d for d in comparison.differences if d.semantic_similarity]\n        else None,\n    }\n\n# Save report\nwith open(\"experiment_report.json\", \"w\") as f:\n    json.dump(report, f, indent=2)\n\nprint(\"\\n=== Experiment Report ===\")\nprint(json.dumps(report, indent=2))\n\n# Find best option\nbest_cost = min(results.items(), key=lambda x: x[1].total_cost_usd)\nprint(f\"\\nLowest cost: {best_cost[0]} (${best_cost[1].total_cost_usd:.4f})\")\n</code></pre>"},{"location":"examples/replay/#system-prompt-testing","title":"System Prompt Testing","text":"<p>Test different instruction styles:</p> <pre><code>from prela.replay import ReplayEngine, compare_replays\nfrom prela.replay.loader import TraceLoader\n\ntrace = TraceLoader.from_file(\"trace.jsonl\")\nengine = ReplayEngine(trace)\n\n# Original\noriginal = engine.replay_exact()\n\n# Test variations\nprompts = {\n    \"concise\": \"You are a helpful assistant. Be concise and direct.\",\n    \"detailed\": \"You are a helpful assistant. Provide detailed explanations with examples.\",\n    \"technical\": \"You are a technical expert. Use precise terminology and cite sources.\",\n}\n\nresults = {}\n\nfor name, prompt in prompts.items():\n    result = engine.replay_with_modifications(system_prompt=prompt)\n    results[name] = result\n\n# Analyze outputs\nprint(\"System Prompt Comparison\")\nprint(\"=\" * 50)\n\nfor name, result in results.items():\n    comparison = compare_replays(original, result)\n\n    print(f\"\\n{name.upper()}\")\n    print(f\"  Prompt: {prompts[name]}\")\n    print(f\"  Output length: {len(result.spans[0].output) if result.spans else 0} chars\")\n    print(f\"  Tokens: {result.total_tokens}\")\n    print(f\"  Cost: ${result.total_cost_usd:.4f}\")\n\n    # Semantic similarity\n    similarities = [\n        d.semantic_similarity\n        for d in comparison.differences\n        if d.semantic_similarity\n    ]\n    if similarities:\n        avg_sim = sum(similarities) / len(similarities)\n        print(f\"  Avg similarity: {avg_sim:.1%}\")\n</code></pre>"},{"location":"examples/replay/#regression-testing","title":"Regression Testing","text":"<p>Ensure new model versions don't break functionality:</p> <pre><code>from prela.replay import ReplayEngine, compare_replays\nfrom prela.replay.loader import TraceLoader\n\ndef regression_test(trace_file, new_model, similarity_threshold=0.85):\n    \"\"\"\n    Test if new model produces similar results to original.\n\n    Returns:\n        dict: Test results with pass/fail status\n    \"\"\"\n    trace = TraceLoader.from_file(trace_file)\n    engine = ReplayEngine(trace)\n\n    # Baseline\n    original = engine.replay_exact()\n\n    # New version\n    new_result = engine.replay_with_modifications(model=new_model)\n\n    # Compare\n    comparison = compare_replays(original, new_result)\n\n    # Analyze\n    passed_spans = []\n    failed_spans = []\n\n    for diff in comparison.differences:\n        if diff.field == \"output\" and diff.semantic_similarity:\n            if diff.semantic_similarity &gt;= similarity_threshold:\n                passed_spans.append({\n                    \"name\": diff.span_name,\n                    \"similarity\": diff.semantic_similarity,\n                })\n            else:\n                failed_spans.append({\n                    \"name\": diff.span_name,\n                    \"similarity\": diff.semantic_similarity,\n                    \"original\": diff.original_value[:200],\n                    \"new\": diff.new_value[:200],\n                })\n\n    # Results\n    total_spans = len(passed_spans) + len(failed_spans)\n    pass_rate = len(passed_spans) / total_spans if total_spans &gt; 0 else 0\n\n    return {\n        \"passed\": pass_rate &gt;= 0.95,  # 95% pass rate required\n        \"pass_rate\": pass_rate,\n        \"passed_spans\": len(passed_spans),\n        \"failed_spans\": len(failed_spans),\n        \"failures\": failed_spans,\n    }\n\n\n# Run regression tests\ntest_files = [\n    \"test_cases/customer_support.jsonl\",\n    \"test_cases/code_review.jsonl\",\n    \"test_cases/data_analysis.jsonl\",\n]\n\nfor test_file in test_files:\n    print(f\"\\nTesting: {test_file}\")\n    result = regression_test(test_file, new_model=\"gpt-4o\")\n\n    if result[\"passed\"]:\n        print(f\"\u2713 PASSED ({result['pass_rate']:.1%} similarity)\")\n    else:\n        print(f\"\u2717 FAILED ({result['pass_rate']:.1%} similarity)\")\n        print(f\"  Failed spans: {result['failed_spans']}\")\n        for failure in result[\"failures\"]:\n            print(f\"\\n  - {failure['name']} ({failure['similarity']:.1%})\")\n            print(f\"    Original: {failure['original']}...\")\n            print(f\"    New: {failure['new']}...\")\n</code></pre>"},{"location":"examples/replay/#cost-optimization","title":"Cost Optimization","text":"<p>Find the cheapest model that maintains quality:</p> <pre><code>from prela.replay import ReplayEngine, compare_replays\nfrom prela.replay.loader import TraceLoader\n\ndef find_optimal_model(trace_file, models, min_similarity=0.85):\n    \"\"\"\n    Find cheapest model with acceptable quality.\n\n    Args:\n        trace_file: Path to trace file\n        models: List of model names to test\n        min_similarity: Minimum semantic similarity threshold\n\n    Returns:\n        dict: Best model and cost analysis\n    \"\"\"\n    trace = TraceLoader.from_file(trace_file)\n    engine = ReplayEngine(trace)\n\n    # Baseline\n    original = engine.replay_exact()\n\n    # Test models\n    candidates = []\n\n    for model in models:\n        result = engine.replay_with_modifications(model=model)\n        comparison = compare_replays(original, result)\n\n        # Calculate average similarity\n        similarities = [\n            d.semantic_similarity\n            for d in comparison.differences\n            if d.semantic_similarity\n        ]\n        avg_similarity = sum(similarities) / len(similarities) if similarities else 0\n\n        if avg_similarity &gt;= min_similarity:\n            candidates.append({\n                \"model\": model,\n                \"cost\": result.total_cost_usd,\n                \"similarity\": avg_similarity,\n                \"tokens\": result.total_tokens,\n            })\n\n    # Sort by cost (cheapest first)\n    candidates.sort(key=lambda x: x[\"cost\"])\n\n    return {\n        \"original_cost\": original.total_cost_usd,\n        \"original_model\": original.spans[0].attributes.get(\"llm.model\") if original.spans else None,\n        \"candidates\": candidates,\n        \"best_option\": candidates[0] if candidates else None,\n    }\n\n\n# Test model lineup\nmodels_to_test = [\n    \"gpt-4o\",\n    \"gpt-4o-mini\",\n    \"claude-sonnet-4-20250514\",\n    \"claude-3-haiku-20240307\",\n]\n\nresult = find_optimal_model(\"trace.jsonl\", models_to_test)\n\nprint(\"Cost Optimization Results\")\nprint(\"=\" * 50)\nprint(f\"Original: {result['original_model']} (${result['original_cost']:.4f})\")\n\nif result[\"best_option\"]:\n    best = result[\"best_option\"]\n    savings = result[\"original_cost\"] - best[\"cost\"]\n    savings_pct = (savings / result[\"original_cost\"]) * 100\n\n    print(f\"\\n\u2713 Best Option: {best['model']}\")\n    print(f\"  Cost: ${best['cost']:.4f}\")\n    print(f\"  Savings: ${savings:.4f} ({savings_pct:.1f}%)\")\n    print(f\"  Quality: {best['similarity']:.1%} similar\")\n    print(f\"  Tokens: {best['tokens']}\")\n\n    print(\"\\nAll Candidates:\")\n    for candidate in result[\"candidates\"]:\n        print(f\"  - {candidate['model']}: ${candidate['cost']:.4f} ({candidate['similarity']:.1%})\")\nelse:\n    print(\"\\n\u2717 No models met quality threshold\")\n</code></pre>"},{"location":"examples/replay/#mock-tool-responses","title":"Mock Tool Responses","text":"<p>Test different tool outputs:</p> <pre><code>from prela.replay import ReplayEngine\nfrom prela.replay.loader import TraceLoader\n\ntrace = TraceLoader.from_file(\"trace_with_tools.jsonl\")\nengine = ReplayEngine(trace)\n\n# Original execution\noriginal = engine.replay_exact()\n\n# Test with different tool responses\nmock_responses = {\n    \"search\": {\n        \"results\": [\n            {\"title\": \"Alternative Result 1\", \"url\": \"https://example.com/1\"},\n            {\"title\": \"Alternative Result 2\", \"url\": \"https://example.com/2\"},\n        ]\n    },\n    \"calculator\": {\n        \"result\": 42  # Different calculation result\n    },\n}\n\nmodified = engine.replay_with_modifications(\n    mock_tool_responses=mock_responses\n)\n\n# Compare how agent adapts to different tool outputs\nprint(\"Tool Response Testing\")\nprint(\"=\" * 50)\n\nprint(\"\\nOriginal Tool Outputs:\")\nfor span in original.spans:\n    if span.span_type == \"tool\":\n        print(f\"  {span.name}: {span.output[:100]}...\")\n\nprint(\"\\nModified Tool Outputs:\")\nfor span in modified.spans:\n    if span.span_type == \"tool\":\n        print(f\"  {span.name}: {span.output[:100]}...\")\n\nprint(\"\\nAgent Behavior Changes:\")\n# Compare final outputs\noriginal_output = original.spans[-1].output if original.spans else \"\"\nmodified_output = modified.spans[-1].output if modified.spans else \"\"\n\nprint(f\"Original: {original_output[:200]}...\")\nprint(f\"Modified: {modified_output[:200]}...\")\n</code></pre>"},{"location":"examples/replay/#tool-re-execution","title":"Tool Re-execution","text":"<p>Re-execute tools during replay with safety controls.</p>"},{"location":"examples/replay/#basic-tool-re-execution","title":"Basic Tool Re-execution","text":"<pre><code>from prela.replay import ReplayEngine\nfrom prela.replay.loader import TraceLoader\n\n# Load trace with tool calls\ntrace = TraceLoader.from_file(\"trace_with_tools.jsonl\")\nengine = ReplayEngine(trace)\n\n# Define tool implementations\ndef my_calculator(input_data):\n    \"\"\"Calculate sum of two numbers.\"\"\"\n    a = input_data.get(\"a\", 0)\n    b = input_data.get(\"b\", 0)\n    result = a + b\n    print(f\"Calculator: {a} + {b} = {result}\")\n    return {\"result\": result}\n\ndef my_search(input_data):\n    \"\"\"Search for information.\"\"\"\n    query = input_data.get(\"query\", \"\")\n    print(f\"Searching for: {query}\")\n    # Simulate fresh search\n    return {\n        \"results\": [\n            {\"title\": \"Fresh Result 1\", \"url\": \"https://example.com/1\"},\n            {\"title\": \"Fresh Result 2\", \"url\": \"https://example.com/2\"},\n        ]\n    }\n\n# Create tool registry\ntool_registry = {\n    \"calculator\": my_calculator,\n    \"search\": my_search,\n}\n\n# Original execution (cached data)\noriginal = engine.replay_exact()\n\n# Re-execute tools with current implementations\nmodified = engine.replay_with_modifications(\n    enable_tool_execution=True,\n    tool_registry=tool_registry,\n)\n\n# Compare results\nprint(\"\\nTool Re-execution Comparison\")\nprint(\"=\" * 60)\n\nfor orig_span, mod_span in zip(original.spans, modified.spans):\n    if orig_span.span_type == \"tool\":\n        print(f\"\\n{orig_span.name}:\")\n        print(f\"  Cached:  {orig_span.output}\")\n        print(f\"  Fresh:   {mod_span.output}\")\n        print(f\"  Changed: {orig_span.output != mod_span.output}\")\n</code></pre>"},{"location":"examples/replay/#using-allowlist-for-safety","title":"Using Allowlist for Safety","text":"<pre><code>from prela.replay import ReplayEngine\nfrom prela.replay.loader import TraceLoader\n\ntrace = TraceLoader.from_file(\"trace_with_tools.jsonl\")\nengine = ReplayEngine(trace)\n\n# Define tools\ndef safe_calculator(input_data):\n    return {\"result\": input_data[\"a\"] + input_data[\"b\"]}\n\ndef dangerous_delete(input_data):\n    # This should never execute!\n    raise RuntimeError(\"This tool should be blocked!\")\n\ntool_registry = {\n    \"calculator\": safe_calculator,\n    \"delete_file\": dangerous_delete,\n}\n\n# Only allow calculator to execute\nresult = engine.replay_with_modifications(\n    enable_tool_execution=True,\n    tool_execution_allowlist=[\"calculator\"],  # Only allow this\n    tool_registry=tool_registry,\n)\n\n# Check which tools ran\nfor span in result.spans:\n    if span.span_type == \"tool\":\n        if span.error:\n            print(f\"\u274c {span.name}: {span.error}\")\n        else:\n            print(f\"\u2705 {span.name}: {span.output}\")\n\n# Output:\n# \u2705 calculator: {'result': 10}\n# \u274c delete_file: Tool 'delete_file' not in allowlist\n</code></pre>"},{"location":"examples/replay/#using-blocklist-for-safety","title":"Using Blocklist for Safety","text":"<pre><code># Block dangerous tools explicitly\nresult = engine.replay_with_modifications(\n    enable_tool_execution=True,\n    tool_execution_blocklist=[\"delete_file\", \"shutdown\", \"execute_code\"],\n    tool_registry=tool_registry,\n)\n\n# Blocked tools will fail with error\nfor span in result.spans:\n    if span.span_type == \"tool\" and span.error:\n        print(f\"\ud83d\udeab Blocked: {span.name}\")\n</code></pre>"},{"location":"examples/replay/#testing-with-mock-vs-real-execution","title":"Testing with Mock vs Real Execution","text":"<pre><code># Priority 1: Mocks (highest)\nmock_result = engine.replay_with_modifications(\n    mock_tool_responses={\n        \"search\": {\"results\": [\"Mocked result\"]}\n    }\n)\n\n# Priority 2: Real execution (if no mocks)\nreal_result = engine.replay_with_modifications(\n    enable_tool_execution=True,\n    tool_registry=tool_registry,\n)\n\n# Priority 3: Cached (default, if execution not enabled)\ncached_result = engine.replay_exact()\n\nprint(\"Mock output:\", mock_result.spans[0].output)\nprint(\"Real output:\", real_result.spans[0].output)\nprint(\"Cached output:\", cached_result.spans[0].output)\n</code></pre>"},{"location":"examples/replay/#retrieval-re-execution","title":"Retrieval Re-execution","text":"<p>Re-query vector databases to test with updated data.</p>"},{"location":"examples/replay/#chromadb-re-execution","title":"ChromaDB Re-execution","text":"<pre><code>from prela.replay import ReplayEngine\nfrom prela.replay.loader import TraceLoader\nimport chromadb\n\n# Load trace with retrieval spans\ntrace = TraceLoader.from_file(\"trace_with_retrieval.jsonl\")\nengine = ReplayEngine(trace)\n\n# Setup ChromaDB with fresh data\nclient = chromadb.Client()\ncollection = client.create_collection(\"my_docs\")\n\n# Add updated documents\ncollection.add(\n    documents=[\n        \"Python is a high-level programming language\",\n        \"JavaScript is widely used for web development\",\n        \"Rust provides memory safety without garbage collection\",\n    ],\n    ids=[\"1\", \"2\", \"3\"],\n)\n\n# Original execution (cached documents)\noriginal = engine.replay_exact()\n\n# Re-query with current vector store\nmodified = engine.replay_with_modifications(\n    enable_retrieval_execution=True,\n    retrieval_client=collection,\n)\n\n# Compare retrieved documents\nprint(\"Retrieval Comparison\")\nprint(\"=\" * 60)\n\nfor orig_span, mod_span in zip(original.spans, modified.spans):\n    if orig_span.span_type == \"retrieval\":\n        print(f\"\\nQuery: {orig_span.input}\")\n        print(f\"\\nOriginal Documents ({len(orig_span.output)}):\")\n        for doc in orig_span.output[:2]:\n            print(f\"  - {doc.get('text', '')[:80]}...\")\n\n        print(f\"\\nFresh Documents ({len(mod_span.output)}):\")\n        for doc in mod_span.output[:2]:\n            print(f\"  - {doc.get('text', '')[:80]}...\")\n</code></pre>"},{"location":"examples/replay/#query-override","title":"Query Override","text":"<pre><code># Original query: \"What is Python?\"\n# Test with different query\nresult = engine.replay_with_modifications(\n    enable_retrieval_execution=True,\n    retrieval_client=collection,\n    retrieval_query_override=\"What is JavaScript?\",\n)\n\nprint(f\"Original query: {original.spans[0].input}\")\nprint(f\"New query: {result.spans[0].input}\")\nprint(f\"Results changed: {original.spans[0].output != result.spans[0].output}\")\n</code></pre>"},{"location":"examples/replay/#ab-testing-retrieval-strategies","title":"A/B Testing Retrieval Strategies","text":"<pre><code># Test multiple query formulations\nqueries = [\n    \"Python programming language features\",  # Detailed\n    \"Python features\",                        # Concise\n    \"What makes Python popular?\",            # Question form\n]\n\nresults = {}\nfor query in queries:\n    result = engine.replay_with_modifications(\n        enable_retrieval_execution=True,\n        retrieval_client=collection,\n        retrieval_query_override=query,\n    )\n\n    # Analyze results\n    results[query] = {\n        \"doc_count\": len(result.spans[0].output),\n        \"avg_score\": sum(d.get(\"score\", 0) for d in result.spans[0].output) / len(result.spans[0].output) if result.spans[0].output else 0,\n    }\n\n# Find best query\nbest_query = max(results.items(), key=lambda x: x[1][\"avg_score\"])\nprint(f\"\\nBest Query: {best_query[0]}\")\nprint(f\"Avg Score: {best_query[1]['avg_score']:.2f}\")\n</code></pre>"},{"location":"examples/replay/#custom-retry-configuration","title":"Custom Retry Configuration","text":"<p>Configure retry behavior for different scenarios.</p>"},{"location":"examples/replay/#high-retry-configuration-flaky-networks","title":"High-Retry Configuration (Flaky Networks)","text":"<pre><code>from prela.replay import ReplayEngine\nfrom prela.replay.loader import TraceLoader\n\ntrace = TraceLoader.from_file(\"trace.jsonl\")\n\n# More retries, longer delays for unreliable connections\nengine = ReplayEngine(\n    trace,\n    max_retries=5,              # More attempts\n    retry_initial_delay=2.0,    # Start with 2s delay\n    retry_max_delay=120.0,      # Allow up to 2 minutes\n    retry_exponential_base=2.0,\n)\n\nresult = engine.replay_with_modifications(model=\"gpt-4o\")\n\n# Check retry statistics\ntotal_retries = sum(span.retry_count for span in result.spans)\nprint(f\"Total retries needed: {total_retries}\")\n\n# Show which spans needed retries\nfor span in result.spans:\n    if span.retry_count &gt; 0:\n        print(f\"\u26a0\ufe0f  {span.name}: {span.retry_count} retries\")\n</code></pre>"},{"location":"examples/replay/#fast-fail-configuration-production","title":"Fast-Fail Configuration (Production)","text":"<pre><code># Minimal retries for production (fail fast)\nengine = ReplayEngine(\n    trace,\n    max_retries=1,           # Only 1 retry\n    retry_initial_delay=0.5, # Quick retry\n    retry_max_delay=2.0,     # Max 2s delay\n)\n\nresult = engine.replay_with_modifications(model=\"gpt-4o\")\n</code></pre>"},{"location":"examples/replay/#monitoring-retry-patterns","title":"Monitoring Retry Patterns","text":"<pre><code>import time\n\nstart = time.time()\n\nresult = engine.replay_with_modifications(model=\"gpt-4o\")\n\nelapsed = time.time() - start\n\n# Analyze retry impact\nretry_stats = {\n    \"total_spans\": len(result.spans),\n    \"spans_with_retries\": sum(1 for s in result.spans if s.retry_count &gt; 0),\n    \"total_retries\": sum(s.retry_count for s in result.spans),\n    \"elapsed_time\": elapsed,\n}\n\nprint(\"\\nRetry Statistics\")\nprint(\"=\" * 60)\nprint(f\"Total Spans: {retry_stats['total_spans']}\")\nprint(f\"Spans with Retries: {retry_stats['spans_with_retries']}\")\nprint(f\"Total Retry Attempts: {retry_stats['total_retries']}\")\nprint(f\"Elapsed Time: {retry_stats['elapsed_time']:.2f}s\")\n\n# Calculate estimated retry overhead\nbase_time = result.total_duration_ms / 1000\nretry_overhead = elapsed - base_time\nprint(f\"Estimated Retry Overhead: {retry_overhead:.2f}s\")\n</code></pre>"},{"location":"examples/replay/#semantic-fallback-example","title":"Semantic Fallback Example","text":"<p>Compare accuracy with and without sentence-transformers.</p>"},{"location":"examples/replay/#using-fallback-no-dependencies","title":"Using Fallback (No Dependencies)","text":"<pre><code>from prela.replay import ReplayEngine, compare_replays\nfrom prela.replay.loader import TraceLoader\n\ntrace = TraceLoader.from_file(\"trace.jsonl\")\nengine = ReplayEngine(trace)\n\n# Original execution\noriginal = engine.replay_exact()\n\n# Modified execution\nmodified = engine.replay_with_modifications(\n    model=\"gpt-4o\",\n    temperature=0.7,\n)\n\n# Compare (will use fallback if sentence-transformers not installed)\ncomparison = compare_replays(original, modified)\n\n# Check which method was used\nif comparison.semantic_similarity_available:\n    print(f\"\u2705 Using embeddings: {comparison.semantic_similarity_model}\")\nelse:\n    print(\"\u26a0\ufe0f  Using fallback: difflib + Jaccard\")\n\n# Show similarities\nprint(\"\\nSemantic Similarities:\")\nfor diff in comparison.differences:\n    if diff.field == \"output\" and diff.semantic_similarity:\n        method = \"embeddings\" if comparison.semantic_similarity_available else \"fallback\"\n        print(f\"{diff.span_name}: {diff.semantic_similarity:.1%} ({method})\")\n</code></pre>"},{"location":"examples/replay/#accuracy-comparison","title":"Accuracy Comparison","text":"<pre><code># Test fallback accuracy\ntest_pairs = [\n    (\"Hello World\", \"hello world\"),           # Case change\n    (\"The quick brown fox\", \"the fast brown fox\"),  # Word change\n    (\"cat dog bird\", \"dog bird cat\"),         # Word reorder\n    (\"Python programming\", \"JavaScript programming\"),  # Different topic\n]\n\nprint(\"\\nFallback Accuracy Test\")\nprint(\"=\" * 60)\n\nfrom prela.replay.comparison import ReplayComparator\n\n# Use fallback explicitly\ncomparator = ReplayComparator(use_semantic_similarity=False)\n\nfor text1, text2 in test_pairs:\n    similarity = comparator._compute_fallback_similarity(text1, text2)\n    print(f\"\\n'{text1}' vs '{text2}'\")\n    print(f\"  Similarity: {similarity:.2%}\")\n\n    if similarity &gt; 0.9:\n        status = \"\u2705 Highly similar\"\n    elif similarity &gt; 0.7:\n        status = \"\u26a0\ufe0f  Moderately similar\"\n    else:\n        status = \"\u274c Different\"\n    print(f\"  Status: {status}\")\n</code></pre>"},{"location":"examples/replay/#when-to-install-sentence-transformers","title":"When to Install sentence-transformers","text":"<pre><code># Check if you need better accuracy\ncomparison = compare_replays(original, modified)\n\nlow_confidence_count = sum(\n    1 for diff in comparison.differences\n    if diff.semantic_similarity and 0.6 &lt; diff.semantic_similarity &lt; 0.8\n)\n\nif low_confidence_count &gt; 5 and not comparison.semantic_similarity_available:\n    print(\"\u26a0\ufe0f  Consider installing sentence-transformers for better accuracy\")\n    print(\"   pip install prela[similarity]\")\n    print(f\"   {low_confidence_count} similarities in ambiguous range (60-80%)\")\nelse:\n    print(\"\u2705 Fallback accuracy is sufficient for this use case\")\n</code></pre>"},{"location":"examples/replay/#cli-workflow","title":"CLI Workflow","text":"<p>Using the command-line interface:</p>"},{"location":"examples/replay/#1-capture-trace-with-replay-data","title":"1. Capture Trace with Replay Data","text":"<pre><code># Enable replay capture in your application\nexport PRELA_CAPTURE_FOR_REPLAY=true\n\n# Run your application\npython my_agent.py\n\n# Traces saved to traces.jsonl with replay data\n</code></pre>"},{"location":"examples/replay/#2-exact-replay-verify","title":"2. Exact Replay (Verify)","text":"<pre><code># Quick verification\nprela replay traces.jsonl\n\n# Output:\n# Trace ID: abc-123\n# Duration: 2.5s\n# Tokens: 1,234\n# Cost: $0.0185\n</code></pre>"},{"location":"examples/replay/#3-modified-replay-experiment","title":"3. Modified Replay (Experiment)","text":"<pre><code># Test with GPT-4o\nprela replay traces.jsonl --model gpt-4o --compare\n\n# Output:\n# Original: gpt-4 ($0.0185)\n# Modified: gpt-4o ($0.0092)\n# Savings: $0.0093 (50.3%)\n# Avg Similarity: 87.2%\n</code></pre>"},{"location":"examples/replay/#4-save-results","title":"4. Save Results","text":"<pre><code># Export comparison to JSON\nprela replay traces.jsonl \\\n  --model gpt-4o \\\n  --temperature 0.7 \\\n  --compare \\\n  --output experiment_results.json\n</code></pre>"},{"location":"examples/replay/#5-batch-processing","title":"5. Batch Processing","text":"<pre><code># Process multiple traces\nfor trace in test_cases/*.jsonl; do\n  echo \"Processing: $trace\"\n  prela replay \"$trace\" --model gpt-4o --compare\ndone\n</code></pre>"},{"location":"examples/replay/#cicd-integration","title":"CI/CD Integration","text":"<p>Automate regression testing in CI:</p> <pre><code># .github/workflows/regression_test.yml\nname: Regression Test\n\non:\n  pull_request:\n    branches: [main]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n\n    steps:\n      - uses: actions/checkout@v3\n\n      - name: Set up Python\n        uses: actions/setup-python@v4\n        with:\n          python-version: '3.11'\n\n      - name: Install dependencies\n        run: |\n          pip install prela sentence-transformers\n\n      - name: Run regression tests\n        env:\n          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}\n          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}\n        run: |\n          python scripts/regression_test.py\n\n      - name: Upload results\n        if: always()\n        uses: actions/upload-artifact@v3\n        with:\n          name: regression-results\n          path: regression_results/\n</code></pre> <p>scripts/regression_test.py:</p> <pre><code>#!/usr/bin/env python3\n\"\"\"Regression test runner for CI.\"\"\"\n\nimport glob\nimport json\nimport sys\nfrom pathlib import Path\n\nfrom prela.replay import ReplayEngine, compare_replays\nfrom prela.replay.loader import TraceLoader\n\n\ndef main():\n    test_cases = glob.glob(\"test_traces/*.jsonl\")\n    results = []\n    failed = []\n\n    for test_file in test_cases:\n        print(f\"Testing: {test_file}\")\n\n        trace = TraceLoader.from_file(test_file)\n        engine = ReplayEngine(trace)\n\n        original = engine.replay_exact()\n        modified = engine.replay_with_modifications(model=\"gpt-4o\")\n\n        comparison = compare_replays(original, modified)\n\n        # Check similarity threshold\n        similarities = [\n            d.semantic_similarity\n            for d in comparison.differences\n            if d.semantic_similarity\n        ]\n        avg_similarity = sum(similarities) / len(similarities) if similarities else 0\n\n        passed = avg_similarity &gt;= 0.85\n\n        result = {\n            \"test\": test_file,\n            \"passed\": passed,\n            \"similarity\": avg_similarity,\n            \"cost_delta\": modified.total_cost_usd - original.total_cost_usd,\n        }\n        results.append(result)\n\n        if not passed:\n            failed.append(result)\n\n    # Save results\n    Path(\"regression_results\").mkdir(exist_ok=True)\n    with open(\"regression_results/summary.json\", \"w\") as f:\n        json.dump(results, f, indent=2)\n\n    # Print summary\n    print(\"\\n\" + \"=\" * 50)\n    print(f\"Passed: {len(results) - len(failed)}/{len(results)}\")\n    print(f\"Failed: {len(failed)}\")\n\n    if failed:\n        print(\"\\nFailures:\")\n        for f in failed:\n            print(f\"  - {f['test']} ({f['similarity']:.1%})\")\n        sys.exit(1)\n\n    print(\"\\n\u2713 All regression tests passed!\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"examples/replay/#next-steps","title":"Next Steps","text":"<ul> <li>Replay Concepts: Understand replay fundamentals</li> <li>CLI Reference: Complete CLI documentation</li> <li>API Reference: Detailed API documentation</li> </ul>"},{"location":"examples/test-scenarios/","title":"Production-Validated Test Scenarios","text":"<p>These test scenarios validate all core Prela SDK features with real Anthropic Claude API calls. All 21 features have been validated in Phase 4 of the SDK testing process.</p> <p>Validation Status</p> <p>\u2705 21/21 features validated (100%) \u2705 4/4 performance criteria met (100%) \u2705 4/4 documentation checks passed (100%)</p>"},{"location":"examples/test-scenarios/#overview","title":"Overview","text":"<p>The test scenarios directory contains 6 production-ready scripts that demonstrate and validate:</p> <ol> <li>File Exporter - Traces saved to <code>./test_traces</code> directory</li> <li>Console Exporter - Colored tree-structured output</li> <li>Anthropic Instrumentation - Automatic LLM call tracing</li> <li>Span Hierarchy - Parent-child span relationships</li> <li>Streaming - Streaming response capture</li> <li>Tool Calling - Tool use event capture</li> <li>Error Handling - Error status and attributes</li> <li>Replay Engine - Model switching and comparison</li> <li>Evaluation Framework - Systematic testing with assertions</li> <li>CLI Commands - All 11 CLI commands validated</li> </ol>"},{"location":"examples/test-scenarios/#quick-start","title":"Quick Start","text":""},{"location":"examples/test-scenarios/#prerequisites","title":"Prerequisites","text":"<pre><code># Set API key\nexport ANTHROPIC_API_KEY=\"sk-ant-...\"\n\n# Install SDK\ncd /Users/gw/prela/sdk\npip install -e .\n</code></pre>"},{"location":"examples/test-scenarios/#run-all-scenarios","title":"Run All Scenarios","text":"<pre><code>cd /Users/gw/prela/sdk/examples/test_scenarios\n\n# Run each scenario\npython 01_simple_success.py\npython 02_multi_step.py\npython 03_rate_limit_failure.py\npython 04_streaming.py\npython 05_tool_calling.py\npython 06_evaluation.py\n</code></pre>"},{"location":"examples/test-scenarios/#scenario-1-simple-success","title":"Scenario 1: Simple Success","text":"<p>File: <code>01_simple_success.py</code></p> <p>Validates basic LLM tracing with file exporter.</p> <pre><code>import prela\nfrom anthropic import Anthropic\n\n# Initialize with file exporter\ntracer = prela.init(\n    service_name=\"simple-success\",\n    exporter=\"file\",\n    file_path=\"./test_traces\"\n)\n\n# Make API call - automatically traced\nclient = Anthropic()\nresponse = client.messages.create(\n    model=\"claude-sonnet-4-20250514\",\n    max_tokens=100,\n    messages=[{\"role\": \"user\", \"content\": \"What is 2+2?\"}]\n)\n\nprint(f\"Response: {response.content[0].text}\")\n</code></pre> <p>Validates:</p> <ul> <li>\u2705 File exporter creates <code>./test_traces/</code> directory</li> <li>\u2705 Traces saved in JSONL format</li> <li>\u2705 Anthropic instrumentation captures all LLM calls</li> <li>\u2705 Token usage recorded (<code>llm.input_tokens</code>, <code>llm.output_tokens</code>)</li> <li>\u2705 Span attributes include model, provider, latency</li> </ul> <p>Expected Output:</p> <pre><code>\u2713 Prela initialized\n\u2713 Trace file: ./test_traces/traces-2026-01-30-001.jsonl\n\u2713 Making simple Claude API call...\n\u2713 Response: 2 + 2 equals 4.\n\u2713 Tokens: 20 in, 14 out\n\u2713 Trace saved with 1 span\n</code></pre>"},{"location":"examples/test-scenarios/#scenario-2-multi-step-workflow","title":"Scenario 2: Multi-Step Workflow","text":"<p>File: <code>02_multi_step.py</code></p> <p>Validates span hierarchy with parent-child relationships.</p> <pre><code>import prela\nfrom anthropic import Anthropic\n\ntracer = prela.init(service_name=\"multi-step\")\n\ndef research_step():\n    with tracer.span(\"step_1_research\"):\n        client = Anthropic()\n        response = client.messages.create(...)\n        return response.content[0].text\n\ndef analysis_step():\n    with tracer.span(\"step_2_analysis\"):\n        # ... similar ...\n\ndef summary_step():\n    with tracer.span(\"step_3_summary\"):\n        # ... similar ...\n\n# Parent span wraps all steps\nwith tracer.span(\"research_workflow\"):\n    results = []\n    results.append(research_step())\n    results.append(analysis_step())\n    results.append(summary_step())\n</code></pre> <p>Validates:</p> <ul> <li>\u2705 Span hierarchy with nested operations</li> <li>\u2705 Parent-child relationships via <code>parent_span_id</code></li> <li>\u2705 Context propagation across functions</li> <li>\u2705 Tree visualization with <code>prela show</code></li> </ul> <p>CLI Validation:</p> <pre><code>$ prela show &lt;trace_id&gt;\n\n\u2514\u2500 research_workflow (3.5s) \u2713\n   \u251c\u2500 step_1_research (1.2s) \u2713\n   \u251c\u2500 step_2_analysis (1.1s) \u2713\n   \u2514\u2500 step_3_summary (0.8s) \u2713\n</code></pre>"},{"location":"examples/test-scenarios/#scenario-3-rate-limit-handling","title":"Scenario 3: Rate Limit Handling","text":"<p>File: <code>03_rate_limit_failure.py</code></p> <p>Validates error capture and status tracking.</p> <pre><code>import prela\nfrom anthropic import Anthropic\n\ntracer = prela.init(service_name=\"rate-limit-test\")\n\ntry:\n    client = Anthropic(api_key=\"invalid-key\")\n    response = client.messages.create(...)\nexcept Exception as e:\n    print(f\"Error captured: {e}\")\n</code></pre> <p>Validates:</p> <ul> <li>\u2705 Error handling for API failures</li> <li>\u2705 Span status set to <code>\"error\"</code></li> <li>\u2705 Error attributes: <code>error.type</code>, <code>error.message</code>, <code>error.stack_trace</code></li> <li>\u2705 CLI <code>prela errors</code> command shows failed traces</li> </ul> <p>CLI Validation:</p> <pre><code>$ prela errors --limit 5\n\nShowing 1 error trace (from last 50):\n\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 Trace ID   \u2502 Root Span    \u2502 Duration \u2502 Status \u2502 Spans \u2502 Time                 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 abc-123... \u2502 llm call     \u2502 52ms     \u2502 error  \u2502 1     \u2502 2026-01-30 12:34:56  \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"examples/test-scenarios/#scenario-4-streaming-responses","title":"Scenario 4: Streaming Responses","text":"<p>File: <code>04_streaming.py</code></p> <p>Validates streaming LLM response capture.</p> <pre><code>import prela\nfrom anthropic import Anthropic\n\ntracer = prela.init(service_name=\"streaming-test\")\n\nclient = Anthropic()\nwith client.messages.stream(\n    model=\"claude-sonnet-4-20250514\",\n    max_tokens=100,\n    messages=[{\"role\": \"user\", \"content\": \"Tell a story\"}]\n) as stream:\n    for text in stream.text_stream:\n        print(text, end=\"\", flush=True)\n</code></pre> <p>Validates:</p> <ul> <li>\u2705 Streaming response capture</li> <li>\u2705 <code>llm.stream=true</code> attribute</li> <li>\u2705 Token usage from final message</li> <li>\u2705 Text content aggregation</li> </ul> <p>Span Attributes:</p> <pre><code>{\n  \"llm.stream\": true,\n  \"llm.prompt_tokens\": 15,\n  \"llm.completion_tokens\": 89,\n  \"llm.latency_ms\": 1234.5\n}\n</code></pre>"},{"location":"examples/test-scenarios/#scenario-5-tool-calling","title":"Scenario 5: Tool Calling","text":"<p>File: <code>05_tool_calling.py</code></p> <p>Validates LLM tool/function calling.</p> <pre><code>import prela\nfrom anthropic import Anthropic\n\ntracer = prela.init(service_name=\"tool-test\")\n\ntools = [{\n    \"name\": \"get_weather\",\n    \"description\": \"Get weather for a location\",\n    \"input_schema\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"location\": {\"type\": \"string\"}\n        }\n    }\n}]\n\nclient = Anthropic()\nresponse = client.messages.create(\n    model=\"claude-sonnet-4-20250514\",\n    max_tokens=100,\n    tools=tools,\n    messages=[{\"role\": \"user\", \"content\": \"What's the weather in SF?\"}]\n)\n</code></pre> <p>Validates:</p> <ul> <li>\u2705 Tool use detection</li> <li>\u2705 Stop reason = <code>\"tool_use\"</code></li> <li>\u2705 Tool call events with <code>tool.id</code>, <code>tool.name</code>, <code>tool.input</code></li> </ul> <p>Span Events:</p> <pre><code>{\n  \"events\": [\n    {\n      \"name\": \"tool_call\",\n      \"attributes\": {\n        \"tool.id\": \"toolu_123\",\n        \"tool.name\": \"get_weather\",\n        \"tool.input\": {\"location\": \"San Francisco\"}\n      }\n    }\n  ]\n}\n</code></pre>"},{"location":"examples/test-scenarios/#scenario-6-evaluation-framework","title":"Scenario 6: Evaluation Framework","text":"<p>File: <code>06_evaluation.py</code></p> <p>Validates systematic testing with assertions.</p> <pre><code>import prela\nfrom prela.evals import EvalCase, EvalSuite, EvalRunner\nfrom prela.evals.assertions import ContainsAssertion, RegexAssertion\n\n# Define test cases\ncases = [\n    EvalCase(\n        id=\"test_addition\",\n        name=\"Addition test\",\n        input={\"query\": \"What is 5+3?\"},\n        assertions=[\n            ContainsAssertion(text=\"8\")\n        ]\n    ),\n    # ... more cases\n]\n\n# Create suite\nsuite = EvalSuite(name=\"Math QA Tests\", cases=cases)\n\n# Run evaluation\nrunner = EvalRunner(suite, agent_function)\nresult = runner.run()\n\nprint(result.summary())\n</code></pre> <p>Validates:</p> <ul> <li>\u2705 Eval framework (EvalCase, EvalSuite, EvalRunner)</li> <li>\u2705 Assertions execute correctly</li> <li>\u2705 Tracer integration during eval runs</li> <li>\u2705 Summary report generation</li> </ul> <p>Expected Output:</p> <pre><code>Evaluation Suite: Math QA Tests\nTotal Cases: 3\nPassed: 3 (100.0%)\nFailed: 0 (0.0%)\n\nCase Results:\n  \u2713 Addition test (842ms)\n  \u2713 Complex calculation (1231ms)\n  \u2713 JSON format test (923ms)\n</code></pre>"},{"location":"examples/test-scenarios/#cli-validation","title":"CLI Validation","text":"<p>After running scenarios, verify all CLI commands:</p>"},{"location":"examples/test-scenarios/#list-traces","title":"List Traces","text":"<pre><code>$ prela list\n\nShowing 22 traces (from last 50):\n\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 Trace ID   \u2502 Root Span    \u2502 Duration \u2502 Status \u2502 Spans \u2502 Time                 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 abc-123... \u2502 simple call  \u2502 1234ms   \u2502 success\u2502 1     \u2502 2026-01-30 12:34:56  \u2502\n\u2502 def-456... \u2502 workflow     \u2502 3456ms   \u2502 success\u2502 4     \u2502 2026-01-30 12:33:21  \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"examples/test-scenarios/#show-trace-details","title":"Show Trace Details","text":"<pre><code>$ prela show abc-123\n\nTrace: abc-123 @ 12:34:56\nService: simple-success\nStatus: success\nDuration: 1234ms\nSpans: 1\n\n\u2514\u2500 anthropic.messages.create (1234ms) \u2713\n   llm.model: claude-sonnet-4-20250514\n   llm.input_tokens: 20\n   llm.output_tokens: 14\n   llm.latency_ms: 1234.5\n</code></pre>"},{"location":"examples/test-scenarios/#compact-mode","title":"Compact Mode","text":"<pre><code>$ prela show abc-123 --compact\n\n\u2514\u2500 anthropic.messages.create (1234ms) \u2713\n\n\ud83d\udca1 Tip: Run without --compact to see full span details and events\n</code></pre>"},{"location":"examples/test-scenarios/#most-recent-trace","title":"Most Recent Trace","text":"<pre><code>$ prela last\n\n# Shows most recent trace with full details\n# Equivalent to: prela list | head -1 | prela show\n</code></pre>"},{"location":"examples/test-scenarios/#filter-errors","title":"Filter Errors","text":"<pre><code>$ prela errors --limit 10\n\nShowing 2 error traces (from last 50):\n...\n</code></pre>"},{"location":"examples/test-scenarios/#real-time-monitoring","title":"Real-Time Monitoring","text":"<pre><code>$ prela tail --compact\n\nWatching for new traces (Ctrl+C to stop)...\n\n[12:34:56] \u2514\u2500 simple call (1234ms) \u2713\n[12:35:12] \u2514\u2500 workflow (3456ms) \u2713\n[12:35:45] \u2514\u2500 streaming (2345ms) \u2713\n</code></pre>"},{"location":"examples/test-scenarios/#performance-validation","title":"Performance Validation","text":"<p>All performance criteria validated:</p>"},{"location":"examples/test-scenarios/#sdk-overhead","title":"SDK Overhead","text":"<ul> <li>Target: &lt; 5% of request time</li> <li>Actual: &lt; 100ms instrumentation overhead (~1-2% for 1-2 second API calls)</li> <li>Status: \u2705 PASS</li> </ul>"},{"location":"examples/test-scenarios/#trace-file-writes","title":"Trace File Writes","text":"<ul> <li>Target: Non-blocking</li> <li>Actual: Async file I/O, scripts complete without waiting</li> <li>Status: \u2705 PASS</li> </ul>"},{"location":"examples/test-scenarios/#cli-commands-response","title":"CLI Commands Response","text":"<ul> <li>Target: &lt; 1 second</li> <li>Actual: &lt; 100ms for list/show/search</li> <li>Status: \u2705 PASS</li> </ul>"},{"location":"examples/test-scenarios/#replay-engine","title":"Replay Engine","text":"<ul> <li>Target: Reasonable time</li> <li>Actual: ~2 seconds for API call replay</li> <li>Status: \u2705 PASS</li> </ul>"},{"location":"examples/test-scenarios/#documentation-validation","title":"Documentation Validation","text":"<p>All documentation criteria validated:</p>"},{"location":"examples/test-scenarios/#test-scenario-comments","title":"Test Scenario Comments","text":"<ul> <li>Target: Clear docstrings</li> <li>Actual: All 6 scenarios have detailed docstrings</li> <li>Status: \u2705 PASS</li> </ul>"},{"location":"examples/test-scenarios/#expected-outputs","title":"Expected Outputs","text":"<ul> <li>Target: Documented</li> <li>Actual: SDK_LOCAL_TESTING.md documents all expected outputs</li> <li>Status: \u2705 PASS</li> </ul>"},{"location":"examples/test-scenarios/#error-messages","title":"Error Messages","text":"<ul> <li>Target: Helpful and actionable</li> <li>Actual: All errors include clear messages and suggestions</li> <li>Status: \u2705 PASS</li> </ul>"},{"location":"examples/test-scenarios/#cli-help-text","title":"CLI Help Text","text":"<ul> <li>Target: Accurate</li> <li>Actual: <code>prela --help</code> shows complete, accurate help</li> <li>Status: \u2705 PASS</li> </ul>"},{"location":"examples/test-scenarios/#full-validation-report","title":"Full Validation Report","text":"<p>See the complete Phase 4 validation report with all evidence:</p> <p>\ud83d\udcc4 Phase 4 Validation Results</p> <p>Summary:</p> <ul> <li>Total Features Validated: 21/21 (100%)</li> <li>Performance Criteria Met: 4/4 (100%)</li> <li>Documentation Quality: 4/4 (100%)</li> <li>Overall Status: \u2705 COMPLETE</li> </ul>"},{"location":"examples/test-scenarios/#next-steps","title":"Next Steps","text":"<p>After validating these scenarios:</p> <ol> <li>Explore Advanced Examples: See sdk/examples/ for more patterns</li> <li>Read Integration Guides: Check Integrations for framework-specific usage</li> <li>Build Your Agent: Apply these patterns to production applications</li> <li>Deploy Observability: Use file exporter or OTLP exporter for production monitoring</li> </ol>"},{"location":"examples/test-scenarios/#troubleshooting","title":"Troubleshooting","text":""},{"location":"examples/test-scenarios/#api-key-not-set","title":"API Key Not Set","text":"<pre><code># Error: \"Could not resolve authentication method\"\nexport ANTHROPIC_API_KEY=\"sk-ant-...\"\n</code></pre>"},{"location":"examples/test-scenarios/#module-not-found","title":"Module Not Found","text":"<pre><code># Error: \"No module named 'prela'\"\ncd /Users/gw/prela/sdk\npip install -e .\n</code></pre>"},{"location":"examples/test-scenarios/#no-traces-generated","title":"No Traces Generated","text":"<pre><code># Check directory exists\nls -la ./test_traces/\n\n# Verify JSONL contents\ncat ./test_traces/traces-*.jsonl | jq .\n</code></pre>"},{"location":"examples/test-scenarios/#cli-command-not-found","title":"CLI Command Not Found","text":"<pre><code># Ensure CLI tools installed\npip install -e \".[cli]\"\n\n# Verify installation\nwhich prela\nprela --version\n</code></pre>"},{"location":"examples/test-scenarios/#related-documentation","title":"Related Documentation","text":"<ul> <li>Getting Started</li> <li>CLI Reference</li> <li>Evaluation Framework</li> <li>API Reference</li> </ul>"},{"location":"getting-started/configuration/","title":"Configuration","text":"<p>Configure Prela using function parameters or environment variables.</p>"},{"location":"getting-started/configuration/#basic-configuration","title":"Basic Configuration","text":""},{"location":"getting-started/configuration/#service-name","title":"Service Name","text":"<p>Identify your application in traces:</p> <pre><code>import prela\n\nprela.init(service_name=\"customer-support-bot\")\n</code></pre> <p>Or via environment variable:</p> <pre><code>export PRELA_SERVICE_NAME=\"customer-support-bot\"\n</code></pre> <pre><code>prela.init()  # Uses PRELA_SERVICE_NAME\n</code></pre>"},{"location":"getting-started/configuration/#exporters","title":"Exporters","text":"<p>Choose where traces are sent.</p>"},{"location":"getting-started/configuration/#console-exporter","title":"Console Exporter","text":"<p>Print traces to stdout (default):</p> <pre><code>prela.init(\n    service_name=\"my-agent\",\n    exporter=\"console\"\n)\n</code></pre> <p>Options:</p> <ul> <li><code>pretty</code>: Enable pretty-printing (default: <code>True</code>)</li> <li><code>indent</code>: JSON indentation spaces (default: <code>2</code>)</li> </ul> <pre><code>from prela.exporters import ConsoleExporter\n\nprela.init(\n    service_name=\"my-agent\",\n    exporter=ConsoleExporter(pretty=True, indent=4)\n)\n</code></pre>"},{"location":"getting-started/configuration/#file-exporter","title":"File Exporter","text":"<p>Save traces to JSONL files:</p> <pre><code>prela.init(\n    service_name=\"my-agent\",\n    exporter=\"file\",\n    directory=\"./traces\"\n)\n</code></pre> <p>Files are organized by date:</p> <pre><code>traces/\n\u251c\u2500\u2500 2025-01-26/\n\u2502   \u251c\u2500\u2500 trace_abc123.json\n\u2502   \u251c\u2500\u2500 trace_def456.json\n\u2502   \u2514\u2500\u2500 trace_ghi789.json\n\u2514\u2500\u2500 2025-01-27/\n    \u2514\u2500\u2500 trace_jkl012.json\n</code></pre> <p>Options:</p> <ul> <li><code>directory</code>: Base directory (default: <code>\"./traces\"</code>)</li> <li><code>max_file_size</code>: Rotate after N bytes (default: <code>None</code>, no rotation)</li> </ul> <pre><code>from prela.exporters import FileExporter\n\nprela.init(\n    service_name=\"my-agent\",\n    exporter=FileExporter(\n        directory=\"./production-traces\",\n        max_file_size=10 * 1024 * 1024  # 10 MB\n    )\n)\n</code></pre>"},{"location":"getting-started/configuration/#custom-exporter","title":"Custom Exporter","text":"<p>Implement your own exporter:</p> <pre><code>from prela.exporters import BaseExporter, ExportResult\nfrom typing import List\nfrom prela.core import Span\n\nclass MyExporter(BaseExporter):\n    def export(self, spans: List[Span]) -&gt; ExportResult:\n        # Send to your backend\n        for span in spans:\n            my_backend.send(span.to_dict())\n        return ExportResult.SUCCESS\n\nprela.init(\n    service_name=\"my-agent\",\n    exporter=MyExporter()\n)\n</code></pre>"},{"location":"getting-started/configuration/#sampling","title":"Sampling","text":"<p>Control which traces are recorded.</p>"},{"location":"getting-started/configuration/#sample-rate","title":"Sample Rate","text":"<p>Sample a percentage of traces:</p> <pre><code>prela.init(\n    service_name=\"my-agent\",\n    sample_rate=0.1  # Sample 10% of traces\n)\n</code></pre> <p>Uses probability-based sampling with hash consistency (same trace ID = same decision).</p>"},{"location":"getting-started/configuration/#always-onoff","title":"Always On/Off","text":"<p>For development or to disable tracing:</p> <pre><code># Development: trace everything\nprela.init(service_name=\"my-agent\", sample_rate=1.0)\n\n# Disable tracing\nprela.init(service_name=\"my-agent\", sample_rate=0.0)\n</code></pre>"},{"location":"getting-started/configuration/#custom-sampler","title":"Custom Sampler","text":"<p>Implement your own sampling logic:</p> <pre><code>from prela.core import BaseSampler\n\nclass MySampler(BaseSampler):\n    def should_sample(self, trace_id: str) -&gt; bool:\n        # Custom logic\n        return my_sampling_logic(trace_id)\n\nfrom prela import init, get_tracer\n\ntracer = get_tracer()\ntracer.sampler = MySampler()\n</code></pre>"},{"location":"getting-started/configuration/#auto-instrumentation","title":"Auto-Instrumentation","text":"<p>Enable or disable auto-instrumentation:</p> <pre><code># Enabled by default\nprela.init(service_name=\"my-agent\", auto_instrument=True)\n\n# Disable (manual instrumentation required)\nprela.init(service_name=\"my-agent\", auto_instrument=False)\n</code></pre> <p>Environment variable:</p> <pre><code>export PRELA_AUTO_INSTRUMENT=false\n</code></pre>"},{"location":"getting-started/configuration/#debug-mode","title":"Debug Mode","text":"<p>Enable verbose logging:</p> <pre><code>prela.init(service_name=\"my-agent\", debug=True)\n</code></pre> <p>Or via environment:</p> <pre><code>export PRELA_DEBUG=true\n</code></pre> <p>Logs include:</p> <ul> <li>Instrumentation status</li> <li>Span creation/completion</li> <li>Export attempts</li> <li>Sampling decisions</li> <li>Errors and warnings</li> </ul>"},{"location":"getting-started/configuration/#environment-variables","title":"Environment Variables","text":"<p>All configuration can be set via environment variables:</p> Variable Description Default <code>PRELA_SERVICE_NAME</code> Service name <code>\"unknown\"</code> <code>PRELA_EXPORTER</code> Exporter type (<code>\"console\"</code>, <code>\"file\"</code>) <code>\"console\"</code> <code>PRELA_SAMPLE_RATE</code> Sampling rate (0.0-1.0) <code>1.0</code> <code>PRELA_AUTO_INSTRUMENT</code> Enable auto-instrumentation <code>true</code> <code>PRELA_DEBUG</code> Enable debug logging <code>false</code> <code>PRELA_TRACE_DIR</code> Directory for file exporter <code>\"./traces\"</code> <p>Example:</p> <pre><code>export PRELA_SERVICE_NAME=\"my-agent\"\nexport PRELA_EXPORTER=\"file\"\nexport PRELA_SAMPLE_RATE=\"0.1\"\nexport PRELA_AUTO_INSTRUMENT=\"true\"\nexport PRELA_DEBUG=\"false\"\nexport PRELA_TRACE_DIR=\"./traces\"\n</code></pre> <pre><code>import prela\n\n# Uses all environment variables\nprela.init()\n</code></pre> <p>Priority: Function parameters override environment variables.</p>"},{"location":"getting-started/configuration/#production-configuration","title":"Production Configuration","text":"<p>Recommended settings for production:</p> <pre><code>import prela\n\nprela.init(\n    service_name=\"production-agent\",\n    exporter=\"file\",\n    directory=\"/var/log/prela/traces\",\n    sample_rate=0.1,  # 10% sampling\n    debug=False\n)\n</code></pre> <p>With environment variables:</p> <pre><code>export PRELA_SERVICE_NAME=\"production-agent\"\nexport PRELA_EXPORTER=\"file\"\nexport PRELA_TRACE_DIR=\"/var/log/prela/traces\"\nexport PRELA_SAMPLE_RATE=\"0.1\"\nexport PRELA_DEBUG=\"false\"\n</code></pre>"},{"location":"getting-started/configuration/#development-configuration","title":"Development Configuration","text":"<p>Recommended settings for development:</p> <pre><code>import prela\n\nprela.init(\n    service_name=\"dev-agent\",\n    exporter=\"console\",\n    sample_rate=1.0,  # Trace everything\n    debug=True\n)\n</code></pre>"},{"location":"getting-started/configuration/#cli-configuration","title":"CLI Configuration","text":"<p>Configure CLI behavior with <code>.prela.yaml</code>:</p> <pre><code># .prela.yaml\nservice_name: my-agent\ntrace_directory: ./traces\ndefault_exporter: file\n</code></pre> <p>Place in your project root or home directory (<code>~/.prela.yaml</code>).</p>"},{"location":"getting-started/configuration/#configuration-examples","title":"Configuration Examples","text":""},{"location":"getting-started/configuration/#multiple-exporters","title":"Multiple Exporters","text":"<p>Send traces to multiple destinations:</p> <pre><code>from prela.exporters import ConsoleExporter, FileExporter\n\nconsole = ConsoleExporter(pretty=True)\nfile = FileExporter(directory=\"./traces\")\n\n# Export to both (requires custom tracer setup)\ntracer = prela.get_tracer()\n# Note: Current version supports single exporter\n# Multi-exporter support coming in future release\n</code></pre>"},{"location":"getting-started/configuration/#dynamic-configuration","title":"Dynamic Configuration","text":"<p>Change configuration at runtime:</p> <pre><code>import prela\n\n# Initial setup\nprela.init(service_name=\"my-agent\", sample_rate=1.0)\n\n# Later: reduce sampling\ntracer = prela.get_tracer()\nfrom prela.core import ProbabilitySampler\ntracer.sampler = ProbabilitySampler(rate=0.1)\n</code></pre>"},{"location":"getting-started/configuration/#per-environment-configuration","title":"Per-Environment Configuration","text":"<pre><code>import os\nimport prela\n\nenv = os.getenv(\"ENVIRONMENT\", \"development\")\n\nif env == \"production\":\n    prela.init(\n        service_name=\"my-agent\",\n        exporter=\"file\",\n        directory=\"/var/log/traces\",\n        sample_rate=0.05,  # 5%\n        debug=False\n    )\nelif env == \"staging\":\n    prela.init(\n        service_name=\"my-agent-staging\",\n        exporter=\"file\",\n        directory=\"./traces\",\n        sample_rate=0.2,  # 20%\n        debug=True\n    )\nelse:  # development\n    prela.init(\n        service_name=\"my-agent-dev\",\n        exporter=\"console\",\n        sample_rate=1.0,\n        debug=True\n    )\n</code></pre>"},{"location":"getting-started/configuration/#next-steps","title":"Next Steps","text":"<ul> <li>Concepts: Tracing - Understand distributed tracing</li> <li>Concepts: Sampling - Learn about sampling strategies</li> <li>Concepts: Exporters - Deep dive into exporters</li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>Install Prela using pip, the Python package manager.</p>"},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<ul> <li>Python: 3.9 or higher</li> <li>pip: Latest version recommended</li> </ul>"},{"location":"getting-started/installation/#install-via-pip","title":"Install via pip","text":"<pre><code>pip install prela\n</code></pre> <p>This installs the core Prela SDK with zero dependencies.</p>"},{"location":"getting-started/installation/#optional-dependencies","title":"Optional Dependencies","text":"<p>Install with optional integrations:</p>"},{"location":"getting-started/installation/#openai","title":"OpenAI","text":"<pre><code>pip install prela[openai]\n</code></pre> <p>Includes: <code>openai&gt;=1.0.0</code></p>"},{"location":"getting-started/installation/#anthropic","title":"Anthropic","text":"<pre><code>pip install prela[anthropic]\n</code></pre> <p>Includes: <code>anthropic&gt;=0.40.0</code></p>"},{"location":"getting-started/installation/#langchain","title":"LangChain","text":"<pre><code>pip install prela[langchain]\n</code></pre> <p>Includes: <code>langchain&gt;=0.1.0</code></p>"},{"location":"getting-started/installation/#llamaindex","title":"LlamaIndex","text":"<pre><code>pip install prela[llamaindex]\n</code></pre> <p>Includes: <code>llama-index&gt;=0.9.0</code></p>"},{"location":"getting-started/installation/#crewai","title":"CrewAI","text":"<pre><code>pip install prela[crewai]\n</code></pre> <p>Includes: <code>crewai&gt;=0.30.0</code></p>"},{"location":"getting-started/installation/#autogen","title":"AutoGen","text":"<pre><code>pip install prela[autogen]\n</code></pre> <p>Includes: <code>autogen&gt;=0.2.0</code></p>"},{"location":"getting-started/installation/#langgraph","title":"LangGraph","text":"<pre><code>pip install prela[langgraph]\n</code></pre> <p>Includes: <code>langgraph&gt;=0.0.20</code></p>"},{"location":"getting-started/installation/#swarm","title":"Swarm","text":"<pre><code>pip install prela[swarm]\n</code></pre> <p>Includes: <code>openai-swarm</code></p>"},{"location":"getting-started/installation/#all-integrations","title":"All Integrations","text":"<pre><code>pip install prela[all]\n</code></pre> <p>Includes all optional dependencies.</p>"},{"location":"getting-started/installation/#development","title":"Development","text":"<pre><code>pip install prela[dev]\n</code></pre> <p>Includes testing and development tools:</p> <ul> <li><code>pytest&gt;=7.4.0</code></li> <li><code>pytest-asyncio&gt;=0.21.0</code></li> <li><code>pytest-cov&gt;=4.1.0</code></li> <li><code>black&gt;=23.7.0</code></li> <li><code>ruff&gt;=0.0.285</code></li> <li><code>mypy&gt;=1.5.0</code></li> </ul>"},{"location":"getting-started/installation/#verify-installation","title":"Verify Installation","text":"<p>Check that Prela is installed correctly:</p> <pre><code>python -c \"import prela; print(prela.__version__)\"\n</code></pre> <p>Expected output:</p> <pre><code>0.1.0\n</code></pre>"},{"location":"getting-started/installation/#upgrade","title":"Upgrade","text":"<p>Upgrade to the latest version:</p> <pre><code>pip install --upgrade prela\n</code></pre>"},{"location":"getting-started/installation/#uninstall","title":"Uninstall","text":"<p>Remove Prela:</p> <pre><code>pip uninstall prela\n</code></pre>"},{"location":"getting-started/installation/#development-installation","title":"Development Installation","text":"<p>For contributors, install from source:</p> <pre><code># Clone SDK repository\ngit clone https://github.com/garrettw2200/prela-sdk.git\ncd prela-sdk\n\n# Install in editable mode with dev dependencies\npip install -e \".[dev,all]\"\n\n# Run tests\npytest\n</code></pre>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<ul> <li>Quick Start - Create your first trace</li> <li>Configuration - Configure Prela</li> </ul>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":"<p>Get your first trace in 5 minutes with Prela's auto-instrumentation.</p>"},{"location":"getting-started/quickstart/#step-1-install-prela","title":"Step 1: Install Prela","text":"<pre><code>pip install prela\n</code></pre>"},{"location":"getting-started/quickstart/#step-2-initialize-prela","title":"Step 2: Initialize Prela","text":"<p>Add one line to your application:</p> <pre><code>import prela\n\n# Initialize with service name\nprela.init(service_name=\"my-agent\")\n</code></pre> <p>That's it! Prela will automatically instrument any LLM SDKs you use.</p>"},{"location":"getting-started/quickstart/#step-3-run-your-agent","title":"Step 3: Run Your Agent","text":"<p>Use your LLM SDK as normal:</p> AnthropicOpenAILangChain <pre><code>import prela\nfrom anthropic import Anthropic\n\n# Initialize Prela\nprela.init(service_name=\"my-agent\")\n\n# Use Anthropic normally - automatic tracing!\nclient = Anthropic(api_key=\"sk-...\")\nresponse = client.messages.create(\n    model=\"claude-sonnet-4-20250514\",\n    max_tokens=1024,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Explain quantum computing\"}\n    ]\n)\n\nprint(response.content[0].text)\n</code></pre> <pre><code>import prela\nfrom openai import OpenAI\n\n# Initialize Prela\nprela.init(service_name=\"my-agent\")\n\n# Use OpenAI normally - automatic tracing!\nclient = OpenAI(api_key=\"sk-...\")\nresponse = client.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Explain quantum computing\"}\n    ]\n)\n\nprint(response.choices[0].message.content)\n</code></pre> <pre><code>import prela\nfrom langchain.llms import OpenAI\nfrom langchain.agents import initialize_agent, Tool\n\n# Initialize Prela\nprela.init(service_name=\"my-agent\")\n\n# Create agent - automatic tracing!\nllm = OpenAI(temperature=0)\ntools = [\n    Tool(\n        name=\"Calculator\",\n        func=lambda x: eval(x),\n        description=\"Useful for math\"\n    )\n]\n\nagent = initialize_agent(\n    tools,\n    llm,\n    agent=\"zero-shot-react-description\"\n)\n\nresult = agent.run(\"What is 25 * 4?\")\nprint(result)\n</code></pre>"},{"location":"getting-started/quickstart/#step-4-view-your-traces","title":"Step 4: View Your Traces","text":"<p>By default, traces are printed to the console:</p> <pre><code>{\n  \"trace_id\": \"abc123...\",\n  \"span_id\": \"def456...\",\n  \"name\": \"anthropic.messages.create\",\n  \"span_type\": \"llm\",\n  \"status\": \"success\",\n  \"started_at\": \"2025-01-26T10:30:00.123456Z\",\n  \"ended_at\": \"2025-01-26T10:30:01.234567Z\",\n  \"duration_ms\": 1111.1,\n  \"attributes\": {\n    \"service.name\": \"my-agent\",\n    \"llm.vendor\": \"anthropic\",\n    \"llm.model\": \"claude-sonnet-4-20250514\",\n    \"llm.input_tokens\": 50,\n    \"llm.output_tokens\": 200,\n    \"llm.total_tokens\": 250,\n    \"llm.latency_ms\": 1111.1\n  },\n  \"events\": [\n    {\n      \"name\": \"llm.request\",\n      \"timestamp\": \"2025-01-26T10:30:00.123456Z\",\n      \"attributes\": {\n        \"messages\": [{\"role\": \"user\", \"content\": \"Explain quantum computing\"}]\n      }\n    },\n    {\n      \"name\": \"llm.response\",\n      \"timestamp\": \"2025-01-26T10:30:01.234567Z\",\n      \"attributes\": {\n        \"content\": \"Quantum computing is...\"\n      }\n    }\n  ]\n}\n</code></pre>"},{"location":"getting-started/quickstart/#whats-captured","title":"What's Captured?","text":"<p>Every trace automatically includes:</p>"},{"location":"getting-started/quickstart/#request-data","title":"Request Data","text":"<ul> <li>Model name (<code>llm.model</code>)</li> <li>Temperature, max tokens, etc.</li> <li>Input messages or prompt</li> <li>System prompt (if provided)</li> </ul>"},{"location":"getting-started/quickstart/#response-data","title":"Response Data","text":"<ul> <li>Token usage (input, output, total)</li> <li>Completion reason (stop, length, tool_use)</li> <li>Response content</li> <li>Model used (actual model from API)</li> </ul>"},{"location":"getting-started/quickstart/#performance","title":"Performance","text":"<ul> <li>Total latency in milliseconds</li> <li>Time-to-first-token (for streaming)</li> <li>Start and end timestamps</li> </ul>"},{"location":"getting-started/quickstart/#tool-usage","title":"Tool Usage","text":"<ul> <li>Tool/function calls detected</li> <li>Tool names and arguments</li> <li>Tool call IDs</li> </ul>"},{"location":"getting-started/quickstart/#errors","title":"Errors","text":"<ul> <li>Exception type and message</li> <li>Stack traces</li> <li>Error status codes</li> </ul>"},{"location":"getting-started/quickstart/#export-to-file","title":"Export to File","text":"<p>Save traces to a file instead of console:</p> <pre><code>import prela\n\nprela.init(\n    service_name=\"my-agent\",\n    exporter=\"file\",\n    directory=\"./traces\"  # Traces saved here\n)\n</code></pre> <p>Traces are saved in JSONL format (one JSON object per line):</p> <pre><code>cat traces/2025-01-26/trace_abc123.json\n</code></pre>"},{"location":"getting-started/quickstart/#custom-spans","title":"Custom Spans","text":"<p>Create custom spans for your own functions:</p> <pre><code>import prela\n\ntracer = prela.get_tracer()\n\ndef my_function():\n    with tracer.span(\"my_operation\", span_type=\"custom\"):\n        # Your code here\n        result = expensive_computation()\n        return result\n</code></pre>"},{"location":"getting-started/quickstart/#cli-commands","title":"CLI Commands","text":"<p>Use the CLI to explore traces:</p> <pre><code># List recent traces\nprela list\n\n# Show trace details\nprela show trace_abc123\n\n# Search for traces\nprela search --status error --service my-agent\n</code></pre>"},{"location":"getting-started/quickstart/#whats-next","title":"What's Next?","text":"<p>Now that you have basic tracing working:</p> <ul> <li>Configuration - Learn about all configuration options</li> <li>Concepts - Understand how tracing works</li> <li>Integrations - Deep dive into each integration</li> <li>Evaluation - Test your agents systematically</li> </ul>"},{"location":"getting-started/quickstart/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/quickstart/#no-traces-appearing","title":"No traces appearing?","text":"<p>Check that:</p> <ol> <li>You called <code>prela.init()</code> before using your LLM SDK</li> <li>Your LLM SDK is installed (<code>pip list | grep anthropic</code>)</li> <li>Auto-instrumentation is enabled (default)</li> </ol>"},{"location":"getting-started/quickstart/#want-to-disable-auto-instrumentation","title":"Want to disable auto-instrumentation?","text":"<pre><code>prela.init(service_name=\"my-agent\", auto_instrument=False)\n</code></pre> <p>Then manually instrument:</p> <pre><code>from prela.instrumentation import AnthropicInstrumentor\n\ninstrumentor = AnthropicInstrumentor()\ninstrumentor.instrument(tracer=prela.get_tracer())\n</code></pre>"},{"location":"getting-started/quickstart/#need-help","title":"Need help?","text":"<ul> <li>GitHub Issues</li> <li>Concepts Documentation</li> </ul>"},{"location":"guides/DRIFT_DETECTION_QUICKSTART/","title":"Drift Detection - Quick Start Guide","text":"<p>This guide shows you how to use Prela's drift detection API to monitor agent behavior anomalies.</p>"},{"location":"guides/DRIFT_DETECTION_QUICKSTART/#prerequisites","title":"Prerequisites","text":"<ul> <li>Prela API Gateway running</li> <li>ClickHouse with agent baselines table</li> <li>Active agents with trace data (minimum 7 days recommended)</li> </ul>"},{"location":"guides/DRIFT_DETECTION_QUICKSTART/#step-1-calculate-baselines","title":"Step 1: Calculate Baselines","text":"<p>First, establish baseline metrics for your agents:</p> <pre><code># Calculate baselines for all agents in a project\ncurl -X POST \"http://localhost:8000/api/v1/drift/projects/my-project/baselines/calculate\"\n\n# Response:\n{\n  \"baselines_calculated\": 5\n}\n</code></pre> <p>What it does: Analyzes last 7 days of agent behavior and stores statistical baselines (mean, stddev, percentiles) for 16 metrics.</p> <p>Schedule: Run this daily via cron to keep baselines up-to-date.</p>"},{"location":"guides/DRIFT_DETECTION_QUICKSTART/#step-2-check-for-drift","title":"Step 2: Check for Drift","text":"<p>Monitor for anomalies in agent behavior:</p> <pre><code># Check all agents\ncurl \"http://localhost:8000/api/v1/drift/projects/my-project/drift/check\"\n\n# Check specific agent\ncurl \"http://localhost:8000/api/v1/drift/projects/my-project/drift/check?agent_name=researcher\"\n\n# Adjust sensitivity (1.0 = more alerts, 4.0 = fewer alerts)\ncurl \"http://localhost:8000/api/v1/drift/projects/my-project/drift/check?sensitivity=2.0\"\n\n# Check last 12 hours only\ncurl \"http://localhost:8000/api/v1/drift/projects/my-project/drift/check?lookback_hours=12\"\n</code></pre> <p>Response (when drift detected): <pre><code>{\n  \"alerts\": [\n    {\n      \"agent_name\": \"researcher\",\n      \"service_name\": \"my-service\",\n      \"anomalies\": [\n        {\n          \"metric_name\": \"duration\",\n          \"current_value\": 12847.3,\n          \"baseline_mean\": 5243.2,\n          \"change_percent\": 145.0,\n          \"severity\": \"high\",\n          \"direction\": \"increased\",\n          \"unit\": \"ms\",\n          \"sample_size\": 87\n        }\n      ],\n      \"root_causes\": [\n        {\n          \"type\": \"model_change\",\n          \"description\": \"Model changed: gpt-4, gpt-4o\",\n          \"confidence\": 0.9\n        }\n      ],\n      \"detected_at\": \"2026-01-30T14:23:45\"\n    }\n  ],\n  \"count\": 1\n}\n</code></pre></p>"},{"location":"guides/DRIFT_DETECTION_QUICKSTART/#step-3-view-historical-baselines","title":"Step 3: View Historical Baselines","text":"<pre><code># List all baselines for a project\ncurl \"http://localhost:8000/api/v1/drift/projects/my-project/baselines\"\n\n# Filter by agent\ncurl \"http://localhost:8000/api/v1/drift/projects/my-project/baselines?agent_name=researcher\"\n\n# Limit results\ncurl \"http://localhost:8000/api/v1/drift/projects/my-project/baselines?limit=20\"\n</code></pre>"},{"location":"guides/DRIFT_DETECTION_QUICKSTART/#understanding-alerts","title":"Understanding Alerts","text":""},{"location":"guides/DRIFT_DETECTION_QUICKSTART/#severity-levels","title":"Severity Levels","text":"Severity Z-Score Meaning Action Critical &gt; 4\u03c3 Extreme deviation Immediate investigation High 3-4\u03c3 Significant deviation Investigate soon Medium 2-3\u03c3 Notable deviation Monitor closely Low 1-2\u03c3 Minor deviation Track trend"},{"location":"guides/DRIFT_DETECTION_QUICKSTART/#metrics-monitored","title":"Metrics Monitored","text":"<ol> <li>Duration - How long agents take to complete tasks</li> <li>Token Usage - LLM token consumption per execution</li> <li>Tool Calls - Frequency of tool invocations</li> <li>Success Rate - Percentage of successful completions</li> <li>Cost - USD spent per execution</li> </ol>"},{"location":"guides/DRIFT_DETECTION_QUICKSTART/#root-cause-types","title":"Root Cause Types","text":"<ol> <li>model_change - Agent switched LLM models (confidence: 0.9)</li> <li>input_complexity_increase - Input size/complexity grew (confidence: 0.8)</li> <li>error_rate_increase - More failures detected (confidence: 0.95)</li> </ol>"},{"location":"guides/DRIFT_DETECTION_QUICKSTART/#example-scenarios","title":"Example Scenarios","text":""},{"location":"guides/DRIFT_DETECTION_QUICKSTART/#scenario-1-performance-regression","title":"Scenario 1: Performance Regression","text":"<p>Problem: Agent suddenly takes 2x longer to respond</p> <p>Detection: <pre><code>curl \"http://localhost:8000/api/v1/drift/projects/prod/drift/check?agent_name=qa_agent\"\n</code></pre></p> <p>Alert: <pre><code>{\n  \"metric_name\": \"duration\",\n  \"current_value\": 10234.5,\n  \"baseline_mean\": 5123.7,\n  \"change_percent\": 99.7,\n  \"severity\": \"high\",\n  \"direction\": \"increased\"\n}\n</code></pre></p> <p>Root Cause: \"Input complexity increased by 85.3%\"</p> <p>Action: Investigate input data sources, consider prompt optimization</p>"},{"location":"guides/DRIFT_DETECTION_QUICKSTART/#scenario-2-cost-spike","title":"Scenario 2: Cost Spike","text":"<p>Problem: Daily costs increased from $50 to $200</p> <p>Detection: <pre><code>curl \"http://localhost:8000/api/v1/drift/projects/prod/drift/check?sensitivity=1.5\"\n</code></pre></p> <p>Alert: <pre><code>{\n  \"metric_name\": \"cost\",\n  \"current_value\": 0.082,\n  \"baseline_mean\": 0.021,\n  \"change_percent\": 290.5,\n  \"severity\": \"critical\"\n}\n</code></pre></p> <p>Root Cause: \"Model changed: gpt-4o-mini, gpt-4\"</p> <p>Action: Verify model selection logic, consider reverting to cheaper model</p>"},{"location":"guides/DRIFT_DETECTION_QUICKSTART/#scenario-3-success-rate-drop","title":"Scenario 3: Success Rate Drop","text":"<p>Problem: Agent failing 15% more requests</p> <p>Detection: <pre><code>curl \"http://localhost:8000/api/v1/drift/projects/prod/drift/check?lookback_hours=6\"\n</code></pre></p> <p>Alert: <pre><code>{\n  \"metric_name\": \"success_rate\",\n  \"current_value\": 0.832,\n  \"baseline_mean\": 0.982,\n  \"change_percent\": -15.3,\n  \"severity\": \"high\",\n  \"direction\": \"decreased\"\n}\n</code></pre></p> <p>Root Cause: \"Error rate increased (success rate dropped 15.3%)\"</p> <p>Action: Check external API health, review error logs</p>"},{"location":"guides/DRIFT_DETECTION_QUICKSTART/#automation-examples","title":"Automation Examples","text":""},{"location":"guides/DRIFT_DETECTION_QUICKSTART/#daily-baseline-update-cron","title":"Daily Baseline Update (Cron)","text":"<pre><code># /etc/cron.d/prela-baselines\n0 2 * * * curl -X POST \"http://api.prela.internal/api/v1/drift/projects/prod/baselines/calculate\"\n</code></pre>"},{"location":"guides/DRIFT_DETECTION_QUICKSTART/#alerting-script","title":"Alerting Script","text":"<pre><code>import requests\nimport smtplib\nfrom email.mime.text import MIMEText\n\ndef check_drift():\n    response = requests.get(\n        \"http://api.prela.internal/api/v1/drift/projects/prod/drift/check\",\n        params={\"sensitivity\": 2.0}\n    )\n    data = response.json()\n\n    if data[\"count\"] &gt; 0:\n        send_alert_email(data[\"alerts\"])\n\ndef send_alert_email(alerts):\n    msg = MIMEText(f\"Detected {len(alerts)} drift alerts:\\n\\n{alerts}\")\n    msg[\"Subject\"] = \"Prela Drift Alert\"\n    msg[\"To\"] = \"ops@company.com\"\n\n    smtp = smtplib.SMTP(\"localhost\")\n    smtp.send_message(msg)\n    smtp.quit()\n\nif __name__ == \"__main__\":\n    check_drift()\n</code></pre>"},{"location":"guides/DRIFT_DETECTION_QUICKSTART/#slack-integration","title":"Slack Integration","text":"<pre><code>import requests\n\ndef send_slack_alert(alerts):\n    webhook_url = \"https://hooks.slack.com/services/YOUR/WEBHOOK/URL\"\n\n    for alert in alerts:\n        severity_emoji = {\n            \"critical\": \"\ud83d\udd34\",\n            \"high\": \"\ud83d\udfe0\",\n            \"medium\": \"\ud83d\udfe1\",\n            \"low\": \"\ud83d\udd35\"\n        }\n\n        message = {\n            \"text\": f\"{severity_emoji.get(alert['anomalies'][0]['severity'])} Drift Alert: {alert['agent_name']}\",\n            \"blocks\": [\n                {\n                    \"type\": \"section\",\n                    \"text\": {\n                        \"type\": \"mrkdwn\",\n                        \"text\": f\"*Agent:* {alert['agent_name']}\\n*Anomalies:* {len(alert['anomalies'])}\"\n                    }\n                }\n            ]\n        }\n\n        requests.post(webhook_url, json=message)\n</code></pre>"},{"location":"guides/DRIFT_DETECTION_QUICKSTART/#configuration-recommendations","title":"Configuration Recommendations","text":"Environment Window Days Sensitivity Lookback Hours Schedule Development 3 3.0 6 Manual Staging 7 2.5 12 Daily 2am Production 7 2.0 24 Every 6 hours High-Traffic 14 2.0 6 Hourly"},{"location":"guides/DRIFT_DETECTION_QUICKSTART/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/DRIFT_DETECTION_QUICKSTART/#no-baselines-found","title":"No Baselines Found","text":"<p>Symptom: <code>\"message\": \"No baseline found for agent\"</code></p> <p>Cause: Agent hasn't run enough times in the window period</p> <p>Solution: 1. Check agent has &gt;10 executions in last 7 days 2. Verify agent name is correct (case-sensitive) 3. Run baseline calculation manually</p>"},{"location":"guides/DRIFT_DETECTION_QUICKSTART/#insufficient-data","title":"Insufficient Data","text":"<p>Symptom: <code>\"baselines_calculated\": 0, \"message\": \"Insufficient data\"</code></p> <p>Cause: Not enough historical traces</p> <p>Solution: 1. Reduce window_days: <code>?window_days=3</code> 2. Wait for more agent executions 3. Check ClickHouse for span data</p>"},{"location":"guides/DRIFT_DETECTION_QUICKSTART/#too-many-alerts","title":"Too Many Alerts","text":"<p>Symptom: Receiving alerts constantly</p> <p>Cause: Sensitivity too low</p> <p>Solution: 1. Increase sensitivity: <code>?sensitivity=3.0</code> 2. Adjust lookback hours: <code>?lookback_hours=48</code> 3. Review root causes to filter noise</p>"},{"location":"guides/DRIFT_DETECTION_QUICKSTART/#next-steps","title":"Next Steps","text":"<ol> <li>Frontend Integration: Build React dashboard showing drift alerts</li> <li>Notification System: Set up email/Slack alerts</li> <li>Alert Rules: Configure which anomalies trigger notifications</li> <li>Historical Analysis: Track how baselines evolve over time</li> </ol> <p>For questions or feedback, visit: https://github.com/anthropics/prela</p>"}]}